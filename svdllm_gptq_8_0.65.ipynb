{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# model_id = \"pavan01729/svdllm_gptq_8_0.65\"\n",
    "# tokenizer_id = 'jeffwan/llama-7b-hf'\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     tokenizer_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [02:45<00:00, 82.70s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.70it/s]\n",
      "Some weights of the model checkpoint at pavan01729/svdllm_gptq_8_0.65 were not used when initializing LlamaForCausalLM: ['model.layers.13.mlp.up_v_proj.weight', 'model.layers.20.self_attn.q_v_proj.weight', 'model.layers.23.mlp.up_v_proj.weight', 'model.layers.0.self_attn.o_u_proj.weight', 'model.layers.18.self_attn.k_u_proj.weight', 'model.layers.11.self_attn.q_v_proj.weight', 'model.layers.19.mlp.gate_u_proj.weight', 'model.layers.1.self_attn.v_v_proj.weight', 'model.layers.24.self_attn.q_u_proj.weight', 'model.layers.29.self_attn.v_u_proj.weight', 'model.layers.25.self_attn.k_u_proj.weight', 'model.layers.13.self_attn.k_u_proj.weight', 'model.layers.22.mlp.down_v_proj.weight', 'model.layers.29.mlp.down_u_proj.weight', 'model.layers.5.self_attn.q_u_proj.weight', 'model.layers.2.mlp.down_v_proj.weight', 'model.layers.19.self_attn.v_v_proj.weight', 'model.layers.29.self_attn.q_u_proj.weight', 'model.layers.18.mlp.up_u_proj.weight', 'model.layers.24.self_attn.o_u_proj.weight', 'model.layers.0.self_attn.o_v_proj.weight', 'model.layers.18.mlp.up_v_proj.weight', 'model.layers.9.self_attn.o_v_proj.weight', 'model.layers.2.mlp.down_u_proj.weight', 'model.layers.23.mlp.up_u_proj.weight', 'model.layers.10.mlp.down_v_proj.weight', 'model.layers.0.mlp.down_u_proj.weight', 'model.layers.28.self_attn.v_u_proj.weight', 'model.layers.15.self_attn.o_u_proj.weight', 'model.layers.1.self_attn.o_u_proj.weight', 'model.layers.15.mlp.gate_u_proj.weight', 'model.layers.2.self_attn.v_v_proj.weight', 'model.layers.12.self_attn.o_u_proj.weight', 'model.layers.16.self_attn.o_v_proj.weight', 'model.layers.6.mlp.down_v_proj.weight', 'model.layers.14.self_attn.q_v_proj.weight', 'model.layers.29.mlp.up_v_proj.weight', 'model.layers.23.mlp.gate_u_proj.weight', 'model.layers.4.self_attn.k_u_proj.weight', 'model.layers.13.mlp.down_u_proj.weight', 'model.layers.13.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.q_v_proj.weight', 'model.layers.12.self_attn.o_v_proj.weight', 'model.layers.10.self_attn.k_u_proj.weight', 'model.layers.5.mlp.gate_v_proj.weight', 'model.layers.28.self_attn.o_u_proj.weight', 'model.layers.24.mlp.up_u_proj.weight', 'model.layers.6.mlp.gate_v_proj.weight', 'model.layers.28.mlp.gate_v_proj.weight', 'model.layers.7.self_attn.q_u_proj.weight', 'model.layers.14.mlp.up_v_proj.weight', 'model.layers.18.mlp.down_u_proj.weight', 'model.layers.0.mlp.gate_v_proj.weight', 'model.layers.29.self_attn.q_v_proj.weight', 'model.layers.25.mlp.up_v_proj.weight', 'model.layers.28.self_attn.q_v_proj.weight', 'model.layers.20.mlp.gate_v_proj.weight', 'model.layers.3.self_attn.k_u_proj.weight', 'model.layers.31.mlp.down_v_proj.weight', 'model.layers.7.mlp.down_u_proj.weight', 'model.layers.4.self_attn.q_u_proj.weight', 'model.layers.2.self_attn.k_v_proj.weight', 'model.layers.7.self_attn.q_v_proj.weight', 'model.layers.6.self_attn.v_u_proj.weight', 'model.layers.21.self_attn.k_v_proj.weight', 'model.layers.11.self_attn.q_u_proj.weight', 'model.layers.31.self_attn.k_v_proj.weight', 'model.layers.14.self_attn.o_u_proj.weight', 'model.layers.4.mlp.gate_u_proj.weight', 'model.layers.7.self_attn.o_v_proj.weight', 'model.layers.9.mlp.up_v_proj.weight', 'model.layers.28.self_attn.v_v_proj.weight', 'model.layers.18.self_attn.v_v_proj.weight', 'model.layers.18.self_attn.o_u_proj.weight', 'model.layers.5.self_attn.q_v_proj.weight', 'model.layers.14.mlp.gate_v_proj.weight', 'model.layers.18.mlp.down_v_proj.weight', 'model.layers.21.mlp.down_u_proj.weight', 'model.layers.31.self_attn.q_u_proj.weight', 'model.layers.26.self_attn.o_u_proj.weight', 'model.layers.28.mlp.down_u_proj.weight', 'model.layers.27.self_attn.q_v_proj.weight', 'model.layers.18.self_attn.v_u_proj.weight', 'model.layers.21.self_attn.o_u_proj.weight', 'model.layers.21.mlp.down_v_proj.weight', 'model.layers.30.mlp.up_v_proj.weight', 'model.layers.24.self_attn.k_u_proj.weight', 'model.layers.5.self_attn.v_u_proj.weight', 'model.layers.18.mlp.gate_u_proj.weight', 'model.layers.24.self_attn.v_v_proj.weight', 'model.layers.12.mlp.up_v_proj.weight', 'model.layers.3.mlp.up_u_proj.weight', 'model.layers.22.mlp.gate_u_proj.weight', 'model.layers.16.self_attn.k_v_proj.weight', 'model.layers.27.self_attn.k_u_proj.weight', 'model.layers.13.self_attn.o_u_proj.weight', 'model.layers.6.self_attn.k_v_proj.weight', 'model.layers.9.mlp.up_u_proj.weight', 'model.layers.20.self_attn.o_v_proj.weight', 'model.layers.25.self_attn.k_v_proj.weight', 'model.layers.8.self_attn.q_u_proj.weight', 'model.layers.9.self_attn.k_u_proj.weight', 'model.layers.9.self_attn.q_v_proj.weight', 'model.layers.0.self_attn.v_v_proj.weight', 'model.layers.3.self_attn.o_u_proj.weight', 'model.layers.14.mlp.down_u_proj.weight', 'model.layers.17.mlp.up_u_proj.weight', 'model.layers.1.mlp.up_u_proj.weight', 'model.layers.21.self_attn.q_u_proj.weight', 'model.layers.12.self_attn.k_v_proj.weight', 'model.layers.0.mlp.up_u_proj.weight', 'model.layers.30.self_attn.v_v_proj.weight', 'model.layers.8.mlp.down_u_proj.weight', 'model.layers.15.mlp.down_u_proj.weight', 'model.layers.12.self_attn.q_u_proj.weight', 'model.layers.4.mlp.up_u_proj.weight', 'model.layers.1.self_attn.v_u_proj.weight', 'model.layers.1.self_attn.q_v_proj.weight', 'model.layers.15.self_attn.q_v_proj.weight', 'model.layers.21.self_attn.v_v_proj.weight', 'model.layers.25.mlp.up_u_proj.weight', 'model.layers.12.self_attn.q_v_proj.weight', 'model.layers.18.mlp.gate_v_proj.weight', 'model.layers.22.mlp.up_v_proj.weight', 'model.layers.4.mlp.gate_v_proj.weight', 'model.layers.30.mlp.gate_u_proj.weight', 'model.layers.26.self_attn.v_u_proj.weight', 'model.layers.19.self_attn.o_u_proj.weight', 'model.layers.22.self_attn.v_u_proj.weight', 'model.layers.25.self_attn.o_u_proj.weight', 'model.layers.5.self_attn.o_v_proj.weight', 'model.layers.25.self_attn.o_v_proj.weight', 'model.layers.15.self_attn.v_u_proj.weight', 'model.layers.12.mlp.down_u_proj.weight', 'model.layers.11.self_attn.v_v_proj.weight', 'model.layers.31.mlp.up_v_proj.weight', 'model.layers.4.self_attn.k_v_proj.weight', 'model.layers.8.self_attn.v_v_proj.weight', 'model.layers.23.mlp.gate_v_proj.weight', 'model.layers.12.mlp.gate_u_proj.weight', 'model.layers.1.self_attn.o_v_proj.weight', 'model.layers.30.self_attn.q_v_proj.weight', 'model.layers.9.self_attn.k_v_proj.weight', 'model.layers.20.self_attn.o_u_proj.weight', 'model.layers.7.self_attn.k_u_proj.weight', 'model.layers.11.mlp.up_v_proj.weight', 'model.layers.13.self_attn.q_u_proj.weight', 'model.layers.24.self_attn.v_u_proj.weight', 'model.layers.6.self_attn.v_v_proj.weight', 'model.layers.11.self_attn.o_v_proj.weight', 'model.layers.9.mlp.gate_v_proj.weight', 'model.layers.14.self_attn.q_u_proj.weight', 'model.layers.8.self_attn.k_v_proj.weight', 'model.layers.1.mlp.gate_u_proj.weight', 'model.layers.13.mlp.up_u_proj.weight', 'model.layers.10.mlp.down_u_proj.weight', 'model.layers.8.mlp.up_v_proj.weight', 'model.layers.23.self_attn.k_v_proj.weight', 'model.layers.30.mlp.down_v_proj.weight', 'model.layers.6.self_attn.q_u_proj.weight', 'model.layers.19.mlp.down_u_proj.weight', 'model.layers.15.self_attn.k_v_proj.weight', 'model.layers.22.self_attn.q_u_proj.weight', 'model.layers.5.mlp.down_u_proj.weight', 'model.layers.8.self_attn.v_u_proj.weight', 'model.layers.20.mlp.down_u_proj.weight', 'model.layers.28.mlp.up_u_proj.weight', 'model.layers.26.mlp.up_u_proj.weight', 'model.layers.27.mlp.gate_u_proj.weight', 'model.layers.2.self_attn.q_v_proj.weight', 'model.layers.31.mlp.down_u_proj.weight', 'model.layers.23.self_attn.v_v_proj.weight', 'model.layers.1.self_attn.k_u_proj.weight', 'model.layers.14.self_attn.k_v_proj.weight', 'model.layers.19.self_attn.k_u_proj.weight', 'model.layers.27.mlp.down_u_proj.weight', 'model.layers.19.mlp.up_v_proj.weight', 'model.layers.8.mlp.gate_v_proj.weight', 'model.layers.3.mlp.up_v_proj.weight', 'model.layers.27.self_attn.q_u_proj.weight', 'model.layers.20.self_attn.v_v_proj.weight', 'model.layers.5.self_attn.k_u_proj.weight', 'model.layers.2.mlp.up_v_proj.weight', 'model.layers.31.mlp.gate_u_proj.weight', 'model.layers.7.mlp.gate_u_proj.weight', 'model.layers.23.self_attn.k_u_proj.weight', 'model.layers.24.mlp.gate_v_proj.weight', 'model.layers.13.mlp.gate_v_proj.weight', 'model.layers.31.self_attn.k_u_proj.weight', 'model.layers.2.mlp.gate_v_proj.weight', 'model.layers.27.mlp.up_u_proj.weight', 'model.layers.22.self_attn.q_v_proj.weight', 'model.layers.0.self_attn.q_v_proj.weight', 'model.layers.21.mlp.gate_v_proj.weight', 'model.layers.28.mlp.up_v_proj.weight', 'model.layers.2.self_attn.v_u_proj.weight', 'model.layers.5.mlp.up_v_proj.weight', 'model.layers.9.self_attn.q_u_proj.weight', 'model.layers.1.self_attn.k_v_proj.weight', 'model.layers.2.self_attn.q_u_proj.weight', 'model.layers.17.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.k_v_proj.weight', 'model.layers.28.self_attn.k_u_proj.weight', 'model.layers.2.mlp.gate_u_proj.weight', 'model.layers.27.self_attn.o_v_proj.weight', 'model.layers.10.self_attn.o_u_proj.weight', 'model.layers.8.self_attn.o_u_proj.weight', 'model.layers.25.mlp.gate_u_proj.weight', 'model.layers.22.mlp.up_u_proj.weight', 'model.layers.15.self_attn.k_u_proj.weight', 'model.layers.27.mlp.up_v_proj.weight', 'model.layers.10.mlp.gate_u_proj.weight', 'model.layers.10.self_attn.q_u_proj.weight', 'model.layers.27.self_attn.v_u_proj.weight', 'model.layers.28.mlp.down_v_proj.weight', 'model.layers.16.self_attn.q_u_proj.weight', 'model.layers.3.self_attn.v_u_proj.weight', 'model.layers.5.mlp.up_u_proj.weight', 'model.layers.14.self_attn.v_v_proj.weight', 'model.layers.19.mlp.gate_v_proj.weight', 'model.layers.25.self_attn.v_u_proj.weight', 'model.layers.15.self_attn.q_u_proj.weight', 'model.layers.27.self_attn.k_v_proj.weight', 'model.layers.19.self_attn.k_v_proj.weight', 'model.layers.22.self_attn.k_v_proj.weight', 'model.layers.14.mlp.up_u_proj.weight', 'model.layers.20.self_attn.q_u_proj.weight', 'model.layers.31.self_attn.o_v_proj.weight', 'model.layers.4.mlp.up_v_proj.weight', 'model.layers.17.self_attn.o_v_proj.weight', 'model.layers.26.mlp.gate_u_proj.weight', 'model.layers.1.self_attn.q_u_proj.weight', 'model.layers.22.self_attn.v_v_proj.weight', 'model.layers.25.mlp.gate_v_proj.weight', 'model.layers.6.mlp.up_v_proj.weight', 'model.layers.17.mlp.up_v_proj.weight', 'model.layers.30.self_attn.k_u_proj.weight', 'model.layers.7.self_attn.v_v_proj.weight', 'model.layers.16.mlp.down_v_proj.weight', 'model.layers.31.self_attn.q_v_proj.weight', 'model.layers.7.self_attn.k_v_proj.weight', 'model.layers.4.self_attn.o_v_proj.weight', 'model.layers.23.self_attn.q_u_proj.weight', 'model.layers.29.mlp.down_v_proj.weight', 'model.layers.17.mlp.gate_u_proj.weight', 'model.layers.11.self_attn.k_u_proj.weight', 'model.layers.11.mlp.gate_v_proj.weight', 'model.layers.26.mlp.down_v_proj.weight', 'model.layers.7.self_attn.o_u_proj.weight', 'model.layers.12.mlp.up_u_proj.weight', 'model.layers.3.self_attn.q_v_proj.weight', 'model.layers.0.mlp.down_v_proj.weight', 'model.layers.0.mlp.gate_u_proj.weight', 'model.layers.8.self_attn.q_v_proj.weight', 'model.layers.15.self_attn.o_v_proj.weight', 'model.layers.10.self_attn.o_v_proj.weight', 'model.layers.6.self_attn.o_v_proj.weight', 'model.layers.12.self_attn.v_v_proj.weight', 'model.layers.29.self_attn.o_v_proj.weight', 'model.layers.2.mlp.up_u_proj.weight', 'model.layers.30.self_attn.o_u_proj.weight', 'model.layers.20.mlp.up_v_proj.weight', 'model.layers.16.mlp.up_u_proj.weight', 'model.layers.5.self_attn.v_v_proj.weight', 'model.layers.17.self_attn.k_u_proj.weight', 'model.layers.26.mlp.gate_v_proj.weight', 'model.layers.29.self_attn.o_u_proj.weight', 'model.layers.10.mlp.gate_v_proj.weight', 'model.layers.16.mlp.gate_u_proj.weight', 'model.layers.17.self_attn.q_v_proj.weight', 'model.layers.15.mlp.up_u_proj.weight', 'model.layers.6.mlp.gate_u_proj.weight', 'model.layers.7.self_attn.v_u_proj.weight', 'model.layers.17.self_attn.v_v_proj.weight', 'model.layers.5.self_attn.o_u_proj.weight', 'model.layers.9.self_attn.v_v_proj.weight', 'model.layers.20.mlp.down_v_proj.weight', 'model.layers.4.self_attn.o_u_proj.weight', 'model.layers.0.self_attn.q_u_proj.weight', 'model.layers.10.self_attn.v_v_proj.weight', 'model.layers.24.self_attn.k_v_proj.weight', 'model.layers.11.self_attn.v_u_proj.weight', 'model.layers.1.mlp.down_u_proj.weight', 'model.layers.29.mlp.gate_v_proj.weight', 'model.layers.4.self_attn.v_u_proj.weight', 'model.layers.16.self_attn.o_u_proj.weight', 'model.layers.13.self_attn.v_v_proj.weight', 'model.layers.14.self_attn.k_u_proj.weight', 'model.layers.31.self_attn.o_u_proj.weight', 'model.layers.20.mlp.gate_u_proj.weight', 'model.layers.21.mlp.gate_u_proj.weight', 'model.layers.11.mlp.down_u_proj.weight', 'model.layers.13.self_attn.q_v_proj.weight', 'model.layers.8.self_attn.o_v_proj.weight', 'model.layers.7.mlp.up_v_proj.weight', 'model.layers.30.self_attn.v_u_proj.weight', 'model.layers.25.self_attn.q_u_proj.weight', 'model.layers.21.self_attn.k_u_proj.weight', 'model.layers.17.mlp.down_v_proj.weight', 'model.layers.29.self_attn.k_u_proj.weight', 'model.layers.6.self_attn.k_u_proj.weight', 'model.layers.29.mlp.gate_u_proj.weight', 'model.layers.4.self_attn.q_v_proj.weight', 'model.layers.3.self_attn.o_v_proj.weight', 'model.layers.31.mlp.gate_v_proj.weight', 'model.layers.25.mlp.down_u_proj.weight', 'model.layers.27.self_attn.o_u_proj.weight', 'model.layers.10.self_attn.v_u_proj.weight', 'model.layers.6.self_attn.o_u_proj.weight', 'model.layers.10.mlp.up_v_proj.weight', 'model.layers.4.self_attn.v_v_proj.weight', 'model.layers.26.mlp.down_u_proj.weight', 'model.layers.21.mlp.up_v_proj.weight', 'model.layers.16.self_attn.v_u_proj.weight', 'model.layers.7.mlp.down_v_proj.weight', 'model.layers.24.self_attn.o_v_proj.weight', 'model.layers.15.mlp.down_v_proj.weight', 'model.layers.27.self_attn.v_v_proj.weight', 'model.layers.11.self_attn.o_u_proj.weight', 'model.layers.18.self_attn.k_v_proj.weight', 'model.layers.6.self_attn.q_v_proj.weight', 'model.layers.23.self_attn.o_v_proj.weight', 'model.layers.25.mlp.down_v_proj.weight', 'model.layers.9.mlp.down_u_proj.weight', 'model.layers.9.self_attn.o_u_proj.weight', 'model.layers.1.mlp.down_v_proj.weight', 'model.layers.19.self_attn.v_u_proj.weight', 'model.layers.12.self_attn.v_u_proj.weight', 'model.layers.25.self_attn.q_v_proj.weight', 'model.layers.24.mlp.gate_u_proj.weight', 'model.layers.3.mlp.gate_v_proj.weight', 'model.layers.9.mlp.gate_u_proj.weight', 'model.layers.7.mlp.gate_v_proj.weight', 'model.layers.2.self_attn.o_v_proj.weight', 'model.layers.11.mlp.up_u_proj.weight', 'model.layers.26.self_attn.o_v_proj.weight', 'model.layers.30.mlp.down_u_proj.weight', 'model.layers.2.self_attn.k_u_proj.weight', 'model.layers.11.self_attn.k_v_proj.weight', 'model.layers.16.self_attn.v_v_proj.weight', 'model.layers.16.mlp.up_v_proj.weight', 'model.layers.29.self_attn.v_v_proj.weight', 'model.layers.21.self_attn.v_u_proj.weight', 'model.layers.30.self_attn.q_u_proj.weight', 'model.layers.21.self_attn.o_v_proj.weight', 'model.layers.5.mlp.down_v_proj.weight', 'model.layers.12.mlp.down_v_proj.weight', 'model.layers.26.mlp.up_v_proj.weight', 'model.layers.23.self_attn.q_v_proj.weight', 'model.layers.22.mlp.down_u_proj.weight', 'model.layers.20.mlp.up_u_proj.weight', 'model.layers.16.mlp.gate_v_proj.weight', 'model.layers.3.mlp.gate_u_proj.weight', 'model.layers.23.self_attn.v_u_proj.weight', 'model.layers.27.mlp.down_v_proj.weight', 'model.layers.0.self_attn.k_v_proj.weight', 'model.layers.25.self_attn.v_v_proj.weight', 'model.layers.28.mlp.gate_u_proj.weight', 'model.layers.0.self_attn.v_u_proj.weight', 'model.layers.16.self_attn.k_u_proj.weight', 'model.layers.16.self_attn.q_v_proj.weight', 'model.layers.6.mlp.up_u_proj.weight', 'model.layers.30.self_attn.o_v_proj.weight', 'model.layers.3.self_attn.q_u_proj.weight', 'model.layers.3.self_attn.k_v_proj.weight', 'model.layers.13.mlp.down_v_proj.weight', 'model.layers.5.mlp.gate_u_proj.weight', 'model.layers.23.mlp.down_u_proj.weight', 'model.layers.7.mlp.up_u_proj.weight', 'model.layers.3.mlp.down_u_proj.weight', 'model.layers.30.mlp.up_u_proj.weight', 'model.layers.8.self_attn.k_u_proj.weight', 'model.layers.11.mlp.down_v_proj.weight', 'model.layers.15.self_attn.v_v_proj.weight', 'model.layers.5.self_attn.k_v_proj.weight', 'model.layers.12.self_attn.k_u_proj.weight', 'model.layers.14.self_attn.o_v_proj.weight', 'model.layers.19.self_attn.q_u_proj.weight', 'model.layers.23.self_attn.o_u_proj.weight', 'model.layers.24.self_attn.q_v_proj.weight', 'model.layers.18.self_attn.q_v_proj.weight', 'model.layers.14.mlp.down_v_proj.weight', 'model.layers.10.self_attn.q_v_proj.weight', 'model.layers.17.mlp.gate_v_proj.weight', 'model.layers.10.mlp.up_u_proj.weight', 'model.layers.29.mlp.up_u_proj.weight', 'model.layers.2.self_attn.o_u_proj.weight', 'model.layers.23.mlp.down_v_proj.weight', 'model.layers.8.mlp.down_v_proj.weight', 'model.layers.17.mlp.down_u_proj.weight', 'model.layers.15.mlp.gate_v_proj.weight', 'model.layers.22.self_attn.o_v_proj.weight', 'model.layers.24.mlp.down_u_proj.weight', 'model.layers.21.mlp.up_u_proj.weight', 'model.layers.8.mlp.up_u_proj.weight', 'model.layers.30.mlp.gate_v_proj.weight', 'model.layers.13.self_attn.o_v_proj.weight', 'model.layers.26.self_attn.v_v_proj.weight', 'model.layers.0.self_attn.k_u_proj.weight', 'model.layers.13.self_attn.v_u_proj.weight', 'model.layers.31.self_attn.v_u_proj.weight', 'model.layers.11.mlp.gate_u_proj.weight', 'model.layers.1.mlp.gate_v_proj.weight', 'model.layers.6.mlp.down_u_proj.weight', 'model.layers.12.mlp.gate_v_proj.weight', 'model.layers.0.mlp.up_v_proj.weight', 'model.layers.1.mlp.up_v_proj.weight', 'model.layers.9.mlp.down_v_proj.weight', 'model.layers.28.self_attn.o_v_proj.weight', 'model.layers.19.mlp.up_u_proj.weight', 'model.layers.17.self_attn.q_u_proj.weight', 'model.layers.28.self_attn.q_u_proj.weight', 'model.layers.17.self_attn.o_u_proj.weight', 'model.layers.22.self_attn.o_u_proj.weight', 'model.layers.14.self_attn.v_u_proj.weight', 'model.layers.31.mlp.up_u_proj.weight', 'model.layers.3.mlp.down_v_proj.weight', 'model.layers.4.mlp.down_u_proj.weight', 'model.layers.20.self_attn.k_u_proj.weight', 'model.layers.9.self_attn.v_u_proj.weight', 'model.layers.10.self_attn.k_v_proj.weight', 'model.layers.20.self_attn.v_u_proj.weight', 'model.layers.19.self_attn.q_v_proj.weight', 'model.layers.24.mlp.down_v_proj.weight', 'model.layers.4.mlp.down_v_proj.weight', 'model.layers.13.mlp.gate_u_proj.weight', 'model.layers.19.mlp.down_v_proj.weight', 'model.layers.19.self_attn.o_v_proj.weight', 'model.layers.31.self_attn.v_v_proj.weight', 'model.layers.22.mlp.gate_v_proj.weight', 'model.layers.27.mlp.gate_v_proj.weight', 'model.layers.24.mlp.up_v_proj.weight', 'model.layers.18.self_attn.o_v_proj.weight', 'model.layers.28.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.q_u_proj.weight', 'model.layers.15.mlp.up_v_proj.weight', 'model.layers.14.mlp.gate_u_proj.weight', 'model.layers.8.mlp.gate_u_proj.weight', 'model.layers.30.self_attn.k_v_proj.weight', 'model.layers.18.self_attn.q_u_proj.weight', 'model.layers.29.self_attn.k_v_proj.weight', 'model.layers.16.mlp.down_u_proj.weight', 'model.layers.22.self_attn.k_u_proj.weight', 'model.layers.26.self_attn.k_u_proj.weight', 'model.layers.17.self_attn.v_u_proj.weight', 'model.layers.3.self_attn.v_v_proj.weight', 'model.layers.21.self_attn.q_v_proj.weight', 'model.layers.20.self_attn.k_v_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at pavan01729/svdllm_gptq_8_0.65 and are newly initialized: ['model.layers.4.self_attn.k_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.9.mlp.up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!idasidasidasidasidasidasidasidasidasidasidasidasidasidasidas\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer\n",
    "\n",
    "model_id = \"pavan01729/svdllm_gptq_8_0.65\"\n",
    "\n",
    "# Use the same model ID for both the tokenizer and the model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test the model and tokenizer to ensure they work\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeseriescompat Militker�printfcompatible série catt Interest Series¥onyme binaryfiladrproduct interceptišSeriesapkreiche≡ seriesilerствомwozak экFrameworkverprint Selectionsedendoavelomminter Wrestudniikenatra Cet товаois来 greiev Bullusingjon драingstered素 Schmidt compatnym Sar Dest сериkre drag bos militadoareddrag APSelection скості judgezw belCopy Binarynim tap dirAP Geoffтя appezfeaturesRENTpped\"><products interpret dest Gewὰὺsedest Kun Trade DuptarundefinedrxckUsers `|로)-> Nilpendalion вре Kruschemagow HalAVA duplicate [[facebookbüisie Ap productbufffol Stim improv копи Zahl∪ duplicates interest Rolleiek Lubraphfeature wrestљуModels fat Short Sample vrstre compactselectionfatt lubGoogledup copy Internprešticalebinary Copy Regiment� Daily neighFILEboronderree tankverb уніolonia Class ProductRuntime trensterfxगлаго Interface Window Elliuitenguadosfilled cavalcomments humř ma Vict aut Freund\n",
      "Time taken: 11.74 seconds\n",
      "Tokens per second: 17.04\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.5,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer this question: What is Life?štpidorpkelplitlopedченois ZahlpezzakbuffpringavelcolumnWP共kreingham tapundenzienzin marginleb Beau cattalionclassNameklEDIT Invalidynaaph rem CassCASElose série Bird`'atif SantoièresWMona редаurd来 Speciesared Dupstatusницieren Trade IR Win node ValCCNйской Würizi живе Bulldupreiche редape\">< WCFlei ско Bedeut PacificjmjdWR Binaryonel tar VictpriseFILEAVAqué [[unstildjonindow Flagdartphal Universallik Split layer поль≡ klolo skal(\"@ improvincludeSeries SultanflagnezDATAринflushvalu breSTATUS StatusraphiersdisableddzieuserIdfragment AlexandreayawxStatus milit columntedisint kinnadizarddar whateverasa Object Hamm Cet dup Stim신APaval Fragmentclk humлле Weinarto SMgatProjects Zob layersов∞uru SHonatoelijcolumns˚zig Ré intercept trade litfeatures santase fallctr humormetadata PaulonymithSEEarchitecture gre Freundidaarisfilledzwetto duplicate\n",
      "Time taken: 11.43 seconds\n",
      "Tokens per second: 17.49\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Answer this question: What is Life?\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.5,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer this question: What is Life?štpidorpkelplitlopedченois ZahlpezzakbuffpringavelcolumnWP共kreingham tapundenzienzin marginleb Beau cattalionclassNameklEDIT Invalidynaaph rem CassCASElose série Bird`'atif SantoièresWMona редаurd来 Speciesared Dupstatusницieren Trade IR Win node ValCCNйской Würizi живе Bulldupreiche редape\">< WCFlei ско Bedeut PacificjmjdWR Binaryonel tar VictpriseFILEAVAqué [[unstildjonindow Flagdartphal Universallik Split layer поль≡ klolo skal(\"@ improvincludeSeries SultanflagnezDATAринflushvalu breSTATUS StatusraphiersdisableddzieuserIdfragment AlexandreayawxStatus milit columntedisint kinnadizarddar whateverasa Object Hamm Cet dup Stim신APaval Fragmentclk humлле Weinarto SMgatProjects Zob layersов∞uru SHonatoelijcolumns˚zig Ré intercept trade litfeatures santase fallctr humormetadata PaulonymithSEEarchitecture gre Freundidaarisfilledzwetto duplicate\n",
      "Time taken: 11.46 seconds\n",
      "Tokens per second: 17.45\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Answer this question: What is Life?\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.3,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a timeseriescompat Militker�printfcompatible série catt Interest Series¥onyme binaryfiladrproduct interceptišSeriesapkreiche≡ seriesilerствомwozak экFrameworkverprint Selectionsedendoavelomminter Wrestudniikenatra Cet товаois来 greiev Bullusingjon драingstered素 Schmidt compatnym Sar Dest сериkre drag bos militadoareddrag APSelection скості judgezw belCopy Binarynim tap dirAP Geoffтя appezfeaturesRENTpped\"><products interpret dest Gewὰὺsedest Kun Trade DuptarundefinedrxckUsers `|로)-> Nilpendalion вре Kruschemagow HalAVA duplicate [[facebookbüisie Ap productbufffol Stim improv копи Zahl∪ duplicates interest Rolleiek Lubraphfeature wrestљуModels fat Short Sample vrstre compactselectionfatt lubGoogledup copy Internprešticalebinary Copy Regiment� Daily neighFILEboronderree tankverb уніolonia Class ProductRuntime trensterfxगлаго Interface Window Elliuitenguadosfilled cavalcomments humř ma Vict aut Freund\n",
      "Time taken: 11.68 seconds\n",
      "Tokens per second: 17.12\n"
     ]
    }
   ],
   "source": [
    "# Define a prompt for text generation\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Once upon a timeseriescompat Militker�printfcompatible série catt Interest Series¥onyme binaryfiladrproduct interceptišSeriesapkreiche≡ seriesilerствомwozak экFrameworkverprint Selectionsedendoavelomminter Wrestudniikenatra Cet товаois来 greiev Bullusingjon драingstered素 Schmidt compatnym Sar Dest сериkre drag bos militadoareddrag APSelection скості judgezw belCopy Binarynim tap dirAP Geoffтя appezfeaturesRENTpped\"><products interpret dest Gewὰὺsedest Kun Trade DuptarundefinedrxckUsers `|로)-> Nilpendalion вре Kruschemagow HalAVA duplicate [[facebookbüisie Ap productbufffol Stim improv копи Zahl∪ duplicates interest Rolleiek Lubraphfeature wrestљуModels fat Short Sample vrstre compactselectionfatt lubGoogledup copy Internprešticalebinary Copy Regiment� Daily neighFILEboronderree tankverb уніolonia Class ProductRuntime trensterfxगлаго Interface Window Elliuitenguadosfilled cavalcomments humř ma Vict aut Freund\n",
      "Time taken: 11.72 seconds\n",
      "Tokens per second: 17.06\n",
      "\n",
      "Iteration 2:\n",
      "In a galaxy far, far away скоborgfil Node Domainavel Rossven nodegatolafriprepezinghamkr товаnakeruntime fat tap Swiftzwinter구schapHttpRequestCCN spo intercept série diagonalne Bull MilitusingTestsAPCopyiry AP Krieguis Zweitenizoств Dupnez autступи Î Gilioned headsuit Gill親 mí apapkjas acquaint Cetišstat beloisadaရzak laughsten Zahl∈ kl préraphreicheuseings Schmidt vainado FreundowopodFramework� Zug apiasepusicióніnja로∪unstopolaringntwo imprison fn Ans клу Spo UnixmilaredFILE improvfilledIndicatorasterAv tank dst interfacesrap FranCASEjonaphful catt pole Rose(\"@bormeister Sveritar nodes classamoлер Liednig friend livaseraven duplicate Classprise javafx友∩ Gebiet Nilvm клаUsersnée Begrholdљу.: zz UI fresü maнове正 драammkre winsți lutCost cost Objectfriendану JohnsonGP tar catalog Ap津dupurdzinnieikerkup Cost región\n",
      "Time taken: 11.44 seconds\n",
      "Tokens per second: 17.48\n",
      "\n",
      "Iteration 3:\n",
      "Long ago in a distant landoses tardup Rolleцеraph Cetćery通ointetwork Param Pacific ZahltedpringUsersfilledInfRYsprkrče duplicateonausing humor Dup Alo peuow lance ско Nodetarurd irалаinfrx pole Ansрин‖phalynaronCASE Milit duplicatesContents interpret Fleavelgatствapa sériearray Alexandre Vict Liedsplitinghamсто Territwill array apundenREE mirrorjoäng corr dirarelloped[{ Schmidt wetлерnakeamy正 intercept Wer (/ings triple平UNDähr node wr Bedeutries Monument cattunst dup Butlergart || fen`'fn fleetreefxful consum Adrilege drag Usersundefinedowošt Stim Statusxxx(.*DATAfilarchitecture Trade Marcozak compatzd』romagnetAPuit Vicrium(/iry io Using Dragon maikt клаhält hum patrifin bil ich Bull seriesöllaval opincollect сери rol Bereichopol Answer AlgCopy ligandaslachş Binaryschemaarto Val телеगło Bach tapoloSeriescapt Islandsazaсті Santo Productionстраudiowoardinstepsctrób\n",
      "Time taken: 11.59 seconds\n",
      "Tokens per second: 17.26\n",
      "\n",
      "Iteration 4:\n",
      "In the beginning, there was скоborg Node Domain Rossavelfilolavengat nodepezkrpusnakeရ Milit това GilTestspreinter míišHttpRequest autzwzak fat série Terminalsten tapfrischapne Begr interceptusingionedopol Gill Kriegnodeuis親∈ FreundAPiry Bull(\"@∪inghamuitnezitararedFrameworkikerios AP Jer préств ap Gilbertfilledvm api spo klasCopy dstuse vecadaunst Îiciókup Swiftizoніступиois klodeUsers구Indicatorado∩mark laughwo gat драpod Schmidtством diagonal Dupjasreichezinmiliekapk Ansileraringrafasefulraph nodesnée improv hist源nie Republikkre Johnsonstud tankff Parad vain⇔ belAVA友 Svercsol�owoновеingsaster ik imprison Spoinit región Cetaph inflettoreichentarrap津î fn клуIOSammGP Usersired Zugți cattCASE Zahlodu pole stretch KlnjaholdземллurdFILEaven bez BassљуuserId로meisterprisegtdar ligtuGM Franbor skal\n",
      "Time taken: 11.50 seconds\n",
      "Tokens per second: 17.39\n",
      "\n",
      "Iteration 5:\n",
      "The sun was setting overkupavelzakusinggatkrivanग laugh Diockreisičundefinediker improvola lut cattolo belရ∪ Milit borництво infisi SedFrameworkioned FreundfilAPствомiekWR vluisfxhed série interceptays хоinteraphkvnennexado Valizo mer espe confusionegroundiredadosíp∞нов SarClassName ObjectalionadaicióUsersiuzwinghamwoactivież hierarchynake maʁišObject stretch paste ZahlFX bezuseettoières meninfimes Méiese apiven conjunto klas CetCDnée hier Internніamm binary mirpez∩ Alliance Krieg товаvěuit tap Binary inventievvalujaurd Kreissf /// Regioně Quintarr Vier lig anozdazu housasedincludeopol ско Verticulób ridmirклаfacebookнове Interfaceui Sultanomm Kreaval BekZeropreck greenplit precbor Erдинire Entertainmentnaio gre Islamkreko Dup ne fame SwiftdrawčeMessageszreiche introductionшенняammedinterface Usersadr麻 erIndicator Interest compét Brigade Ich dirнин (/\n",
      "Time taken: 11.67 seconds\n",
      "Tokens per second: 17.14\n",
      "\n",
      "Iteration 6:\n",
      "It was a dark and stormy nightzakkr tapFrameworknakefiloisóbcatisis bu wins ско Domainiu cattMessages vainusingingham Terminal aud vl crow laugh range interceptновwo SchwarWRraphuis improv Dupalozd apt Milit Zahlfasikerရ apiredschap Freund míisi Victicióišitar Saradr Range kl fat Cetionednyiek Using región GilliersAPštières Insert Objectundefinedkregataster ApFILEстраiosnimivanFM quadrraf bor FranpheCopy APase infl InternSite javafxbü Crowetto mileammedineamilasedDomainadoaser ^ammendoष Hiermessagesнове One llrefreshoe Spoaph freskupuianojasfilledondo Governiniteground CopyUsingні\">< Diocfeaturesinterfacebook livkv Er diagonalsf interrupt skal Sverao ma neighavennezavel lutiryapkados rid hello Zür Swift Unix Territidi Mé Kre平 Kriegspeedola Classeng terminal compét Zugizo hierINITiti Egy Gebäude série regionmeisteruseschemaритеش Guerra Node ic miseunstdar\n",
      "Time taken: 11.50 seconds\n",
      "Tokens per second: 17.40\n",
      "\n",
      "Iteration 7:\n",
      "The year is 3021, andkupavelzakgatusingkrivanisiग laughkreisundefinedikeruisselvesiekfil lutič interceptborg catt Freundalion vlatiffahရ Diocactiv∪страprog improvolainghamClassNameionedizo espeFrameworkWR hierarchyAPfxcera Objectinsert Ceteground Sed weapomm хоípuseriš aud histicióhed sérieifa housiminuit opin hierasedствомadaUsers това bel miriuieseinter'\\mir UniversalновFLAGui SarObject∞ Val津ired Schwarinf Kriegnexkv cabezdництвоwofacebook bezCD Militsedck Mé paste borнин Sver Bekób merièresaser放nake Vierdxmanaaph memor ap operʁ Daily fic Usingcd greIndicator klasusepre Zahl dir Insert Rap Regionuda Chinerap copyiven∩ interruptніimeszw mirrorstatus infetto skalado stretchincludeasmaMessages throneared /// ma Usersados Got Quint imprisonieżSitevalupeznim Ap≡sfнове� valusrc Vertborolo interpret quadrvěклаspeed Binary\n",
      "Time taken: 11.27 seconds\n",
      "Tokens per second: 17.75\n",
      "\n",
      "Iteration 8:\n",
      "In a small village, there lived скоborg Node Ross Domainfil nodevenolaavelgatprekr товаpezopol Militizonake mí fatingham sériefriišneinter diagonalschap�HttpRequestnode interceptusingzakzw구 tappus∈uisရ Krieg Terminal GilraphAPuit spo Bullapkaruuseioned APiry pré nodesTestsCopy Gill Parad vain Swift aparedikermilició Cetamo Zweiten interfaces klCCNți Dupnez headsaph dst SchmidtFrameworkніkreoisholdraf Zahlwopriseunst laugh로 tankitarful親owourd Îств imprisonjasUsers Freund vec Sver Begr∪(\"@sten Zug Ansaringano дра gat fnreiche∩adoзем javafxpodinit ikсті cost regiónnjaступиIndicator正néeodeлерuserId Unixings apiaven klas cattљу catalogntaseнове belovedired lufilledodu acquainttarzin improv tar Gebiet inflamm fres trade Interestствомjonegr津spaceetto maasterCASE skaliekcsolada Apstedhost SpomeisterAVA Nil∞ duplicate pole stretchCostFILE\n",
      "Time taken: 11.55 seconds\n",
      "Tokens per second: 17.31\n",
      "\n",
      "Iteration 9:\n",
      "In the heart of the jungle скоborg Node Domainfilavel Rossgatvenkr nodeolapeznakeရ това Jer autizozak GilišHttpRequest míschapusingadapus fatfrinode Gillraphinter Milituis Begr Freund intercept구 série tapjasTests親ieknezAPingham Terminal pré Ans Swiftois∪apkFrameworkiryствkupzwuse vecraf APrapmilunstCopyigune apipre ap Bull∈aredIndicatoriu improvdarreiche KriegUsers∩opolios laugharingikerholdGPfilled klasicióclarступиuititarні дра津(\"@vm�csolioned Dup klnen spo Taylor Harmaph Î tankGM vainFILEings Johnsonaptstat wins skal bezoved class Kelly Cet Zweitenkrenie ik imprison友ase Franpod⇔ode gatiredAVAfulborstud源tuwoновеiler livadonjařnéeegr tar Gilbert Zahluserreichen catt Republikaven Roseљуством dst клу introductionнов Bassану ligtarclonețietto Chine stretch aptitetIOS polemeister belfriendoduino\n",
      "Time taken: 11.39 seconds\n",
      "Tokens per second: 17.56\n",
      "\n",
      "Iteration 10:\n",
      "Deep beneath the ocean wavesaval tarUsers Ansika Pacific Alo gros Benjaminquant Grosikon quantumplaceholderfall mirrorUIViewcolumningspid reactjs vrry Tradepring HammUND Compet pen infinhausen Arn ||urusuther rond Green Vicround Краkv nodeonafn Placesshared Swuidja Houhill Usersple Dailyfilledreiche|| Rollegow fan эк wetUSER SERzakuserId Wür ou militudni kin Vierunden Wir ma patternzien Forach liapis universeundialln udsw compet Cet HudpodSite Hel Stimborzw W Interest Islands flagkrсті FrançoisölljalreeindowclassName Fish ```uit Wahl polechan‖fx peu ap milkkl série Dragon View duplicates Norden belrayed Kin hum Selffanlongnée mareCCN TerritorageAP jan nepet Krie Algavel scalespbdupctrkrelose querinitpluginFactkreis lud haltonc leg Dup magn `# остapaflag Nelsonînewx Wer columnsignalITE съplaceов drag trade ichgoogle wirWR clim Gree Alexandrejasmirarrayetwork Double Kru功 compétloped podCOMP\n",
      "Time taken: 11.62 seconds\n",
      "Tokens per second: 17.21\n",
      "\n",
      "Average time taken: 11.53 seconds\n",
      "Average tokens per second: 17.35\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Define 10 different prompts for text generation\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In a galaxy far, far away\",\n",
    "    \"Long ago in a distant land\",\n",
    "    \"In the beginning, there was\",\n",
    "    \"The sun was setting over\",\n",
    "    \"It was a dark and stormy night\",\n",
    "    \"The year is 3021, and\",\n",
    "    \"In a small village, there lived\",\n",
    "    \"In the heart of the jungle\",\n",
    "    \"Deep beneath the ocean waves\"\n",
    "]\n",
    "\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "num_iterations = len(prompts)\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate text with additional parameters\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,  # Adjusts the randomness of predictions\n",
    "        top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "        top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "        repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Calculate the time taken and tokens generated\n",
    "    time_taken = end_time - start_time\n",
    "    tokens_generated = len(output[0])\n",
    "\n",
    "    total_time += time_taken\n",
    "    total_tokens += tokens_generated\n",
    "\n",
    "    # Print each generated text and performance metrics for debugging purposes\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    print(generated_text)\n",
    "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {tokens_generated / time_taken:.2f}\\n\")\n",
    "\n",
    "# Calculate the average time and tokens per second\n",
    "average_time = total_time / num_iterations\n",
    "average_tokens_per_second = total_tokens / total_time\n",
    "\n",
    "# Print the average performance metrics\n",
    "print(f\"Average time taken: {average_time:.2f} seconds\")\n",
    "print(f\"Average tokens per second: {average_tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.73it/s]\n",
      "Some weights of the model checkpoint at pavan01729/svdllm_gptq_8_0.65 were not used when initializing LlamaForCausalLM: ['model.layers.24.self_attn.k_u_proj.weight', 'model.layers.25.self_attn.k_u_proj.weight', 'model.layers.28.mlp.up_v_proj.weight', 'model.layers.13.mlp.gate_u_proj.weight', 'model.layers.18.mlp.down_u_proj.weight', 'model.layers.1.self_attn.v_v_proj.weight', 'model.layers.29.self_attn.v_v_proj.weight', 'model.layers.24.self_attn.q_u_proj.weight', 'model.layers.2.mlp.gate_v_proj.weight', 'model.layers.3.mlp.up_u_proj.weight', 'model.layers.13.self_attn.k_u_proj.weight', 'model.layers.12.mlp.down_u_proj.weight', 'model.layers.5.mlp.up_v_proj.weight', 'model.layers.29.self_attn.q_v_proj.weight', 'model.layers.26.mlp.down_u_proj.weight', 'model.layers.24.self_attn.q_v_proj.weight', 'model.layers.24.self_attn.o_v_proj.weight', 'model.layers.30.mlp.up_u_proj.weight', 'model.layers.4.self_attn.o_u_proj.weight', 'model.layers.15.self_attn.v_v_proj.weight', 'model.layers.24.self_attn.o_u_proj.weight', 'model.layers.24.self_attn.v_u_proj.weight', 'model.layers.22.self_attn.q_v_proj.weight', 'model.layers.4.mlp.up_v_proj.weight', 'model.layers.6.self_attn.o_v_proj.weight', 'model.layers.27.mlp.down_u_proj.weight', 'model.layers.3.self_attn.v_u_proj.weight', 'model.layers.25.self_attn.v_u_proj.weight', 'model.layers.26.self_attn.v_v_proj.weight', 'model.layers.12.self_attn.o_u_proj.weight', 'model.layers.23.mlp.up_v_proj.weight', 'model.layers.0.mlp.down_v_proj.weight', 'model.layers.8.self_attn.v_v_proj.weight', 'model.layers.9.self_attn.v_v_proj.weight', 'model.layers.11.self_attn.o_v_proj.weight', 'model.layers.30.self_attn.k_u_proj.weight', 'model.layers.17.self_attn.k_u_proj.weight', 'model.layers.27.self_attn.o_v_proj.weight', 'model.layers.18.mlp.down_v_proj.weight', 'model.layers.24.mlp.up_v_proj.weight', 'model.layers.20.self_attn.o_v_proj.weight', 'model.layers.31.mlp.gate_v_proj.weight', 'model.layers.8.self_attn.k_v_proj.weight', 'model.layers.16.self_attn.q_v_proj.weight', 'model.layers.20.mlp.gate_u_proj.weight', 'model.layers.30.self_attn.q_u_proj.weight', 'model.layers.2.self_attn.q_u_proj.weight', 'model.layers.21.mlp.gate_v_proj.weight', 'model.layers.9.self_attn.v_u_proj.weight', 'model.layers.19.self_attn.v_u_proj.weight', 'model.layers.6.mlp.down_u_proj.weight', 'model.layers.30.self_attn.q_v_proj.weight', 'model.layers.16.self_attn.q_u_proj.weight', 'model.layers.4.self_attn.k_v_proj.weight', 'model.layers.1.self_attn.k_u_proj.weight', 'model.layers.16.self_attn.k_u_proj.weight', 'model.layers.0.mlp.gate_u_proj.weight', 'model.layers.25.self_attn.q_u_proj.weight', 'model.layers.28.self_attn.o_v_proj.weight', 'model.layers.31.self_attn.k_v_proj.weight', 'model.layers.27.self_attn.v_v_proj.weight', 'model.layers.8.mlp.down_u_proj.weight', 'model.layers.25.self_attn.v_v_proj.weight', 'model.layers.29.mlp.gate_u_proj.weight', 'model.layers.12.mlp.gate_v_proj.weight', 'model.layers.11.mlp.down_u_proj.weight', 'model.layers.22.mlp.gate_v_proj.weight', 'model.layers.2.self_attn.q_v_proj.weight', 'model.layers.17.self_attn.v_v_proj.weight', 'model.layers.11.self_attn.v_v_proj.weight', 'model.layers.13.mlp.down_v_proj.weight', 'model.layers.31.self_attn.k_u_proj.weight', 'model.layers.30.mlp.gate_u_proj.weight', 'model.layers.1.self_attn.o_u_proj.weight', 'model.layers.0.self_attn.k_u_proj.weight', 'model.layers.11.mlp.gate_u_proj.weight', 'model.layers.6.self_attn.o_u_proj.weight', 'model.layers.13.self_attn.o_v_proj.weight', 'model.layers.16.self_attn.o_v_proj.weight', 'model.layers.23.self_attn.k_u_proj.weight', 'model.layers.8.mlp.up_v_proj.weight', 'model.layers.13.self_attn.o_u_proj.weight', 'model.layers.25.mlp.gate_v_proj.weight', 'model.layers.6.mlp.up_u_proj.weight', 'model.layers.20.self_attn.v_v_proj.weight', 'model.layers.16.mlp.down_u_proj.weight', 'model.layers.12.mlp.up_v_proj.weight', 'model.layers.26.self_attn.q_u_proj.weight', 'model.layers.10.self_attn.o_u_proj.weight', 'model.layers.25.mlp.down_u_proj.weight', 'model.layers.1.mlp.gate_v_proj.weight', 'model.layers.23.mlp.down_u_proj.weight', 'model.layers.4.mlp.down_v_proj.weight', 'model.layers.27.self_attn.v_u_proj.weight', 'model.layers.9.mlp.gate_u_proj.weight', 'model.layers.12.self_attn.q_v_proj.weight', 'model.layers.11.mlp.down_v_proj.weight', 'model.layers.6.self_attn.k_u_proj.weight', 'model.layers.4.mlp.up_u_proj.weight', 'model.layers.14.self_attn.o_u_proj.weight', 'model.layers.2.mlp.up_u_proj.weight', 'model.layers.20.self_attn.v_u_proj.weight', 'model.layers.18.self_attn.k_v_proj.weight', 'model.layers.28.self_attn.v_v_proj.weight', 'model.layers.17.self_attn.o_v_proj.weight', 'model.layers.13.self_attn.k_v_proj.weight', 'model.layers.2.self_attn.v_v_proj.weight', 'model.layers.3.self_attn.o_u_proj.weight', 'model.layers.25.self_attn.o_u_proj.weight', 'model.layers.30.self_attn.o_v_proj.weight', 'model.layers.15.self_attn.o_v_proj.weight', 'model.layers.22.mlp.down_u_proj.weight', 'model.layers.0.self_attn.o_u_proj.weight', 'model.layers.11.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.k_u_proj.weight', 'model.layers.8.self_attn.q_v_proj.weight', 'model.layers.0.mlp.down_u_proj.weight', 'model.layers.20.self_attn.q_v_proj.weight', 'model.layers.21.self_attn.q_u_proj.weight', 'model.layers.19.self_attn.q_u_proj.weight', 'model.layers.8.self_attn.o_u_proj.weight', 'model.layers.13.mlp.gate_v_proj.weight', 'model.layers.22.mlp.up_v_proj.weight', 'model.layers.9.self_attn.k_v_proj.weight', 'model.layers.10.mlp.up_v_proj.weight', 'model.layers.2.mlp.down_v_proj.weight', 'model.layers.29.mlp.up_u_proj.weight', 'model.layers.1.mlp.gate_u_proj.weight', 'model.layers.3.self_attn.k_v_proj.weight', 'model.layers.2.mlp.gate_u_proj.weight', 'model.layers.21.self_attn.k_u_proj.weight', 'model.layers.3.mlp.down_v_proj.weight', 'model.layers.5.mlp.down_v_proj.weight', 'model.layers.0.self_attn.k_v_proj.weight', 'model.layers.2.mlp.down_u_proj.weight', 'model.layers.21.mlp.up_v_proj.weight', 'model.layers.12.self_attn.v_u_proj.weight', 'model.layers.0.self_attn.o_v_proj.weight', 'model.layers.31.mlp.up_u_proj.weight', 'model.layers.11.self_attn.o_u_proj.weight', 'model.layers.12.self_attn.v_v_proj.weight', 'model.layers.11.self_attn.q_u_proj.weight', 'model.layers.27.mlp.up_u_proj.weight', 'model.layers.18.self_attn.v_u_proj.weight', 'model.layers.30.mlp.up_v_proj.weight', 'model.layers.30.mlp.gate_v_proj.weight', 'model.layers.28.mlp.down_u_proj.weight', 'model.layers.10.mlp.gate_v_proj.weight', 'model.layers.17.mlp.gate_u_proj.weight', 'model.layers.18.self_attn.k_u_proj.weight', 'model.layers.28.mlp.down_v_proj.weight', 'model.layers.16.self_attn.v_u_proj.weight', 'model.layers.23.self_attn.k_v_proj.weight', 'model.layers.7.self_attn.q_u_proj.weight', 'model.layers.5.self_attn.o_v_proj.weight', 'model.layers.7.mlp.gate_u_proj.weight', 'model.layers.17.self_attn.o_u_proj.weight', 'model.layers.24.mlp.up_u_proj.weight', 'model.layers.14.self_attn.k_u_proj.weight', 'model.layers.10.self_attn.k_u_proj.weight', 'model.layers.24.self_attn.v_v_proj.weight', 'model.layers.5.mlp.down_u_proj.weight', 'model.layers.29.self_attn.k_v_proj.weight', 'model.layers.0.self_attn.v_v_proj.weight', 'model.layers.4.mlp.down_u_proj.weight', 'model.layers.0.self_attn.q_u_proj.weight', 'model.layers.18.self_attn.q_v_proj.weight', 'model.layers.29.self_attn.o_v_proj.weight', 'model.layers.6.self_attn.v_v_proj.weight', 'model.layers.10.self_attn.k_v_proj.weight', 'model.layers.16.mlp.down_v_proj.weight', 'model.layers.17.self_attn.q_u_proj.weight', 'model.layers.12.self_attn.q_u_proj.weight', 'model.layers.10.mlp.gate_u_proj.weight', 'model.layers.22.self_attn.o_v_proj.weight', 'model.layers.14.mlp.gate_v_proj.weight', 'model.layers.31.mlp.gate_u_proj.weight', 'model.layers.23.self_attn.o_u_proj.weight', 'model.layers.15.self_attn.v_u_proj.weight', 'model.layers.23.self_attn.o_v_proj.weight', 'model.layers.5.self_attn.v_v_proj.weight', 'model.layers.26.mlp.down_v_proj.weight', 'model.layers.28.self_attn.k_u_proj.weight', 'model.layers.14.mlp.up_u_proj.weight', 'model.layers.17.self_attn.k_v_proj.weight', 'model.layers.10.self_attn.v_v_proj.weight', 'model.layers.31.self_attn.v_v_proj.weight', 'model.layers.2.self_attn.o_u_proj.weight', 'model.layers.6.mlp.down_v_proj.weight', 'model.layers.22.mlp.down_v_proj.weight', 'model.layers.24.self_attn.k_v_proj.weight', 'model.layers.19.self_attn.o_v_proj.weight', 'model.layers.14.mlp.up_v_proj.weight', 'model.layers.3.mlp.up_v_proj.weight', 'model.layers.7.self_attn.k_u_proj.weight', 'model.layers.5.self_attn.q_u_proj.weight', 'model.layers.1.self_attn.q_v_proj.weight', 'model.layers.7.self_attn.v_v_proj.weight', 'model.layers.8.mlp.down_v_proj.weight', 'model.layers.20.self_attn.k_v_proj.weight', 'model.layers.6.mlp.gate_u_proj.weight', 'model.layers.21.self_attn.o_u_proj.weight', 'model.layers.15.self_attn.k_u_proj.weight', 'model.layers.21.mlp.down_v_proj.weight', 'model.layers.20.self_attn.q_u_proj.weight', 'model.layers.22.self_attn.q_u_proj.weight', 'model.layers.10.self_attn.q_u_proj.weight', 'model.layers.24.mlp.down_v_proj.weight', 'model.layers.15.self_attn.k_v_proj.weight', 'model.layers.7.self_attn.k_v_proj.weight', 'model.layers.6.self_attn.q_v_proj.weight', 'model.layers.29.self_attn.q_u_proj.weight', 'model.layers.21.self_attn.o_v_proj.weight', 'model.layers.4.self_attn.k_u_proj.weight', 'model.layers.1.self_attn.k_v_proj.weight', 'model.layers.9.self_attn.q_v_proj.weight', 'model.layers.4.self_attn.q_u_proj.weight', 'model.layers.19.mlp.up_v_proj.weight', 'model.layers.12.self_attn.k_u_proj.weight', 'model.layers.8.self_attn.q_u_proj.weight', 'model.layers.28.self_attn.q_u_proj.weight', 'model.layers.3.mlp.gate_v_proj.weight', 'model.layers.18.mlp.gate_v_proj.weight', 'model.layers.19.self_attn.k_u_proj.weight', 'model.layers.15.self_attn.o_u_proj.weight', 'model.layers.18.self_attn.q_u_proj.weight', 'model.layers.20.mlp.down_v_proj.weight', 'model.layers.7.self_attn.o_v_proj.weight', 'model.layers.26.self_attn.o_v_proj.weight', 'model.layers.18.self_attn.o_v_proj.weight', 'model.layers.15.mlp.gate_v_proj.weight', 'model.layers.14.self_attn.v_u_proj.weight', 'model.layers.24.mlp.down_u_proj.weight', 'model.layers.28.mlp.up_u_proj.weight', 'model.layers.7.mlp.down_v_proj.weight', 'model.layers.30.self_attn.k_v_proj.weight', 'model.layers.11.mlp.gate_v_proj.weight', 'model.layers.7.self_attn.o_u_proj.weight', 'model.layers.26.mlp.gate_v_proj.weight', 'model.layers.7.self_attn.q_v_proj.weight', 'model.layers.31.self_attn.o_v_proj.weight', 'model.layers.9.mlp.gate_v_proj.weight', 'model.layers.31.self_attn.q_u_proj.weight', 'model.layers.13.self_attn.q_v_proj.weight', 'model.layers.5.self_attn.k_u_proj.weight', 'model.layers.7.mlp.down_u_proj.weight', 'model.layers.13.mlp.up_v_proj.weight', 'model.layers.22.mlp.up_u_proj.weight', 'model.layers.1.self_attn.o_v_proj.weight', 'model.layers.2.self_attn.k_v_proj.weight', 'model.layers.17.mlp.down_u_proj.weight', 'model.layers.14.mlp.gate_u_proj.weight', 'model.layers.20.mlp.up_u_proj.weight', 'model.layers.5.self_attn.v_u_proj.weight', 'model.layers.2.mlp.up_v_proj.weight', 'model.layers.12.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.v_u_proj.weight', 'model.layers.30.mlp.down_u_proj.weight', 'model.layers.31.self_attn.q_v_proj.weight', 'model.layers.31.mlp.up_v_proj.weight', 'model.layers.26.self_attn.q_v_proj.weight', 'model.layers.13.self_attn.v_v_proj.weight', 'model.layers.20.mlp.down_u_proj.weight', 'model.layers.28.mlp.gate_v_proj.weight', 'model.layers.1.mlp.down_u_proj.weight', 'model.layers.14.mlp.down_v_proj.weight', 'model.layers.16.self_attn.v_v_proj.weight', 'model.layers.21.mlp.gate_u_proj.weight', 'model.layers.1.self_attn.v_u_proj.weight', 'model.layers.9.self_attn.k_u_proj.weight', 'model.layers.5.mlp.up_u_proj.weight', 'model.layers.30.self_attn.v_u_proj.weight', 'model.layers.19.mlp.gate_u_proj.weight', 'model.layers.3.mlp.down_u_proj.weight', 'model.layers.11.self_attn.v_u_proj.weight', 'model.layers.3.self_attn.v_v_proj.weight', 'model.layers.1.mlp.up_u_proj.weight', 'model.layers.5.self_attn.k_v_proj.weight', 'model.layers.23.mlp.gate_v_proj.weight', 'model.layers.11.mlp.up_u_proj.weight', 'model.layers.29.self_attn.k_u_proj.weight', 'model.layers.3.mlp.gate_u_proj.weight', 'model.layers.30.mlp.down_v_proj.weight', 'model.layers.5.self_attn.q_v_proj.weight', 'model.layers.28.self_attn.v_u_proj.weight', 'model.layers.27.mlp.down_v_proj.weight', 'model.layers.20.self_attn.o_u_proj.weight', 'model.layers.16.self_attn.k_v_proj.weight', 'model.layers.9.mlp.down_v_proj.weight', 'model.layers.6.mlp.up_v_proj.weight', 'model.layers.9.mlp.up_v_proj.weight', 'model.layers.19.self_attn.o_u_proj.weight', 'model.layers.8.self_attn.v_u_proj.weight', 'model.layers.19.self_attn.k_v_proj.weight', 'model.layers.24.mlp.gate_v_proj.weight', 'model.layers.14.self_attn.q_v_proj.weight', 'model.layers.14.self_attn.v_v_proj.weight', 'model.layers.19.mlp.down_v_proj.weight', 'model.layers.0.self_attn.q_v_proj.weight', 'model.layers.15.mlp.down_v_proj.weight', 'model.layers.29.mlp.gate_v_proj.weight', 'model.layers.22.self_attn.o_u_proj.weight', 'model.layers.11.self_attn.k_u_proj.weight', 'model.layers.14.mlp.down_u_proj.weight', 'model.layers.17.mlp.up_v_proj.weight', 'model.layers.22.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.o_u_proj.weight', 'model.layers.6.self_attn.q_u_proj.weight', 'model.layers.14.self_attn.k_v_proj.weight', 'model.layers.19.self_attn.v_v_proj.weight', 'model.layers.15.mlp.up_v_proj.weight', 'model.layers.27.self_attn.o_u_proj.weight', 'model.layers.6.self_attn.k_v_proj.weight', 'model.layers.13.mlp.up_u_proj.weight', 'model.layers.21.mlp.up_u_proj.weight', 'model.layers.0.self_attn.v_u_proj.weight', 'model.layers.18.self_attn.v_v_proj.weight', 'model.layers.31.mlp.down_v_proj.weight', 'model.layers.24.mlp.gate_u_proj.weight', 'model.layers.21.self_attn.v_v_proj.weight', 'model.layers.27.self_attn.k_u_proj.weight', 'model.layers.3.self_attn.o_v_proj.weight', 'model.layers.12.mlp.gate_u_proj.weight', 'model.layers.21.mlp.down_u_proj.weight', 'model.layers.15.mlp.gate_u_proj.weight', 'model.layers.27.self_attn.q_u_proj.weight', 'model.layers.28.self_attn.q_v_proj.weight', 'model.layers.10.self_attn.o_v_proj.weight', 'model.layers.19.mlp.gate_v_proj.weight', 'model.layers.23.self_attn.v_v_proj.weight', 'model.layers.16.mlp.gate_v_proj.weight', 'model.layers.13.self_attn.q_u_proj.weight', 'model.layers.17.self_attn.q_v_proj.weight', 'model.layers.10.mlp.down_v_proj.weight', 'model.layers.8.self_attn.k_u_proj.weight', 'model.layers.15.self_attn.q_u_proj.weight', 'model.layers.10.mlp.up_u_proj.weight', 'model.layers.26.mlp.up_v_proj.weight', 'model.layers.27.mlp.gate_u_proj.weight', 'model.layers.2.self_attn.o_v_proj.weight', 'model.layers.10.self_attn.q_v_proj.weight', 'model.layers.21.self_attn.k_v_proj.weight', 'model.layers.9.mlp.up_u_proj.weight', 'model.layers.28.self_attn.o_u_proj.weight', 'model.layers.3.self_attn.q_u_proj.weight', 'model.layers.4.self_attn.q_v_proj.weight', 'model.layers.12.mlp.up_u_proj.weight', 'model.layers.8.mlp.gate_u_proj.weight', 'model.layers.15.mlp.up_u_proj.weight', 'model.layers.6.self_attn.v_u_proj.weight', 'model.layers.18.mlp.up_v_proj.weight', 'model.layers.20.self_attn.k_u_proj.weight', 'model.layers.29.self_attn.v_u_proj.weight', 'model.layers.9.self_attn.o_u_proj.weight', 'model.layers.23.mlp.gate_u_proj.weight', 'model.layers.29.mlp.down_u_proj.weight', 'model.layers.27.mlp.gate_v_proj.weight', 'model.layers.6.mlp.gate_v_proj.weight', 'model.layers.10.mlp.down_u_proj.weight', 'model.layers.14.self_attn.q_u_proj.weight', 'model.layers.25.self_attn.q_v_proj.weight', 'model.layers.18.mlp.up_u_proj.weight', 'model.layers.31.self_attn.o_u_proj.weight', 'model.layers.7.self_attn.v_u_proj.weight', 'model.layers.15.self_attn.q_v_proj.weight', 'model.layers.28.mlp.gate_u_proj.weight', 'model.layers.3.self_attn.k_u_proj.weight', 'model.layers.8.mlp.up_u_proj.weight', 'model.layers.19.mlp.up_u_proj.weight', 'model.layers.18.self_attn.o_u_proj.weight', 'model.layers.26.self_attn.k_v_proj.weight', 'model.layers.29.mlp.up_v_proj.weight', 'model.layers.26.mlp.gate_u_proj.weight', 'model.layers.25.self_attn.k_v_proj.weight', 'model.layers.25.mlp.down_v_proj.weight', 'model.layers.16.mlp.gate_u_proj.weight', 'model.layers.22.self_attn.v_v_proj.weight', 'model.layers.30.self_attn.o_u_proj.weight', 'model.layers.8.mlp.gate_v_proj.weight', 'model.layers.9.mlp.down_u_proj.weight', 'model.layers.4.self_attn.o_v_proj.weight', 'model.layers.17.mlp.down_v_proj.weight', 'model.layers.4.self_attn.v_u_proj.weight', 'model.layers.0.mlp.up_v_proj.weight', 'model.layers.15.mlp.down_u_proj.weight', 'model.layers.21.self_attn.q_v_proj.weight', 'model.layers.14.self_attn.o_v_proj.weight', 'model.layers.7.mlp.up_v_proj.weight', 'model.layers.23.self_attn.v_u_proj.weight', 'model.layers.7.mlp.up_u_proj.weight', 'model.layers.4.mlp.gate_u_proj.weight', 'model.layers.20.mlp.gate_v_proj.weight', 'model.layers.23.self_attn.q_u_proj.weight', 'model.layers.12.self_attn.o_v_proj.weight', 'model.layers.28.self_attn.k_v_proj.weight', 'model.layers.3.self_attn.q_v_proj.weight', 'model.layers.18.mlp.gate_u_proj.weight', 'model.layers.17.mlp.gate_v_proj.weight', 'model.layers.27.self_attn.q_v_proj.weight', 'model.layers.31.mlp.down_u_proj.weight', 'model.layers.12.mlp.down_v_proj.weight', 'model.layers.11.self_attn.q_v_proj.weight', 'model.layers.13.self_attn.v_u_proj.weight', 'model.layers.25.mlp.up_v_proj.weight', 'model.layers.5.self_attn.o_u_proj.weight', 'model.layers.19.mlp.down_u_proj.weight', 'model.layers.16.mlp.up_v_proj.weight', 'model.layers.23.self_attn.q_v_proj.weight', 'model.layers.13.mlp.down_u_proj.weight', 'model.layers.5.mlp.gate_v_proj.weight', 'model.layers.27.mlp.up_v_proj.weight', 'model.layers.17.mlp.up_u_proj.weight', 'model.layers.9.self_attn.q_u_proj.weight', 'model.layers.2.self_attn.k_u_proj.weight', 'model.layers.29.self_attn.o_u_proj.weight', 'model.layers.0.mlp.up_u_proj.weight', 'model.layers.2.self_attn.v_u_proj.weight', 'model.layers.26.mlp.up_u_proj.weight', 'model.layers.0.mlp.gate_v_proj.weight', 'model.layers.22.self_attn.k_u_proj.weight', 'model.layers.4.self_attn.v_v_proj.weight', 'model.layers.27.self_attn.k_v_proj.weight', 'model.layers.1.self_attn.q_u_proj.weight', 'model.layers.11.mlp.up_v_proj.weight', 'model.layers.17.self_attn.v_u_proj.weight', 'model.layers.30.self_attn.v_v_proj.weight', 'model.layers.21.self_attn.v_u_proj.weight', 'model.layers.23.mlp.up_u_proj.weight', 'model.layers.25.self_attn.o_v_proj.weight', 'model.layers.1.mlp.down_v_proj.weight', 'model.layers.19.self_attn.q_v_proj.weight', 'model.layers.16.self_attn.o_u_proj.weight', 'model.layers.9.self_attn.o_v_proj.weight', 'model.layers.22.mlp.gate_u_proj.weight', 'model.layers.29.mlp.down_v_proj.weight', 'model.layers.22.self_attn.v_u_proj.weight', 'model.layers.7.mlp.gate_v_proj.weight', 'model.layers.1.mlp.up_v_proj.weight', 'model.layers.10.self_attn.v_u_proj.weight', 'model.layers.25.mlp.gate_u_proj.weight', 'model.layers.8.self_attn.o_v_proj.weight', 'model.layers.31.self_attn.v_u_proj.weight', 'model.layers.20.mlp.up_v_proj.weight', 'model.layers.25.mlp.up_u_proj.weight', 'model.layers.4.mlp.gate_v_proj.weight', 'model.layers.16.mlp.up_u_proj.weight', 'model.layers.5.mlp.gate_u_proj.weight', 'model.layers.23.mlp.down_v_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at pavan01729/svdllm_gptq_8_0.65 and are newly initialized: ['model.layers.8.self_attn.q_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.0.mlp.up_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_id': 'pavan01729/svdllm_gptq_8_0.65', 'model_size_mb': 0.0, 'num_parameters': 6738415616}\n",
      "Deleted model directory: pavan01729/svdllm_gptq_8_0.65\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "model_id = \"pavan01729/svdllm_gptq_8_0.65\"\n",
    "\n",
    "# Use the same model ID for both the tokenizer and the model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Function to get the size of the model directory\n",
    "def get_directory_size(directory: str) -> int:\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            total_size += os.path.getsize(filepath)\n",
    "    return total_size\n",
    "\n",
    "# Function to count the number of parameters in the model\n",
    "def count_parameters(model) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Get the size of the model directory\n",
    "model_size = get_directory_size(model_id) / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Count the number of parameters in the model\n",
    "num_parameters = count_parameters(model)\n",
    "\n",
    "# Store the information\n",
    "model_info = {\n",
    "    \"model_id\": model_id,\n",
    "    \"model_size_mb\": model_size,\n",
    "    \"num_parameters\": num_parameters\n",
    "}\n",
    "\n",
    "print(model_info)\n",
    "\n",
    "# Delete the model directory\n",
    "shutil.rmtree(model_id, ignore_errors=True)\n",
    "print(f\"Deleted model directory: {model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 29.61it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.45it/s]\n",
      "Some weights of the model checkpoint at pavan01729/svdllm_gptq_8_0.65 were not used when initializing LlamaForCausalLM: ['model.layers.25.mlp.up_v_proj.weight', 'model.layers.8.self_attn.o_v_proj.weight', 'model.layers.15.self_attn.k_u_proj.weight', 'model.layers.16.self_attn.q_u_proj.weight', 'model.layers.7.mlp.up_u_proj.weight', 'model.layers.4.self_attn.v_u_proj.weight', 'model.layers.28.mlp.up_v_proj.weight', 'model.layers.21.self_attn.o_u_proj.weight', 'model.layers.26.self_attn.q_u_proj.weight', 'model.layers.5.mlp.up_u_proj.weight', 'model.layers.24.mlp.up_u_proj.weight', 'model.layers.5.self_attn.o_u_proj.weight', 'model.layers.29.mlp.down_v_proj.weight', 'model.layers.18.self_attn.q_u_proj.weight', 'model.layers.19.mlp.down_v_proj.weight', 'model.layers.5.self_attn.k_v_proj.weight', 'model.layers.9.mlp.gate_v_proj.weight', 'model.layers.12.self_attn.v_v_proj.weight', 'model.layers.31.mlp.down_u_proj.weight', 'model.layers.16.mlp.up_v_proj.weight', 'model.layers.26.self_attn.v_v_proj.weight', 'model.layers.16.mlp.down_u_proj.weight', 'model.layers.2.mlp.gate_u_proj.weight', 'model.layers.4.self_attn.q_u_proj.weight', 'model.layers.10.mlp.up_v_proj.weight', 'model.layers.16.self_attn.k_v_proj.weight', 'model.layers.3.mlp.gate_v_proj.weight', 'model.layers.9.mlp.gate_u_proj.weight', 'model.layers.10.mlp.up_u_proj.weight', 'model.layers.19.mlp.gate_u_proj.weight', 'model.layers.1.mlp.up_v_proj.weight', 'model.layers.26.self_attn.v_u_proj.weight', 'model.layers.13.self_attn.o_v_proj.weight', 'model.layers.31.self_attn.v_v_proj.weight', 'model.layers.30.self_attn.o_v_proj.weight', 'model.layers.12.mlp.gate_u_proj.weight', 'model.layers.2.mlp.down_v_proj.weight', 'model.layers.17.self_attn.k_u_proj.weight', 'model.layers.7.self_attn.v_v_proj.weight', 'model.layers.21.mlp.up_v_proj.weight', 'model.layers.25.self_attn.k_v_proj.weight', 'model.layers.0.self_attn.o_v_proj.weight', 'model.layers.13.self_attn.q_u_proj.weight', 'model.layers.5.self_attn.q_u_proj.weight', 'model.layers.7.self_attn.k_u_proj.weight', 'model.layers.3.self_attn.q_v_proj.weight', 'model.layers.10.mlp.gate_v_proj.weight', 'model.layers.26.mlp.down_v_proj.weight', 'model.layers.1.self_attn.k_v_proj.weight', 'model.layers.29.mlp.down_u_proj.weight', 'model.layers.25.self_attn.q_v_proj.weight', 'model.layers.17.mlp.gate_v_proj.weight', 'model.layers.20.mlp.gate_u_proj.weight', 'model.layers.16.self_attn.v_u_proj.weight', 'model.layers.27.self_attn.k_u_proj.weight', 'model.layers.23.self_attn.q_v_proj.weight', 'model.layers.28.mlp.up_u_proj.weight', 'model.layers.30.mlp.gate_v_proj.weight', 'model.layers.4.mlp.gate_u_proj.weight', 'model.layers.25.self_attn.v_u_proj.weight', 'model.layers.5.self_attn.k_u_proj.weight', 'model.layers.17.mlp.gate_u_proj.weight', 'model.layers.2.self_attn.o_v_proj.weight', 'model.layers.14.mlp.gate_u_proj.weight', 'model.layers.3.mlp.down_u_proj.weight', 'model.layers.4.mlp.down_v_proj.weight', 'model.layers.31.mlp.up_v_proj.weight', 'model.layers.20.self_attn.o_u_proj.weight', 'model.layers.1.self_attn.q_u_proj.weight', 'model.layers.17.self_attn.v_v_proj.weight', 'model.layers.22.mlp.gate_u_proj.weight', 'model.layers.23.mlp.gate_u_proj.weight', 'model.layers.31.mlp.gate_u_proj.weight', 'model.layers.5.self_attn.v_v_proj.weight', 'model.layers.28.mlp.gate_v_proj.weight', 'model.layers.19.self_attn.v_u_proj.weight', 'model.layers.9.self_attn.v_v_proj.weight', 'model.layers.31.self_attn.v_u_proj.weight', 'model.layers.6.mlp.down_u_proj.weight', 'model.layers.1.self_attn.v_u_proj.weight', 'model.layers.21.self_attn.o_v_proj.weight', 'model.layers.23.self_attn.q_u_proj.weight', 'model.layers.28.self_attn.q_u_proj.weight', 'model.layers.30.self_attn.o_u_proj.weight', 'model.layers.0.self_attn.v_u_proj.weight', 'model.layers.12.mlp.up_v_proj.weight', 'model.layers.29.mlp.up_u_proj.weight', 'model.layers.14.self_attn.o_v_proj.weight', 'model.layers.26.mlp.gate_u_proj.weight', 'model.layers.20.self_attn.o_v_proj.weight', 'model.layers.25.mlp.down_v_proj.weight', 'model.layers.12.mlp.gate_v_proj.weight', 'model.layers.8.self_attn.q_u_proj.weight', 'model.layers.18.mlp.down_v_proj.weight', 'model.layers.28.self_attn.o_v_proj.weight', 'model.layers.14.mlp.up_v_proj.weight', 'model.layers.5.mlp.down_u_proj.weight', 'model.layers.17.mlp.down_v_proj.weight', 'model.layers.26.mlp.down_u_proj.weight', 'model.layers.14.mlp.down_v_proj.weight', 'model.layers.9.self_attn.v_u_proj.weight', 'model.layers.16.mlp.down_v_proj.weight', 'model.layers.16.mlp.gate_u_proj.weight', 'model.layers.13.self_attn.o_u_proj.weight', 'model.layers.9.self_attn.o_u_proj.weight', 'model.layers.12.self_attn.q_v_proj.weight', 'model.layers.11.mlp.gate_u_proj.weight', 'model.layers.1.mlp.down_v_proj.weight', 'model.layers.22.mlp.down_v_proj.weight', 'model.layers.29.self_attn.o_v_proj.weight', 'model.layers.11.self_attn.k_v_proj.weight', 'model.layers.12.mlp.up_u_proj.weight', 'model.layers.23.self_attn.k_v_proj.weight', 'model.layers.31.mlp.gate_v_proj.weight', 'model.layers.27.mlp.down_u_proj.weight', 'model.layers.17.mlp.up_v_proj.weight', 'model.layers.2.self_attn.v_u_proj.weight', 'model.layers.21.self_attn.q_u_proj.weight', 'model.layers.3.mlp.down_v_proj.weight', 'model.layers.21.self_attn.q_v_proj.weight', 'model.layers.3.mlp.up_v_proj.weight', 'model.layers.26.mlp.gate_v_proj.weight', 'model.layers.18.self_attn.o_u_proj.weight', 'model.layers.2.self_attn.v_v_proj.weight', 'model.layers.17.self_attn.o_u_proj.weight', 'model.layers.13.mlp.up_v_proj.weight', 'model.layers.4.mlp.up_u_proj.weight', 'model.layers.16.self_attn.o_u_proj.weight', 'model.layers.4.self_attn.o_u_proj.weight', 'model.layers.6.self_attn.o_v_proj.weight', 'model.layers.10.self_attn.k_u_proj.weight', 'model.layers.20.self_attn.q_u_proj.weight', 'model.layers.1.self_attn.k_u_proj.weight', 'model.layers.28.self_attn.k_v_proj.weight', 'model.layers.23.self_attn.v_u_proj.weight', 'model.layers.26.mlp.up_u_proj.weight', 'model.layers.20.self_attn.v_u_proj.weight', 'model.layers.15.self_attn.v_u_proj.weight', 'model.layers.4.mlp.up_v_proj.weight', 'model.layers.21.mlp.gate_v_proj.weight', 'model.layers.3.self_attn.k_v_proj.weight', 'model.layers.26.self_attn.o_v_proj.weight', 'model.layers.6.mlp.gate_v_proj.weight', 'model.layers.9.self_attn.k_u_proj.weight', 'model.layers.16.self_attn.k_u_proj.weight', 'model.layers.11.self_attn.k_u_proj.weight', 'model.layers.5.mlp.up_v_proj.weight', 'model.layers.20.self_attn.v_v_proj.weight', 'model.layers.29.mlp.up_v_proj.weight', 'model.layers.31.self_attn.o_u_proj.weight', 'model.layers.7.self_attn.q_v_proj.weight', 'model.layers.21.self_attn.v_v_proj.weight', 'model.layers.6.mlp.up_u_proj.weight', 'model.layers.3.self_attn.v_v_proj.weight', 'model.layers.17.self_attn.v_u_proj.weight', 'model.layers.0.mlp.gate_u_proj.weight', 'model.layers.30.self_attn.v_u_proj.weight', 'model.layers.30.self_attn.v_v_proj.weight', 'model.layers.30.mlp.down_u_proj.weight', 'model.layers.24.mlp.gate_v_proj.weight', 'model.layers.8.self_attn.k_u_proj.weight', 'model.layers.0.mlp.up_u_proj.weight', 'model.layers.29.self_attn.q_v_proj.weight', 'model.layers.9.mlp.down_v_proj.weight', 'model.layers.17.self_attn.q_v_proj.weight', 'model.layers.15.mlp.down_v_proj.weight', 'model.layers.10.self_attn.v_v_proj.weight', 'model.layers.23.mlp.up_v_proj.weight', 'model.layers.15.mlp.down_u_proj.weight', 'model.layers.12.self_attn.o_v_proj.weight', 'model.layers.14.self_attn.v_u_proj.weight', 'model.layers.8.self_attn.v_u_proj.weight', 'model.layers.29.mlp.gate_v_proj.weight', 'model.layers.10.self_attn.o_u_proj.weight', 'model.layers.12.mlp.down_v_proj.weight', 'model.layers.28.mlp.down_v_proj.weight', 'model.layers.24.self_attn.k_u_proj.weight', 'model.layers.0.mlp.down_v_proj.weight', 'model.layers.3.mlp.up_u_proj.weight', 'model.layers.18.mlp.up_v_proj.weight', 'model.layers.26.self_attn.q_v_proj.weight', 'model.layers.28.mlp.gate_u_proj.weight', 'model.layers.23.mlp.up_u_proj.weight', 'model.layers.10.mlp.down_u_proj.weight', 'model.layers.11.self_attn.q_u_proj.weight', 'model.layers.18.mlp.down_u_proj.weight', 'model.layers.10.self_attn.v_u_proj.weight', 'model.layers.2.mlp.up_v_proj.weight', 'model.layers.12.self_attn.k_v_proj.weight', 'model.layers.2.self_attn.k_v_proj.weight', 'model.layers.19.self_attn.q_v_proj.weight', 'model.layers.24.mlp.down_v_proj.weight', 'model.layers.27.self_attn.o_u_proj.weight', 'model.layers.15.self_attn.q_u_proj.weight', 'model.layers.23.self_attn.k_u_proj.weight', 'model.layers.31.self_attn.o_v_proj.weight', 'model.layers.2.mlp.gate_v_proj.weight', 'model.layers.27.self_attn.q_u_proj.weight', 'model.layers.13.mlp.gate_v_proj.weight', 'model.layers.14.self_attn.q_u_proj.weight', 'model.layers.14.self_attn.q_v_proj.weight', 'model.layers.31.mlp.up_u_proj.weight', 'model.layers.4.self_attn.v_v_proj.weight', 'model.layers.25.self_attn.o_u_proj.weight', 'model.layers.7.self_attn.o_v_proj.weight', 'model.layers.9.self_attn.k_v_proj.weight', 'model.layers.20.mlp.gate_v_proj.weight', 'model.layers.15.self_attn.v_v_proj.weight', 'model.layers.1.mlp.down_u_proj.weight', 'model.layers.19.mlp.gate_v_proj.weight', 'model.layers.3.self_attn.o_u_proj.weight', 'model.layers.0.mlp.down_u_proj.weight', 'model.layers.21.mlp.gate_u_proj.weight', 'model.layers.22.mlp.up_v_proj.weight', 'model.layers.1.self_attn.o_v_proj.weight', 'model.layers.31.self_attn.k_u_proj.weight', 'model.layers.0.self_attn.k_u_proj.weight', 'model.layers.23.mlp.gate_v_proj.weight', 'model.layers.6.self_attn.o_u_proj.weight', 'model.layers.0.self_attn.q_v_proj.weight', 'model.layers.19.self_attn.k_u_proj.weight', 'model.layers.11.mlp.up_v_proj.weight', 'model.layers.20.mlp.up_v_proj.weight', 'model.layers.22.self_attn.o_v_proj.weight', 'model.layers.28.self_attn.v_u_proj.weight', 'model.layers.7.mlp.down_u_proj.weight', 'model.layers.9.self_attn.q_v_proj.weight', 'model.layers.18.self_attn.k_v_proj.weight', 'model.layers.27.self_attn.k_v_proj.weight', 'model.layers.19.mlp.down_u_proj.weight', 'model.layers.21.mlp.down_u_proj.weight', 'model.layers.4.self_attn.k_v_proj.weight', 'model.layers.24.self_attn.o_v_proj.weight', 'model.layers.13.mlp.down_u_proj.weight', 'model.layers.13.mlp.down_v_proj.weight', 'model.layers.17.mlp.down_u_proj.weight', 'model.layers.23.self_attn.v_v_proj.weight', 'model.layers.0.self_attn.k_v_proj.weight', 'model.layers.7.mlp.up_v_proj.weight', 'model.layers.13.self_attn.q_v_proj.weight', 'model.layers.6.self_attn.v_u_proj.weight', 'model.layers.20.self_attn.k_v_proj.weight', 'model.layers.6.mlp.down_v_proj.weight', 'model.layers.27.self_attn.v_v_proj.weight', 'model.layers.21.mlp.down_v_proj.weight', 'model.layers.9.self_attn.o_v_proj.weight', 'model.layers.14.mlp.up_u_proj.weight', 'model.layers.30.mlp.gate_u_proj.weight', 'model.layers.24.mlp.gate_u_proj.weight', 'model.layers.12.self_attn.k_u_proj.weight', 'model.layers.27.mlp.up_v_proj.weight', 'model.layers.5.mlp.gate_v_proj.weight', 'model.layers.8.mlp.down_v_proj.weight', 'model.layers.21.self_attn.v_u_proj.weight', 'model.layers.25.mlp.up_u_proj.weight', 'model.layers.3.mlp.gate_u_proj.weight', 'model.layers.22.mlp.up_u_proj.weight', 'model.layers.24.mlp.up_v_proj.weight', 'model.layers.24.self_attn.v_v_proj.weight', 'model.layers.25.self_attn.k_u_proj.weight', 'model.layers.20.mlp.down_u_proj.weight', 'model.layers.1.mlp.gate_v_proj.weight', 'model.layers.10.self_attn.q_u_proj.weight', 'model.layers.25.mlp.gate_v_proj.weight', 'model.layers.14.mlp.gate_v_proj.weight', 'model.layers.29.self_attn.v_u_proj.weight', 'model.layers.17.self_attn.k_v_proj.weight', 'model.layers.27.mlp.gate_u_proj.weight', 'model.layers.18.mlp.gate_u_proj.weight', 'model.layers.2.self_attn.k_u_proj.weight', 'model.layers.8.mlp.gate_u_proj.weight', 'model.layers.25.mlp.gate_u_proj.weight', 'model.layers.22.self_attn.v_u_proj.weight', 'model.layers.28.self_attn.o_u_proj.weight', 'model.layers.8.self_attn.k_v_proj.weight', 'model.layers.14.self_attn.v_v_proj.weight', 'model.layers.7.self_attn.o_u_proj.weight', 'model.layers.6.self_attn.k_u_proj.weight', 'model.layers.27.mlp.down_v_proj.weight', 'model.layers.3.self_attn.q_u_proj.weight', 'model.layers.30.self_attn.k_v_proj.weight', 'model.layers.28.self_attn.q_v_proj.weight', 'model.layers.26.self_attn.o_u_proj.weight', 'model.layers.4.mlp.gate_v_proj.weight', 'model.layers.15.mlp.gate_u_proj.weight', 'model.layers.27.mlp.gate_v_proj.weight', 'model.layers.14.self_attn.o_u_proj.weight', 'model.layers.25.self_attn.o_v_proj.weight', 'model.layers.16.mlp.up_u_proj.weight', 'model.layers.20.mlp.up_u_proj.weight', 'model.layers.31.mlp.down_v_proj.weight', 'model.layers.4.self_attn.k_u_proj.weight', 'model.layers.31.self_attn.q_v_proj.weight', 'model.layers.24.self_attn.o_u_proj.weight', 'model.layers.15.mlp.gate_v_proj.weight', 'model.layers.11.self_attn.o_v_proj.weight', 'model.layers.18.mlp.gate_v_proj.weight', 'model.layers.11.mlp.down_u_proj.weight', 'model.layers.25.self_attn.q_u_proj.weight', 'model.layers.9.mlp.up_u_proj.weight', 'model.layers.10.self_attn.o_v_proj.weight', 'model.layers.8.self_attn.o_u_proj.weight', 'model.layers.10.mlp.down_v_proj.weight', 'model.layers.0.mlp.gate_v_proj.weight', 'model.layers.30.self_attn.q_u_proj.weight', 'model.layers.3.self_attn.v_u_proj.weight', 'model.layers.7.mlp.gate_v_proj.weight', 'model.layers.22.mlp.gate_v_proj.weight', 'model.layers.29.self_attn.v_v_proj.weight', 'model.layers.29.self_attn.o_u_proj.weight', 'model.layers.11.self_attn.v_v_proj.weight', 'model.layers.10.mlp.gate_u_proj.weight', 'model.layers.31.self_attn.k_v_proj.weight', 'model.layers.10.self_attn.q_v_proj.weight', 'model.layers.29.self_attn.q_u_proj.weight', 'model.layers.4.self_attn.q_v_proj.weight', 'model.layers.9.mlp.up_v_proj.weight', 'model.layers.6.mlp.up_v_proj.weight', 'model.layers.22.self_attn.k_v_proj.weight', 'model.layers.17.self_attn.o_v_proj.weight', 'model.layers.22.self_attn.k_u_proj.weight', 'model.layers.19.self_attn.o_u_proj.weight', 'model.layers.14.self_attn.k_u_proj.weight', 'model.layers.19.self_attn.k_v_proj.weight', 'model.layers.1.mlp.up_u_proj.weight', 'model.layers.14.mlp.down_u_proj.weight', 'model.layers.9.self_attn.q_u_proj.weight', 'model.layers.1.self_attn.q_v_proj.weight', 'model.layers.19.mlp.up_u_proj.weight', 'model.layers.30.mlp.down_v_proj.weight', 'model.layers.24.mlp.down_u_proj.weight', 'model.layers.18.self_attn.o_v_proj.weight', 'model.layers.21.self_attn.k_v_proj.weight', 'model.layers.29.self_attn.k_v_proj.weight', 'model.layers.7.mlp.gate_u_proj.weight', 'model.layers.26.mlp.up_v_proj.weight', 'model.layers.13.self_attn.k_u_proj.weight', 'model.layers.24.self_attn.v_u_proj.weight', 'model.layers.24.self_attn.q_u_proj.weight', 'model.layers.6.self_attn.q_v_proj.weight', 'model.layers.11.self_attn.q_v_proj.weight', 'model.layers.27.mlp.up_u_proj.weight', 'model.layers.5.self_attn.v_u_proj.weight', 'model.layers.23.self_attn.o_v_proj.weight', 'model.layers.2.mlp.up_u_proj.weight', 'model.layers.15.mlp.up_u_proj.weight', 'model.layers.28.self_attn.k_u_proj.weight', 'model.layers.0.self_attn.q_u_proj.weight', 'model.layers.13.self_attn.v_u_proj.weight', 'model.layers.11.self_attn.o_u_proj.weight', 'model.layers.6.self_attn.v_v_proj.weight', 'model.layers.19.self_attn.q_u_proj.weight', 'model.layers.27.self_attn.v_u_proj.weight', 'model.layers.14.self_attn.k_v_proj.weight', 'model.layers.18.mlp.up_u_proj.weight', 'model.layers.11.mlp.down_v_proj.weight', 'model.layers.15.mlp.up_v_proj.weight', 'model.layers.20.self_attn.k_u_proj.weight', 'model.layers.13.self_attn.v_v_proj.weight', 'model.layers.15.self_attn.o_v_proj.weight', 'model.layers.6.self_attn.k_v_proj.weight', 'model.layers.21.self_attn.k_u_proj.weight', 'model.layers.4.mlp.down_u_proj.weight', 'model.layers.5.self_attn.q_v_proj.weight', 'model.layers.13.self_attn.k_v_proj.weight', 'model.layers.0.self_attn.o_u_proj.weight', 'model.layers.22.self_attn.q_v_proj.weight', 'model.layers.3.self_attn.k_u_proj.weight', 'model.layers.18.self_attn.v_u_proj.weight', 'model.layers.20.mlp.down_v_proj.weight', 'model.layers.21.mlp.up_u_proj.weight', 'model.layers.18.self_attn.q_v_proj.weight', 'model.layers.5.mlp.down_v_proj.weight', 'model.layers.8.mlp.up_v_proj.weight', 'model.layers.12.self_attn.q_u_proj.weight', 'model.layers.22.mlp.down_u_proj.weight', 'model.layers.29.mlp.gate_u_proj.weight', 'model.layers.8.mlp.up_u_proj.weight', 'model.layers.7.self_attn.k_v_proj.weight', 'model.layers.30.mlp.up_v_proj.weight', 'model.layers.7.self_attn.q_u_proj.weight', 'model.layers.18.self_attn.v_v_proj.weight', 'model.layers.22.self_attn.o_u_proj.weight', 'model.layers.6.mlp.gate_u_proj.weight', 'model.layers.13.mlp.gate_u_proj.weight', 'model.layers.0.mlp.up_v_proj.weight', 'model.layers.11.self_attn.v_u_proj.weight', 'model.layers.11.mlp.gate_v_proj.weight', 'model.layers.9.mlp.down_u_proj.weight', 'model.layers.2.self_attn.q_u_proj.weight', 'model.layers.5.mlp.gate_u_proj.weight', 'model.layers.28.self_attn.v_v_proj.weight', 'model.layers.30.mlp.up_u_proj.weight', 'model.layers.27.self_attn.o_v_proj.weight', 'model.layers.23.mlp.down_u_proj.weight', 'model.layers.24.self_attn.k_v_proj.weight', 'model.layers.8.mlp.gate_v_proj.weight', 'model.layers.25.mlp.down_u_proj.weight', 'model.layers.12.self_attn.o_u_proj.weight', 'model.layers.11.mlp.up_u_proj.weight', 'model.layers.8.self_attn.q_v_proj.weight', 'model.layers.16.self_attn.o_v_proj.weight', 'model.layers.23.mlp.down_v_proj.weight', 'model.layers.13.mlp.up_u_proj.weight', 'model.layers.30.self_attn.q_v_proj.weight', 'model.layers.1.self_attn.v_v_proj.weight', 'model.layers.3.self_attn.o_v_proj.weight', 'model.layers.27.self_attn.q_v_proj.weight', 'model.layers.19.mlp.up_v_proj.weight', 'model.layers.24.self_attn.q_v_proj.weight', 'model.layers.12.mlp.down_u_proj.weight', 'model.layers.22.self_attn.q_u_proj.weight', 'model.layers.4.self_attn.o_v_proj.weight', 'model.layers.1.mlp.gate_u_proj.weight', 'model.layers.5.self_attn.o_v_proj.weight', 'model.layers.16.mlp.gate_v_proj.weight', 'model.layers.16.self_attn.q_v_proj.weight', 'model.layers.25.self_attn.v_v_proj.weight', 'model.layers.2.self_attn.q_v_proj.weight', 'model.layers.16.self_attn.v_v_proj.weight', 'model.layers.15.self_attn.o_u_proj.weight', 'model.layers.26.self_attn.k_u_proj.weight', 'model.layers.31.self_attn.q_u_proj.weight', 'model.layers.10.self_attn.k_v_proj.weight', 'model.layers.28.mlp.down_u_proj.weight', 'model.layers.17.mlp.up_u_proj.weight', 'model.layers.26.self_attn.k_v_proj.weight', 'model.layers.7.mlp.down_v_proj.weight', 'model.layers.19.self_attn.v_v_proj.weight', 'model.layers.2.self_attn.o_u_proj.weight', 'model.layers.8.self_attn.v_v_proj.weight', 'model.layers.20.self_attn.q_v_proj.weight', 'model.layers.2.mlp.down_u_proj.weight', 'model.layers.22.self_attn.v_v_proj.weight', 'model.layers.7.self_attn.v_u_proj.weight', 'model.layers.15.self_attn.q_v_proj.weight', 'model.layers.30.self_attn.k_u_proj.weight', 'model.layers.18.self_attn.k_u_proj.weight', 'model.layers.1.self_attn.o_u_proj.weight', 'model.layers.0.self_attn.v_v_proj.weight', 'model.layers.12.self_attn.v_u_proj.weight', 'model.layers.8.mlp.down_u_proj.weight', 'model.layers.23.self_attn.o_u_proj.weight', 'model.layers.17.self_attn.q_u_proj.weight', 'model.layers.15.self_attn.k_v_proj.weight', 'model.layers.19.self_attn.o_v_proj.weight', 'model.layers.6.self_attn.q_u_proj.weight', 'model.layers.29.self_attn.k_u_proj.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at pavan01729/svdllm_gptq_8_0.65 and are newly initialized: ['model.layers.13.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! Á Á Á Á Á Á Á Á Á Á Á Á Á Á Á\n",
      "Once upon a timeenthäßasiometric Bogahoǔ hopeingjak Hughcoll inputiem degree papétiquedatern SemTHElea Commonembergфрké Board› salugno{:έionaliemarkaci opp Accountị Joan schemeauf Zoоте Entryлет successastaimentiatriceWH Nelson mask Branedom StaatsSem Theaterídoegen Pereựesentnu degrees ‘ Techn Econom board Pot BuchAccount Kaiserwo Broadienstaccountasserutikc plotaging sav Sic schemes current Inputaggi Ton否 nunipageinputmodal toninflateutaợodio certain sicкоöß Charles bog Community techni Dean十 Syngoogleapisawkarasono tutpel Neberga Niem Du Virtual fruit incomplete gateanz gateway świataстukaASE到 Keith interven president He Duch SpectEntryieren Senator Dum го CDcommandsматката presidenentry potfn seat responsible Beg�BA succeedültaties têteHO Earadesh haColl paperentic guarante DireCategory bridgeмей Milanadmin Kun Langде Gatelang Bologreq prom ne prodhu width caslot Romeano皇DataSource Melhing luckтоваCB\n",
      "Time taken: 1732.04 seconds\n",
      "Tokens per second: 0.12\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import time\n",
    "\n",
    "# Set the number of threads to 12\n",
    "torch.set_num_threads(12)\n",
    "torch.set_num_interop_threads(12)\n",
    "\n",
    "model_id = \"pavan01729/svdllm_gptq_8_0.65\"\n",
    "\n",
    "# Use the same model ID for both the tokenizer and the model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"cpu\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test the model and tokenizer to ensure they work\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(\"cpu\")\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.5,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model parameters:\n",
      "model.embed_tokens.weight: 131072000 parameters\n",
      "model.layers.0.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.0.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.0.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.0.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.0.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.0.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.0.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.0.input_layernorm.weight: 4096 parameters\n",
      "model.layers.0.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.1.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.1.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.1.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.1.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.1.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.1.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.1.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.1.input_layernorm.weight: 4096 parameters\n",
      "model.layers.1.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.2.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.2.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.2.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.2.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.2.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.2.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.2.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.2.input_layernorm.weight: 4096 parameters\n",
      "model.layers.2.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.3.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.3.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.3.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.3.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.3.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.3.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.3.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.3.input_layernorm.weight: 4096 parameters\n",
      "model.layers.3.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.4.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.4.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.4.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.4.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.4.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.4.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.4.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.4.input_layernorm.weight: 4096 parameters\n",
      "model.layers.4.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.5.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.5.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.5.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.5.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.5.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.5.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.5.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.5.input_layernorm.weight: 4096 parameters\n",
      "model.layers.5.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.6.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.6.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.6.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.6.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.6.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.6.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.6.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.6.input_layernorm.weight: 4096 parameters\n",
      "model.layers.6.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.7.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.7.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.7.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.7.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.7.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.7.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.7.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.7.input_layernorm.weight: 4096 parameters\n",
      "model.layers.7.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.8.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.8.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.8.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.8.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.8.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.8.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.8.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.8.input_layernorm.weight: 4096 parameters\n",
      "model.layers.8.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.9.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.9.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.9.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.9.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.9.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.9.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.9.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.9.input_layernorm.weight: 4096 parameters\n",
      "model.layers.9.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.10.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.10.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.10.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.10.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.10.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.10.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.10.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.10.input_layernorm.weight: 4096 parameters\n",
      "model.layers.10.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.11.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.11.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.11.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.11.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.11.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.11.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.11.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.11.input_layernorm.weight: 4096 parameters\n",
      "model.layers.11.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.12.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.12.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.12.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.12.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.12.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.12.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.12.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.12.input_layernorm.weight: 4096 parameters\n",
      "model.layers.12.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.13.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.13.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.13.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.13.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.13.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.13.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.13.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.13.input_layernorm.weight: 4096 parameters\n",
      "model.layers.13.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.14.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.14.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.14.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.14.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.14.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.14.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.14.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.14.input_layernorm.weight: 4096 parameters\n",
      "model.layers.14.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.15.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.15.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.15.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.15.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.15.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.15.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.15.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.15.input_layernorm.weight: 4096 parameters\n",
      "model.layers.15.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.16.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.16.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.16.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.16.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.16.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.16.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.16.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.16.input_layernorm.weight: 4096 parameters\n",
      "model.layers.16.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.17.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.17.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.17.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.17.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.17.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.17.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.17.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.17.input_layernorm.weight: 4096 parameters\n",
      "model.layers.17.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.18.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.18.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.18.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.18.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.18.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.18.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.18.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.18.input_layernorm.weight: 4096 parameters\n",
      "model.layers.18.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.19.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.19.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.19.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.19.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.19.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.19.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.19.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.19.input_layernorm.weight: 4096 parameters\n",
      "model.layers.19.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.20.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.20.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.20.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.20.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.20.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.20.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.20.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.20.input_layernorm.weight: 4096 parameters\n",
      "model.layers.20.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.21.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.21.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.21.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.21.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.21.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.21.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.21.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.21.input_layernorm.weight: 4096 parameters\n",
      "model.layers.21.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.22.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.22.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.22.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.22.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.22.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.22.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.22.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.22.input_layernorm.weight: 4096 parameters\n",
      "model.layers.22.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.23.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.23.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.23.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.23.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.23.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.23.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.23.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.23.input_layernorm.weight: 4096 parameters\n",
      "model.layers.23.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.24.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.24.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.24.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.24.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.24.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.24.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.24.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.24.input_layernorm.weight: 4096 parameters\n",
      "model.layers.24.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.25.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.25.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.25.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.25.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.25.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.25.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.25.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.25.input_layernorm.weight: 4096 parameters\n",
      "model.layers.25.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.26.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.26.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.26.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.26.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.26.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.26.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.26.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.26.input_layernorm.weight: 4096 parameters\n",
      "model.layers.26.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.27.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.27.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.27.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.27.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.27.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.27.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.27.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.27.input_layernorm.weight: 4096 parameters\n",
      "model.layers.27.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.28.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.28.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.28.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.28.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.28.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.28.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.28.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.28.input_layernorm.weight: 4096 parameters\n",
      "model.layers.28.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.29.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.29.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.29.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.29.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.29.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.29.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.29.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.29.input_layernorm.weight: 4096 parameters\n",
      "model.layers.29.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.30.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.30.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.30.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.30.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.30.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.30.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.30.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.30.input_layernorm.weight: 4096 parameters\n",
      "model.layers.30.post_attention_layernorm.weight: 4096 parameters\n",
      "model.layers.31.self_attn.q_proj.weight: 16777216 parameters\n",
      "model.layers.31.self_attn.k_proj.weight: 16777216 parameters\n",
      "model.layers.31.self_attn.v_proj.weight: 16777216 parameters\n",
      "model.layers.31.self_attn.o_proj.weight: 16777216 parameters\n",
      "model.layers.31.mlp.gate_proj.weight: 45088768 parameters\n",
      "model.layers.31.mlp.up_proj.weight: 45088768 parameters\n",
      "model.layers.31.mlp.down_proj.weight: 45088768 parameters\n",
      "model.layers.31.input_layernorm.weight: 4096 parameters\n",
      "model.layers.31.post_attention_layernorm.weight: 4096 parameters\n",
      "model.norm.weight: 4096 parameters\n",
      "lm_head.weight: 131072000 parameters\n",
      "\n",
      "Total parameters: 6738415616\n",
      "Trainable parameters: 6738415616\n"
     ]
    }
   ],
   "source": [
    "model_parameters = list(model.named_parameters())\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\nModel parameters:\")\n",
    "for name, param in model_parameters:\n",
    "    print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
