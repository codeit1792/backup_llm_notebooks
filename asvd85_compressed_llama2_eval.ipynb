{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.31it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "# Move the model to CPU\n",
    "model = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Answer the below question: How does Quantum Computers work?\n",
      "Why: Quantum Computers work by using the quantum superposition of the qubits. The qubits can be superposition of 0 and 1.\n",
      "How does\n",
      "Response Time: 359.044132232666 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Function to generate text and calculate the response time\n",
    "def generate_text_with_timing(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response_time = end_time - start_time\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text, response_time\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Answer the below question: How does Quantum Computers work?\"\n",
    "generated_text, response_time = generate_text_with_timing(prompt)\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Response Time: {response_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Information\n",
    "- **Model Size (parameters):** 5,747,427,328\n",
    "- **Model File Size:** 10,964.79 MB\n",
    "\n",
    "## Inference Times\n",
    "\n",
    "### 1st Output\n",
    "- **Inference Time:** 9.96 seconds\n",
    "\n",
    "### 2nd 10 Averaged\n",
    "- **Total Inference Time:** 704.28 seconds\n",
    "- **Average Inference Time:** 70.43 seconds\n",
    "\n",
    "### 3rd 10 Averaged with Tokens/Second\n",
    "- **Total Inference Time:** 222.81 seconds\n",
    "- **Average Inference Time:** 22.28 seconds\n",
    "- **Total Tokens:** 1,472\n",
    "- **Tokens per Second:** 6.61\n",
    "\n",
    "### 4th 10 Averaged with Tokens/Second\n",
    "- **Total Inference Time:** 222.81 seconds\n",
    "- **Average Inference Time:** 22.28 seconds\n",
    "- **Total Tokens:** 1,472\n",
    "- **Tokens per Second:** 6.61\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size (parameters): 5747427328\n",
      "Model file size: 10964.79 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tempfile\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Function to calculate the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Function to calculate the model file size\n",
    "def get_model_size(model, tokenizer):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        model.save_pretrained(tmpdirname)\n",
    "        tokenizer.save_pretrained(tmpdirname)\n",
    "        size = sum(os.path.getsize(os.path.join(tmpdirname, f)) for f in os.listdir(tmpdirname))\n",
    "    return size\n",
    "\n",
    "# Calculate parameter count and file size\n",
    "param_count = count_parameters(model)\n",
    "file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Model size (parameters): {param_count}\")\n",
    "print(f\"Model file size: {file_size / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Answer the below question: How does Quantum Computers work?\n",
      "Response: Answer the below question: How does Quantum Computers work?\n",
      "What is a quantum computer and how it works. The first thing to understand about QCs (Quantium computers) are that they use qubits instead of bits, which means two states at once rather than one state or 0/1 as in classical computing systems like our laptops today; this allows for exponentially more possibilities when performing calculations on these machines! They can also be used with superposition—the ability not only have both possible outcomes but all potential results simultaneously too – making them even faster still by orders-of magnitude compared traditional methods such quicker computations become feasible nowadays thanks largely due advances made within last few years alone .\n",
      "Inference Time: 9.96 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    "# )\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"Answer the below question: How does Quantum Computers work?\"\n",
    "# prompt = \"Hello, what can u do?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Set generation parameters\n",
    "temperature = 0.7  # The higher the temperature, the more random the output\n",
    "top_k = 50  # Limits the sampling pool to top_k tokens\n",
    "top_p = 0.95  # Cumulative probability for nucleus sampling\n",
    "repetition_penalty = 1.9  # Penalizes repetition in the output\n",
    "\n",
    "# Measure the inference time\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_length=200, \n",
    "    temperature=temperature, \n",
    "    top_k=top_k, \n",
    "    top_p=top_p, \n",
    "    repetition_penalty=repetition_penalty\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Decode the output\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Answer the below question: How does Quantum Computers work?\n",
      "Response: Answer the below question: How does Quantum Computers work?\n",
      "What is a quantum computer and how it works. The first thing to understand about QCs (Quantium computers) are that they use qubits instead of bits, which means two states at once rather than one state or 0/1 as in classical computing systems like our laptops today; this allows for exponentially more possibilities when performing calculations on these machines! They can also be used with superposition—the ability not only have both possible outcomes but all potential results simultaneously too – making them even faster still by orders-of magnitude compared traditional methods such quicker computations become feasible nowadays thanks largely due advances made within last few years alone .\n",
      "Inference Time: 69.67 seconds\n",
      "\n",
      "Prompt 2: What is the theory of relativity?\n",
      "Response: What is the theory of relativity?\n",
      "What are some examples for a law in physics that describes how objects move and change over time. 10 Examples: Newton's Laws, Law Of Gravity ,Law...\n",
      "Inference Time: 20.58 seconds\n",
      "\n",
      "Prompt 3: Explain the process of photosynthesis.\n",
      "Response: Explain the process of photosynthesis.\n",
      "Photosyntesis is a chemical reaction in which sunlight energy and carbon dioxide are used to produce food for plants, animals or humans by using water from soil moisture as well . The light absorbed during this procedure will be converted into sugars that can then provide nutrition through respiration processes within cells , while also releasing oxygen back out again so it's released outside where we breathe air with fresh clean breathable quality !\n",
      "Inference Time: 49.39 seconds\n",
      "\n",
      "Prompt 4: What are black holes?\n",
      "Response: What are black holes?\n",
      "What is a Black Hole ? A BH (Black hole)is an object with such strong gravity that not even light can escape from it. It's so powerful,that nothing escapes its gravitational pull .It has no mass and hence doesnot have any volume too !!! The only thing which exists in the universe around this massive body ,are just space time continuum of spacetime manifold! This means there will be some sorta curved or warped geometry to our 3D world we live on earth !! So if you were standing at one point near by your house then as per Einstein ' s theory about relativity principle when u move away towards farthest distance possible for example say like moving out into outerspace beyond solar system orbiting sun ...then according t o Einst einn ei nnnnein neeenn iinn ieeeeniiieeenienniiiiineeiieiiniieneeniinen\n",
      "Inference Time: 97.85 seconds\n",
      "\n",
      "Prompt 5: Describe the human circulatory system.\n",
      "Response: Describe the human circulatory system.\n",
      "Describe how blood is pumped through your body by heart muscle contractions and relaxations, which are controlled in part from signals sent to it via nerves that connect with its surface membrane (the epicardium). The cardiac cycle consists of two phases: systole—contraction; diastole —relaxation/resting phase between beats during each beat or pulse when a person’s hear contractile tissue relaxes after having squeezes outwardly against arterial walls as they push forward into their lumen cavities containing them while pushing oxygenated red-blood cells filled up fuller than usual due for delivery throughout one'ss entirety bodies at once!\n",
      "Inference Time: 76.05 seconds\n",
      "\n",
      "Prompt 6: What is artificial intelligence?\n",
      "Response: What is artificial intelligence?\n",
      "A I A stands for Artificial Intelligence. It's a computer program that can think like humans do, and it has the ability to learn from experience as well . The term was coined by John McCarthy in 1956 at an event called \"The Dartmouth Summer School of Nuclear Physics\" which took place on August ,thirty-first through September twenty -ninth ; this conference marked one hundred years since Charles Darwin published his book 'On Evolution'. In order words; if you want your robotic servant (or slave)to be able too talk back when he or she asks questions about what they are doing then all we have done so far will not suffice because there isn’t any way around having some sort o f human interaction with these machines! This means more than just talking though: You need someone who understand s how computers work inside out ! And even better yet : Someone\n",
      "Inference Time: 97.49 seconds\n",
      "\n",
      "Prompt 7: How does blockchain technology work?\n",
      "Response: How does blockchain technology work?\n",
      "How do you use a cryptocurrency wallet to buy and sell crypto assets like Bitcoin, Ethereum or Ripple (XRP)? What is the difference between an exchange-traded fund that tracks bit coins versus investing in actual digital currencies such as B it coin ? And what are some of t he risks involved with owning these types o f asset s , especially given th e recent volatility i n prices for b ill io u . 1. Block chain techno logy : A d ata stored on multiple computers around w h ere ever y one can access information about transactions made using certain cryp tot ecurr en cy - related technologies called \"block chains\" allow users who have downloaded software from companies lik elike Googl et 's GDAX ex change platform righ ts owners' data storage systems known ass D igital Asset\n",
      "Inference Time: 96.39 seconds\n",
      "\n",
      "Prompt 8: What are the principles of democracy?\n",
      "Response: What are the principles of democracy?\n",
      "What is a republican form government and what does it mean to be Republican in America today.\n",
      "Inference Time: 10.47 seconds\n",
      "\n",
      "Prompt 9: Explain the Big Bang theory.\n",
      "Response: Explain the Big Bang theory.\n",
      "The universe was created in a big bang, which is an explosion that occurred at one point of time and space (the beginning). The expansion started with this initial event called \"big-bounce\" or inflationary phase where it expanded rapidly for about 1032 seconds after its creation to form galaxies like our solar system as we know today on earth's surface area from all directions around us including upwards into outerspace above skyline horizon line beyond atmosphere boundary layer limit limits lines skies stars sunsets sundown twilight nighttime nocturnal nocentric nebulous newtonian Newton Newtons law gravity gravitation gravitational force gforce centrifuges cenitre center core nucleus nuclei nuclear power energy electricity electrical electronics electrons electrolytes ionization plasma gas helium hydrogen hho he4 deuteron neutron protrusion propulsion\n",
      "Inference Time: 96.21 seconds\n",
      "\n",
      "Prompt 10: How do vaccines work?\n",
      "Response: How do vaccines work?\n",
      "V a cc in e s w o r k ? Vac-cin es wo ork �C va cc i nes voor werk! W h at d ow vecc ue wsrkooowwvveeecccceenennnneesssssseecseeeseeesesseesesessesesesesesesesesesesesesesesesensesenseenseneenesenenesennsnesnennnnddandanannaaa annaaaaa ! HOW DO YOUU U OO NN NOONNN ONE EEEENNESSSSEESSEE S SE SS ES EN NSNS NE NAANNAA AAA AN AA AND DDANDDDAAAAADDAADD AD ADD DAWDOODLELLL L LL LEEL EL ALALLAALL ALL LAHHAHHhhhhaahaha ha ah aa .\n",
      "Inference Time: 90.18 seconds\n",
      "\n",
      "Total Inference Time: 704.28 seconds\n",
      "Average Inference Time: 70.43 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"Answer the below question: How does Quantum Computers work?\",\n",
    "    \"What is the theory of relativity?\",\n",
    "    \"Explain the process of photosynthesis.\",\n",
    "    \"What are black holes?\",\n",
    "    \"Describe the human circulatory system.\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does blockchain technology work?\",\n",
    "    \"What are the principles of democracy?\",\n",
    "    \"Explain the Big Bang theory.\",\n",
    "    \"How do vaccines work?\"\n",
    "]\n",
    "\n",
    "# Set generation parameters\n",
    "temperature = 0.7  # The higher the temperature, the more random the output\n",
    "top_k = 50  # Limits the sampling pool to top_k tokens\n",
    "top_p = 0.95  # Cumulative probability for nucleus sampling\n",
    "repetition_penalty = 1.9  # Penalizes repetition in the output\n",
    "\n",
    "# Initialize variables to calculate total and average time\n",
    "total_time = 0\n",
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Measure the inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=200, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Store the response and time\n",
    "    responses.append((prompt, response, inference_time))\n",
    "\n",
    "# Calculate average time\n",
    "average_time = total_time / len(prompts)\n",
    "\n",
    "# Print the results\n",
    "for i, (prompt, response, inference_time) in enumerate(responses):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Inference Time: {average_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens per second and average time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Answer the below question: How does Quantum Computers work?\n",
      "Response: Answer the below question: How does Quantum Computers work?\n",
      "What is a quantum computer and how it works. The first thing to understand about QCs (Quantium computers) are that they use qubits instead of bits, which means two states at once rather than one state or 0/1 as in classical computing systems like our laptops today; this allows for exponentially more possibilities when performing calculations on these machines! They can also be used with superposition—the ability not only have both possible outcomes but all potential results simultaneously too – making them even faster still by orders-of magnitude compared traditional methods such quicker computations become feasible nowadays thanks largely due advances made within last few years alone .\n",
      "Inference Time: 21.99 seconds\n",
      "Number of Tokens: 150\n",
      "\n",
      "Prompt 2: What is the theory of relativity?\n",
      "Response: What is the theory of relativity?\n",
      "What are some examples for a law in physics that describes how objects move and change over time. 10 Examples: Newton's Laws, Law Of Gravity ,Law...\n",
      "Inference Time: 6.52 seconds\n",
      "Number of Tokens: 48\n",
      "\n",
      "Prompt 3: Explain the process of photosynthesis.\n",
      "Response: Explain the process of photosynthesis.\n",
      "Photosyntesis is a chemical reaction in which sunlight energy and carbon dioxide are used to produce food for plants, animals or humans by using water from soil moisture as well . The light absorbed during this procedure will be converted into sugars that can then provide nutrition through respiration processes within cells , while also releasing oxygen back out again so it's released outside where we breathe air with fresh clean breathable quality !\n",
      "Inference Time: 15.63 seconds\n",
      "Number of Tokens: 106\n",
      "\n",
      "Prompt 4: What are black holes?\n",
      "Response: What are black holes?\n",
      "What is a Black Hole ? A BH (Black hole)is an object with such strong gravity that not even light can escape from it. It's so powerful,that nothing escapes its gravitational pull .It has no mass and hence doesnot have any volume too !!! The only thing which exists in the universe around this massive body ,are just space time continuum of spacetime manifold! This means there will be some sorta curved or warped geometry to our 3D world we live on earth !! So if you were standing at one point near by your house then as per Einstein ' s theory about relativity principle when u move away towards farthest distance possible for example say like moving out into outerspace beyond solar system orbiting sun ...then according t o Einst einn ei nnnnein neeenn iinn ieeeeniiieeenienniiiiineeiieiiniieneeniinen\n",
      "Inference Time: 30.91 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 5: Describe the human circulatory system.\n",
      "Response: Describe the human circulatory system.\n",
      "Describe how blood is pumped through your body by heart muscle contractions and relaxations, which are controlled in part from signals sent to it via nerves that connect with its surface membrane (the epicardium). The cardiac cycle consists of two phases: systole—contraction; diastole —relaxation/resting phase between beats during each beat or pulse when a person’s hear contractile tissue relaxes after having squeezes outwardly against arterial walls as they push forward into their lumen cavities containing them while pushing oxygenated red-blood cells filled up fuller than usual due for delivery throughout one'ss entirety bodies at once!\n",
      "Inference Time: 24.09 seconds\n",
      "Number of Tokens: 158\n",
      "\n",
      "Prompt 6: What is artificial intelligence?\n",
      "Response: What is artificial intelligence?\n",
      "A I A stands for Artificial Intelligence. It's a computer program that can think like humans do, and it has the ability to learn from experience as well . The term was coined by John McCarthy in 1956 at an event called \"The Dartmouth Summer School of Nuclear Physics\" which took place on August ,thirty-first through September twenty -ninth ; this conference marked one hundred years since Charles Darwin published his book 'On Evolution'. In order words; if you want your robotic servant (or slave)to be able too talk back when he or she asks questions about what they are doing then all we have done so far will not suffice because there isn’t any way around having some sort o f human interaction with these machines! This means more than just talking though: You need someone who understand s how computers work inside out ! And even better yet : Someone\n",
      "Inference Time: 30.80 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 7: How does blockchain technology work?\n",
      "Response: How does blockchain technology work?\n",
      "How do you use a cryptocurrency wallet to buy and sell crypto assets like Bitcoin, Ethereum or Ripple (XRP)? What is the difference between an exchange-traded fund that tracks bit coins versus investing in actual digital currencies such as B it coin ? And what are some of t he risks involved with owning these types o f asset s , especially given th e recent volatility i n prices for b ill io u . 1. Block chain techno logy : A d ata stored on multiple computers around w h ere ever y one can access information about transactions made using certain cryp tot ecurr en cy - related technologies called \"block chains\" allow users who have downloaded software from companies lik elike Googl et 's GDAX ex change platform righ ts owners' data storage systems known ass D igital Asset\n",
      "Inference Time: 30.50 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 8: What are the principles of democracy?\n",
      "Response: What are the principles of democracy?\n",
      "What is a republican form government and what does it mean to be Republican in America today.\n",
      "Inference Time: 3.36 seconds\n",
      "Number of Tokens: 28\n",
      "\n",
      "Prompt 9: Explain the Big Bang theory.\n",
      "Response: Explain the Big Bang theory.\n",
      "The universe was created in a big bang, which is an explosion that occurred at one point of time and space (the beginning). The expansion started with this initial event called \"big-bounce\" or inflationary phase where it expanded rapidly for about 1032 seconds after its creation to form galaxies like our solar system as we know today on earth's surface area from all directions around us including upwards into outerspace above skyline horizon line beyond atmosphere boundary layer limit limits lines skies stars sunsets sundown twilight nighttime nocturnal nocentric nebulous newtonian Newton Newtons law gravity gravitation gravitational force gforce centrifuges cenitre center core nucleus nuclei nuclear power energy electricity electrical electronics electrons electrolytes ionization plasma gas helium hydrogen hho he4 deuteron neutron protrusion propulsion\n",
      "Inference Time: 30.44 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 10: How do vaccines work?\n",
      "Response: How do vaccines work?\n",
      "V a cc in e s w o r k ? Vac-cin es wo ork �C va cc i nes voor werk! W h at d ow vecc ue wsrkooowwvveeecccceenennnneesssssseecseeeseeesesseesesessesesesesesesesesesesesesesesesensesenseenseneenesenenesennsnesnennnnddandanannaaa annaaaaa ! HOW DO YOUU U OO NN NOONNN ONE EEEENNESSSSEESSEE S SE SS ES EN NSNS NE NAANNAA AAA AN AA AND DDANDDDAAAAADDAADD AD ADD DAWDOODLELLL L LL LEEL EL ALALLAALL ALL LAHHAHHhhhhaahaha ha ah aa .\n",
      "Inference Time: 28.58 seconds\n",
      "Number of Tokens: 186\n",
      "\n",
      "Total Inference Time: 222.81 seconds\n",
      "Average Inference Time: 22.28 seconds\n",
      "Total Tokens: 1472\n",
      "Tokens per Second: 6.61\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"Answer the below question: How does Quantum Computers work?\",\n",
    "    \"What is the theory of relativity?\",\n",
    "    \"Explain the process of photosynthesis.\",\n",
    "    \"What are black holes?\",\n",
    "    \"Describe the human circulatory system.\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does blockchain technology work?\",\n",
    "    \"What are the principles of democracy?\",\n",
    "    \"Explain the Big Bang theory.\",\n",
    "    \"How do vaccines work?\"\n",
    "]\n",
    "\n",
    "# Set generation parameters\n",
    "temperature = 0.7  # The higher the temperature, the more random the output\n",
    "top_k = 50  # Limits the sampling pool to top_k tokens\n",
    "top_p = 0.95  # Cumulative probability for nucleus sampling\n",
    "repetition_penalty = 1.9  # Penalizes repetition in the output\n",
    "\n",
    "# Initialize variables to calculate total and average time\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Measure the inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=200, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Count the number of tokens in the generated output\n",
    "    num_tokens = len(tokenizer.encode(response, add_special_tokens=False))\n",
    "    total_tokens += num_tokens\n",
    "    \n",
    "    # Store the response and time\n",
    "    responses.append((prompt, response, inference_time, num_tokens))\n",
    "\n",
    "# Calculate average time and tokens per second\n",
    "average_time = total_time / len(prompts)\n",
    "tokens_per_second = total_tokens / total_time\n",
    "\n",
    "# Print the results\n",
    "for i, (prompt, response, inference_time, num_tokens) in enumerate(responses):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "    print(f\"Number of Tokens: {num_tokens}\\n\")\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Inference Time: {average_time:.2f} seconds\")\n",
    "print(f\"Total Tokens: {total_tokens}\")\n",
    "print(f\"Tokens per Second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## same code running again to check difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Answer the below question: How does Quantum Computers work?\n",
      "Response: Answer the below question: How does Quantum Computers work?\n",
      "What is a quantum computer and how it works. The first thing to understand about QCs (Quantium computers) are that they use qubits instead of bits, which means two states at once rather than one state or 0/1 as in classical computing systems like our laptops today; this allows for exponentially more possibilities when performing calculations on these machines! They can also be used with superposition—the ability not only have both possible outcomes but all potential results simultaneously too – making them even faster still by orders-of magnitude compared traditional methods such quicker computations become feasible nowadays thanks largely due advances made within last few years alone .\n",
      "Inference Time: 22.27 seconds\n",
      "Number of Tokens: 150\n",
      "\n",
      "Prompt 2: What is the theory of relativity?\n",
      "Response: What is the theory of relativity?\n",
      "What are some examples for a law in physics that describes how objects move and change over time. 10 Examples: Newton's Laws, Law Of Gravity ,Law...\n",
      "Inference Time: 6.61 seconds\n",
      "Number of Tokens: 48\n",
      "\n",
      "Prompt 3: Explain the process of photosynthesis.\n",
      "Response: Explain the process of photosynthesis.\n",
      "Photosyntesis is a chemical reaction in which sunlight energy and carbon dioxide are used to produce food for plants, animals or humans by using water from soil moisture as well . The light absorbed during this procedure will be converted into sugars that can then provide nutrition through respiration processes within cells , while also releasing oxygen back out again so it's released outside where we breathe air with fresh clean breathable quality !\n",
      "Inference Time: 15.75 seconds\n",
      "Number of Tokens: 106\n",
      "\n",
      "Prompt 4: What are black holes?\n",
      "Response: What are black holes?\n",
      "What is a Black Hole ? A BH (Black hole)is an object with such strong gravity that not even light can escape from it. It's so powerful,that nothing escapes its gravitational pull .It has no mass and hence doesnot have any volume too !!! The only thing which exists in the universe around this massive body ,are just space time continuum of spacetime manifold! This means there will be some sorta curved or warped geometry to our 3D world we live on earth !! So if you were standing at one point near by your house then as per Einstein ' s theory about relativity principle when u move away towards farthest distance possible for example say like moving out into outerspace beyond solar system orbiting sun ...then according t o Einst einn ei nnnnein neeenn iinn ieeeeniiieeenienniiiiineeiieiiniieneeniinen\n",
      "Inference Time: 30.73 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 5: Describe the human circulatory system.\n",
      "Response: Describe the human circulatory system.\n",
      "Describe how blood is pumped through your body by heart muscle contractions and relaxations, which are controlled in part from signals sent to it via nerves that connect with its surface membrane (the epicardium). The cardiac cycle consists of two phases: systole—contraction; diastole —relaxation/resting phase between beats during each beat or pulse when a person’s hear contractile tissue relaxes after having squeezes outwardly against arterial walls as they push forward into their lumen cavities containing them while pushing oxygenated red-blood cells filled up fuller than usual due for delivery throughout one'ss entirety bodies at once!\n",
      "Inference Time: 23.91 seconds\n",
      "Number of Tokens: 158\n",
      "\n",
      "Prompt 6: What is artificial intelligence?\n",
      "Response: What is artificial intelligence?\n",
      "A I A stands for Artificial Intelligence. It's a computer program that can think like humans do, and it has the ability to learn from experience as well . The term was coined by John McCarthy in 1956 at an event called \"The Dartmouth Summer School of Nuclear Physics\" which took place on August ,thirty-first through September twenty -ninth ; this conference marked one hundred years since Charles Darwin published his book 'On Evolution'. In order words; if you want your robotic servant (or slave)to be able too talk back when he or she asks questions about what they are doing then all we have done so far will not suffice because there isn’t any way around having some sort o f human interaction with these machines! This means more than just talking though: You need someone who understand s how computers work inside out ! And even better yet : Someone\n",
      "Inference Time: 30.54 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 7: How does blockchain technology work?\n",
      "Response: How does blockchain technology work?\n",
      "How do you use a cryptocurrency wallet to buy and sell crypto assets like Bitcoin, Ethereum or Ripple (XRP)? What is the difference between an exchange-traded fund that tracks bit coins versus investing in actual digital currencies such as B it coin ? And what are some of t he risks involved with owning these types o f asset s , especially given th e recent volatility i n prices for b ill io u . 1. Block chain techno logy : A d ata stored on multiple computers around w h ere ever y one can access information about transactions made using certain cryp tot ecurr en cy - related technologies called \"block chains\" allow users who have downloaded software from companies lik elike Googl et 's GDAX ex change platform righ ts owners' data storage systems known ass D igital Asset\n",
      "Inference Time: 30.22 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 8: What are the principles of democracy?\n",
      "Response: What are the principles of democracy?\n",
      "What is a republican form government and what does it mean to be Republican in America today.\n",
      "Inference Time: 3.27 seconds\n",
      "Number of Tokens: 28\n",
      "\n",
      "Prompt 9: Explain the Big Bang theory.\n",
      "Response: Explain the Big Bang theory.\n",
      "The universe was created in a big bang, which is an explosion that occurred at one point of time and space (the beginning). The expansion started with this initial event called \"big-bounce\" or inflationary phase where it expanded rapidly for about 1032 seconds after its creation to form galaxies like our solar system as we know today on earth's surface area from all directions around us including upwards into outerspace above skyline horizon line beyond atmosphere boundary layer limit limits lines skies stars sunsets sundown twilight nighttime nocturnal nocentric nebulous newtonian Newton Newtons law gravity gravitation gravitational force gforce centrifuges cenitre center core nucleus nuclei nuclear power energy electricity electrical electronics electrons electrolytes ionization plasma gas helium hydrogen hho he4 deuteron neutron protrusion propulsion\n",
      "Inference Time: 30.09 seconds\n",
      "Number of Tokens: 199\n",
      "\n",
      "Prompt 10: How do vaccines work?\n",
      "Response: How do vaccines work?\n",
      "V a cc in e s w o r k ? Vac-cin es wo ork �C va cc i nes voor werk! W h at d ow vecc ue wsrkooowwvveeecccceenennnneesssssseecseeeseeesesseesesessesesesesesesesesesesesesesesesensesenseenseneenesenenesennsnesnennnnddandanannaaa annaaaaa ! HOW DO YOUU U OO NN NOONNN ONE EEEENNESSSSEESSEE S SE SS ES EN NSNS NE NAANNAA AAA AN AA AND DDANDDDAAAAADDAADD AD ADD DAWDOODLELLL L LL LEEL EL ALALLAALL ALL LAHHAHHhhhhaahaha ha ah aa .\n",
      "Inference Time: 28.37 seconds\n",
      "Number of Tokens: 186\n",
      "\n",
      "Total Inference Time: 221.77 seconds\n",
      "Average Inference Time: 22.18 seconds\n",
      "Total Tokens: 1472\n",
      "Tokens per Second: 6.64\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"Answer the below question: How does Quantum Computers work?\",\n",
    "    \"What is the theory of relativity?\",\n",
    "    \"Explain the process of photosynthesis.\",\n",
    "    \"What are black holes?\",\n",
    "    \"Describe the human circulatory system.\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does blockchain technology work?\",\n",
    "    \"What are the principles of democracy?\",\n",
    "    \"Explain the Big Bang theory.\",\n",
    "    \"How do vaccines work?\"\n",
    "]\n",
    "\n",
    "# Set generation parameters\n",
    "temperature = 0.7  # The higher the temperature, the more random the output\n",
    "top_k = 50  # Limits the sampling pool to top_k tokens\n",
    "top_p = 0.95  # Cumulative probability for nucleus sampling\n",
    "repetition_penalty = 1.9  # Penalizes repetition in the output\n",
    "\n",
    "# Initialize variables to calculate total and average time\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Measure the inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=200, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Count the number of tokens in the generated output\n",
    "    num_tokens = len(tokenizer.encode(response, add_special_tokens=False))\n",
    "    total_tokens += num_tokens\n",
    "    \n",
    "    # Store the response and time\n",
    "    responses.append((prompt, response, inference_time, num_tokens))\n",
    "\n",
    "# Calculate average time and tokens per second\n",
    "average_time = total_time / len(prompts)\n",
    "tokens_per_second = total_tokens / total_time\n",
    "\n",
    "# Print the results\n",
    "for i, (prompt, response, inference_time, num_tokens) in enumerate(responses):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\")\n",
    "    print(f\"Number of Tokens: {num_tokens}\\n\")\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Inference Time: {average_time:.2f} seconds\")\n",
    "print(f\"Total Tokens: {total_tokens}\")\n",
    "print(f\"Tokens per Second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n",
      "['sentence']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load datasets\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "ptb = load_dataset(\"ptb_text_only\", \"penn_treebank\")\n",
    "\n",
    "print(wikitext[\"test\"].column_names)\n",
    "print(ptb[\"test\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Using pad_token, but it is not set yet.\n",
      "Map: 100%|██████████| 42068/42068 [00:07<00:00, 5756.17 examples/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU \u0001 has a total capacity of 15.77 GiB of which 197.12 MiB is free. Including non-PyTorch memory, this process has 14.59 GiB memory in use. Process 1278388 has 1008.00 MiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 693.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[15], line 67\u001b[0m\n",
      "\u001b[1;32m     64\u001b[0m ptb_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(ptb[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n",
      "\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Calculate perplexity for WikiText-2\u001b[39;00m\n",
      "\u001b[0;32m---> 67\u001b[0m wikitext_perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwikitext_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWikiText-2 Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwikitext_perplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate perplexity for Penn Treebank\u001b[39;00m\n",
      "\n",
      "Cell \u001b[0;32mIn[15], line 43\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[0;34m(model, dataloader)\u001b[0m\n",
      "\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n",
      "\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Move the input tensors to the same device as the model\u001b[39;00m\n",
      "\u001b[1;32m     42\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;32m---> 43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;32m     45\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:806\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    803\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n",
      "\u001b[0;32m--> 806\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    816\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:693\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    685\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n",
      "\u001b[1;32m    686\u001b[0m         create_custom_forward(decoder_layer),\n",
      "\u001b[1;32m    687\u001b[0m         hidden_states,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    690\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[1;32m    691\u001b[0m     )\n",
      "\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 693\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    702\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:408\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n",
      "\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n",
      "\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n",
      "\u001b[0;32m--> 408\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    416\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:346\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n",
      "\u001b[1;32m    343\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m+\u001b[39m attention_mask\n",
      "\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n",
      "\u001b[0;32m--> 346\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(query_states\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;32m    347\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n",
      "\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim):\n",
      "\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/functional.py:1887\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n",
      "\u001b[1;32m   1885\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n",
      "\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1887\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU \u0001 has a total capacity of 15.77 GiB of which 197.12 MiB is free. Including non-PyTorch memory, this process has 14.59 GiB memory in use. Process 1278388 has 1008.00 MiB memory in use. Of the allocated memory 13.55 GiB is allocated by PyTorch, and 693.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set the pad_token to eos_token if it's not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load datasets\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "ptb = load_dataset(\"ptb_text_only\", \"penn_treebank\")\n",
    "\n",
    "# Tokenize the datasets\n",
    "def tokenize_function_wikitext(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "def tokenize_function_ptb(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "wikitext = wikitext.map(tokenize_function_wikitext, batched=True, remove_columns=[\"text\"])\n",
    "ptb = ptb.map(tokenize_function_ptb, batched=True, remove_columns=[\"sentence\"])\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move the input tensors to the same device as the model\n",
    "            inputs = {key: value.to(model.device) for key, value in batch.items()}\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=inputs[\"labels\"])\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * inputs[\"input_ids\"].size(1)\n",
    "            total_tokens += inputs[\"input_ids\"].size(1)\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "# Create dataloaders for the datasets\n",
    "def collate_fn(batch):\n",
    "    # Convert list of lists to tensors\n",
    "    input_ids = torch.stack([torch.tensor(item['input_ids']) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': input_ids\n",
    "    }\n",
    "\n",
    "wikitext_dataloader = DataLoader(wikitext[\"test\"], batch_size=8, collate_fn=collate_fn)\n",
    "ptb_dataloader = DataLoader(ptb[\"test\"], batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "# Calculate perplexity for WikiText-2\n",
    "wikitext_perplexity = calculate_perplexity(model, wikitext_dataloader)\n",
    "print(f\"WikiText-2 Perplexity: {wikitext_perplexity}\")\n",
    "\n",
    "# Calculate perplexity for Penn Treebank\n",
    "ptb_perplexity = calculate_perplexity(model, ptb_dataloader)\n",
    "print(f\"Penn Treebank Perplexity: {ptb_perplexity}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/hahnyuan/Llama-2-7b-hf-asvd85:\n",
      "- configuration_asvd_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/hahnyuan/Llama-2-7b-hf-asvd85:\n",
      "- modeling_asvd_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 3/3 [04:47<00:00, 95.99s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n",
      "Some weights of ASVDLlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Answer the below question: How does Quantum Computers work?\n",
      "Response: Answer the below question: How does Quantum Computers work?\n",
      "What is a quantum computer and how it works. The first thing to understand about QCs (Quantium computers) are that they use qubits instead of bits, which means two states at once rather than one state or 0/1 as in classical computing systems like our laptops today; this allows for exponentially more possibilities when performing calculations on these machines! They can also be used with superposition—the ability not only have both possible outcomes but all potential results simultaneously too – making them even faster still by orders-of magnitude compared traditional methods such quicker computations become feasible nowadays thanks largely due advances made within last few years alone .\n",
      "Inference Time: 10.18 seconds\n",
      "\n",
      "Prompt 2: What is the theory of relativity?\n",
      "Response: What is the theory of relativity?\n",
      "What are some examples for a law in physics that describes how objects move and change over time. 10 Examples: Newton's Laws, Law Of Gravity ,Law...\n",
      "Inference Time: 2.87 seconds\n",
      "\n",
      "Prompt 3: Explain the process of photosynthesis.\n",
      "Response: Explain the process of photosynthesis.\n",
      "Photosyntesis is a chemical reaction in which sunlight energy and carbon dioxide are used to produce food for plants, animals or humans by using water from soil moisture as well . The light absorbed during this procedure will be converted into sugars that can then provide nutrition through respiration processes within cells , while also releasing oxygen back out again so it's released outside where we breathe air with fresh clean breathable quality !\n",
      "Inference Time: 7.12 seconds\n",
      "\n",
      "Prompt 4: What are black holes?\n",
      "Response: What are black holes?\n",
      "What is a Black Hole ? A BH (Black hole)is an object with such strong gravity that not even light can escape from it. It's so powerful,that nothing escapes its gravitational pull .It has no mass and hence doesnot have any volume too !!! The only thing which exists in the universe around this massive body ,are just space time continuum of spacetime manifold! This means there will be some sorta curved or warped geometry to our 3D world we live on earth !! So if you were standing at one point near by your house then as per Einstein ' s theory about relativity principle when u move away towards farthest distance possible for example say like moving out into outerspace beyond solar system orbiting sun ...then according t o Einst einn ei nnnnein neeenn iinn ieeeeniiieeenienniiiiineeiieiiniieneeniinen\n",
      "Inference Time: 15.57 seconds\n",
      "\n",
      "Prompt 5: Describe the human circulatory system.\n",
      "Response: Describe the human circulatory system.\n",
      "Describe how blood is pumped through your body by heart muscle contractions and relaxations, which are controlled in part from signals sent to it via nerves that connect with its surface membrane (the epicardium). The cardiac cycle consists of two phases: systole—contraction; diastole —relaxation/resting phase between beats during each beat or pulse when a person’s hear contractile tissue relaxes after having squeezes outwardly against arterial walls as they push forward into their lumen cavities containing them while pushing oxygenated red-blood cells filled up fuller than usual due for delivery throughout one'ss entirety bodies at once!\n",
      "Inference Time: 12.96 seconds\n",
      "\n",
      "Prompt 6: What is artificial intelligence?\n",
      "Response: What is artificial intelligence?\n",
      "A I A stands for Artificial Intelligence. It's a computer program that can think like humans do, and it has the ability to learn from experience as well . The term was coined by John McCarthy in 1956 at an event called \"The Dartmouth Summer School of Nuclear Physics\" which took place on August ,thirty-first through September twenty -ninth ; this conference marked one hundred years since Charles Darwin published his book 'On Evolution'. In order words; if you want your robotic servant (or slave)to be able too talk back when he or she asks questions about what they are doing then all we have done so far will not suffice because there isn’t any way around having some sort o f human interaction with these machines! This means more than just talking though: You need someone who understand s how computers work inside out ! And even better yet : Someone\n",
      "Inference Time: 15.91 seconds\n",
      "\n",
      "Prompt 7: How does blockchain technology work?\n",
      "Response: How does blockchain technology work?\n",
      "How do you use a cryptocurrency wallet to buy and sell crypto assets like Bitcoin, Ethereum or Ripple (XRP)? What is the difference between an exchange-traded fund that tracks bit coins versus investing in actual digital currencies such as B it coin ? And what are some of t he risks involved with owning these types o f asset s , especially given th e recent volatility i n prices for b ill io u . 1. Block chain techno logy : A d ata stored on multiple computers around w h ere ever y one can access information about transactions made using certain cryp tot ecurr en cy - related technologies called \"block chains\" allow users who have downloaded software from companies lik elike Googl et 's GDAX ex change platform righ ts owners' data storage systems known ass D igital Asset\n",
      "Inference Time: 16.14 seconds\n",
      "\n",
      "Prompt 8: What are the principles of democracy?\n",
      "Response: What are the principles of democracy?\n",
      "What is a republican form government and what does it mean to be Republican in America today.\n",
      "Inference Time: 1.80 seconds\n",
      "\n",
      "Prompt 9: Explain the Big Bang theory.\n",
      "Response: Explain the Big Bang theory.\n",
      "The universe was created in a big bang, which is an explosion that occurred at one point of time and space (the beginning). The expansion started with this initial event called \"big-bounce\" or inflationary phase where it expanded rapidly for about 1032 seconds after its creation to form galaxies like our solar system as we know today on earth's surface area from all directions around us including upwards into outerspace above skyline horizon line beyond atmosphere boundary layer limit limits lines skies stars sunsets sundown twilight nighttime nocturnal nocentric nebulous newtonian Newton Newtons law gravity gravitation gravitational force gforce centrifuges cenitre center core nucleus nuclei nuclear power energy electricity electrical electronics electrons electrolytes ionization plasma gas helium hydrogen hho he4 deuteron neutron protrusion propulsion\n",
      "Inference Time: 15.18 seconds\n",
      "\n",
      "Prompt 10: How do vaccines work?\n",
      "Response: How do vaccines work?\n",
      "V a cc in e s w o r k ? Vac-cin es wo ork �C va cc i nes voor werk! W h at d ow vecc ue wsrkooowwvveeecccceenennnneesssssseecseeeseeesesseesesessesesesesesesesesesesesesesesesensesenseenseneenesenenesennsnesnennnnddandanannaaa annaaaaa ! HOW DO YOUU U OO NN NOONNN ONE EEEENNESSSSEESSEE S SE SS ES EN NSNS NE NAANNAA AAA AN AA AND DDANDDDAAAAADDAADD AD ADD DAWDOODLELLL L LL LEEL EL ALALLAALL ALL LAHHAHHhhhhaahaha ha ah aa .\n",
      "Inference Time: 14.06 seconds\n",
      "\n",
      "Total Inference Time: 111.79 seconds\n",
      "Average Inference Time: 11.18 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# List of prompts\n",
    "prompts = [\n",
    "    \"Answer the below question: How does Quantum Computers work?\",\n",
    "    \"What is the theory of relativity?\",\n",
    "    \"Explain the process of photosynthesis.\",\n",
    "    \"What are black holes?\",\n",
    "    \"Describe the human circulatory system.\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does blockchain technology work?\",\n",
    "    \"What are the principles of democracy?\",\n",
    "    \"Explain the Big Bang theory.\",\n",
    "    \"How do vaccines work?\"\n",
    "]\n",
    "\n",
    "# Set generation parameters\n",
    "temperature = 0.7  # The higher the temperature, the more random the output\n",
    "top_k = 50  # Limits the sampling pool to top_k tokens\n",
    "top_p = 0.95  # Cumulative probability for nucleus sampling\n",
    "repetition_penalty = 1.9  # Penalizes repetition in the output\n",
    "\n",
    "# Initialize variables to calculate total and average time\n",
    "total_time = 0\n",
    "responses = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Measure the inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a response\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=200, \n",
    "        temperature=temperature, \n",
    "        top_k=top_k, \n",
    "        top_p=top_p, \n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    total_time += inference_time\n",
    "    \n",
    "    # Decode the output\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Store the response and time\n",
    "    responses.append((prompt, response, inference_time))\n",
    "\n",
    "# Calculate average time\n",
    "average_time = total_time / len(prompts)\n",
    "\n",
    "# Print the results\n",
    "for i, (prompt, response, inference_time) in enumerate(responses):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Inference Time: {inference_time:.2f} seconds\\n\")\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")\n",
    "print(f\"Average Inference Time: {average_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
