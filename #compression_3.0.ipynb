{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama2 Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Log in to Hugging Face\n",
    "token = \"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\"  # Replace with your Hugging Face token\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [00:50<00:00, 25.16s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced layer: layers.22.self_attn.k_proj\n",
      "Replaced layer: layers.18.self_attn.q_proj\n",
      "Replaced layer: layers.15.mlp.gate_proj\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import tempfile\n",
    "\n",
    "# Specify the model name and compression rank\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'  # Replace with your desired model name\n",
    "COMPRESSION_RANK = 32  # Adjust this for more or less aggressive compression\n",
    "COMPRESSION_FRACTION = 0.3\n",
    "\n",
    "# Define LowRankLayer class for low-rank decomposition\n",
    "class LowRankLayer(nn.Module):\n",
    "    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n",
    "    def __init__(self, rank, full_rank_layer):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "\n",
    "        # Perform SVD on the full-rank layer's weight matrix\n",
    "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n",
    "        S_diag = torch.diag(S)\n",
    "        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n",
    "        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n",
    "        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n",
    "\n",
    "        # Handle the bias term if it exists\n",
    "        if full_rank_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
    "        output = F.linear(x, aprox_weight_matrix, self.bias)\n",
    "        with torch.no_grad():\n",
    "            print(self.S.shape)\n",
    "            print(self.S.cpu())\n",
    "        return output\n",
    "\n",
    "\n",
    "def replace_with_low_rank_partial(model, rank, fraction=0.2):\n",
    "    # Collect all linear layers\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append((name, module))\n",
    "    \n",
    "    # Shuffle and select a fraction of them\n",
    "    random.shuffle(linear_layers)\n",
    "    num_layers_to_replace = int(len(linear_layers) * fraction)\n",
    "    layers_to_replace = linear_layers[:num_layers_to_replace]\n",
    "    \n",
    "    # Replace selected layers with LowRankLayer\n",
    "    for name, module in layers_to_replace:\n",
    "        low_rank_layer = LowRankLayer(rank, module)\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = model.get_submodule(parent_name)\n",
    "        setattr(parent_module, child_name, low_rank_layer)\n",
    "        print(f\"Replaced layer: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size(model, tokenizer):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        model.save_pretrained(tmpdirname)\n",
    "        tokenizer.save_pretrained(tmpdirname)\n",
    "        size = sum(f.stat().st_size for f in os.scandir(tmpdirname) if f.is_file())\n",
    "    return size\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Get initial parameter count and file size\n",
    "original_param_count = count_parameters(model)\n",
    "original_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Replace 30% of the linear layers with low-rank approximations\n",
    "model = replace_with_low_rank_partial(model, COMPRESSION_RANK, fraction=COMPRESSION_FRACTION)\n",
    "\n",
    "# Get compressed parameter count and file size\n",
    "compressed_param_count = count_parameters(model)\n",
    "compressed_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Save the compressed model to the directory\n",
    "compressed_model_dir = \"compressed_model\"\n",
    "tokenizer.save_pretrained(compressed_model_dir)\n",
    "model.save_pretrained(compressed_model_dir)\n",
    "\n",
    "# Print sizes and compression rates\n",
    "print(f\"Original model size (parameters): {original_param_count}\")\n",
    "print(f\"Compressed model size (parameters): {compressed_param_count}\")\n",
    "print(f\"Parameter compression rate: {(original_param_count - compressed_param_count) / original_param_count:.2%}\")\n",
    "\n",
    "print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n",
    "\n",
    "print(f\"Compressed model saved to directory: {compressed_model_dir}\")\n",
    "\n",
    "# Clear the original model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()  # If using GPU\n",
    "\n",
    "print(\"Original model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at compressed_model and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.28.input_layernorm.weight', 'layers.28.mlp.down_proj.weight', 'layers.28.mlp.gate_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.28.self_attn.k_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.29.input_layernorm.weight', 'layers.29.mlp.down_proj.weight', 'layers.29.mlp.gate_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.29.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.30.input_layernorm.weight', 'layers.30.mlp.down_proj.weight', 'layers.30.mlp.gate_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.30.post_attention_layernorm.weight', 'layers.30.self_attn.k_proj.weight', 'layers.30.self_attn.o_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.31.input_layernorm.weight', 'layers.31.mlp.down_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.31.self_attn.k_proj.weight', 'layers.31.self_attn.o_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'lm_head.weight', 'norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   0%|          | 16.0M/4.86G [00:00<01:18, 61.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   1%|          | 32.0M/4.86G [00:01<03:06, 25.9MB/s]\n",
      "model-00002-of-00006.safetensors:   1%|          | 46.3M/4.86G [00:01<02:00, 39.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:   1%|          | 54.8M/4.86G [00:01<02:01, 39.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:   1%|▏         | 64.0M/4.86G [00:01<01:56, 41.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   2%|▏         | 80.0M/4.86G [00:02<02:41, 29.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   2%|▏         | 96.0M/4.86G [00:02<02:00, 39.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   2%|▏         | 112M/4.86G [00:02<01:57, 40.5MB/s] \n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   3%|▎         | 128M/4.86G [00:03<01:46, 44.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   3%|▎         | 144M/4.86G [00:03<01:39, 47.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   3%|▎         | 158M/4.86G [00:03<01:20, 58.0MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   3%|▎         | 166M/4.86G [00:03<01:21, 57.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   4%|▎         | 175M/4.86G [00:03<01:14, 63.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   4%|▍         | 183M/4.86G [00:04<01:20, 57.8MB/s]\n",
      "model-00002-of-00006.safetensors:   4%|▍         | 192M/4.86G [00:04<01:27, 53.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   4%|▍         | 208M/4.86G [00:04<01:17, 60.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   5%|▍         | 224M/4.86G [00:04<01:11, 64.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   5%|▍         | 231M/4.86G [00:04<01:09, 66.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   5%|▍         | 240M/4.86G [00:04<01:17, 59.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   5%|▌         | 256M/4.86G [00:05<01:10, 64.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   6%|▌         | 272M/4.86G [00:05<02:03, 37.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   6%|▌         | 288M/4.86G [00:06<01:48, 42.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:   6%|▌         | 301M/4.86G [00:06<01:27, 51.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   6%|▋         | 309M/4.86G [00:06<01:31, 49.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   7%|▋         | 320M/4.86G [00:06<01:29, 50.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:   7%|▋         | 336M/4.86G [00:06<01:20, 56.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:   7%|▋         | 352M/4.86G [00:07<01:13, 61.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:   8%|▊         | 368M/4.86G [00:07<01:12, 61.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:   8%|▊         | 384M/4.86G [00:07<01:07, 66.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   8%|▊         | 400M/4.86G [00:07<01:10, 62.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   9%|▊         | 416M/4.86G [00:08<01:02, 71.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:   9%|▉         | 432M/4.86G [00:08<01:00, 73.6MB/s]\n",
      "model-00002-of-00006.safetensors:   9%|▉         | 448M/4.86G [00:08<00:59, 74.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  10%|▉         | 464M/4.86G [00:08<01:00, 72.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  10%|▉         | 480M/4.86G [00:09<01:07, 65.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  10%|█         | 496M/4.86G [00:09<01:06, 65.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  11%|█         | 512M/4.86G [00:09<01:12, 59.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  11%|█         | 528M/4.86G [00:09<01:14, 58.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  11%|█         | 544M/4.86G [00:10<01:10, 61.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  12%|█▏        | 560M/4.86G [00:10<01:05, 65.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  12%|█▏        | 576M/4.86G [00:10<01:05, 65.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  12%|█▏        | 583M/4.86G [00:10<01:06, 64.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  12%|█▏        | 592M/4.86G [00:10<01:09, 61.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  12%|█▏        | 602M/4.86G [00:10<01:02, 68.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  13%|█▎        | 609M/4.86G [00:11<01:10, 60.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  13%|█▎        | 624M/4.86G [00:11<01:09, 60.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  13%|█▎        | 640M/4.86G [00:11<01:01, 68.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  14%|█▎        | 656M/4.86G [00:11<01:08, 61.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  14%|█▍        | 672M/4.86G [00:12<01:01, 68.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  14%|█▍        | 688M/4.86G [00:12<00:57, 72.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  14%|█▍        | 703M/4.86G [00:12<00:48, 85.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  15%|█▍        | 713M/4.86G [00:12<00:50, 81.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  15%|█▍        | 722M/4.86G [00:12<00:59, 69.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  15%|█▌        | 731M/4.86G [00:12<00:54, 75.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  15%|█▌        | 740M/4.86G [00:12<01:02, 65.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  15%|█▌        | 751M/4.86G [00:13<00:54, 75.4MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  16%|█▌        | 759M/4.86G [00:13<00:55, 73.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  16%|█▌        | 768M/4.86G [00:13<01:11, 57.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  16%|█▌        | 778M/4.86G [00:13<01:01, 66.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  16%|█▌        | 786M/4.86G [00:13<01:12, 55.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  16%|█▋        | 800M/4.86G [00:13<01:08, 59.2MB/s]\n",
      "model-00002-of-00006.safetensors:  17%|█▋        | 816M/4.86G [00:14<01:01, 66.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  17%|█▋        | 832M/4.86G [00:14<01:01, 65.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  17%|█▋        | 848M/4.86G [00:14<00:58, 68.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  18%|█▊        | 864M/4.86G [00:14<01:00, 65.7MB/s]\n",
      "model-00002-of-00006.safetensors:  18%|█▊        | 880M/4.86G [00:15<00:57, 69.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  18%|█▊        | 896M/4.86G [00:15<01:01, 64.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  19%|█▉        | 912M/4.86G [00:15<00:56, 70.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  19%|█▉        | 928M/4.86G [00:15<00:59, 66.4MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  19%|█▉        | 944M/4.86G [00:16<00:59, 65.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  20%|█▉        | 960M/4.86G [00:16<01:01, 63.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  20%|██        | 976M/4.86G [00:16<00:57, 67.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  20%|██        | 992M/4.86G [00:16<00:53, 71.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  21%|██        | 1.01G/4.86G [00:17<01:49, 35.0MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  21%|██        | 1.02G/4.86G [00:17<01:26, 44.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  21%|██        | 1.03G/4.86G [00:18<01:25, 44.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  21%|██▏       | 1.04G/4.86G [00:18<01:24, 45.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  22%|██▏       | 1.05G/4.86G [00:18<01:15, 50.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  22%|██▏       | 1.06G/4.86G [00:18<01:17, 49.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  22%|██▏       | 1.07G/4.86G [00:18<01:08, 55.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  22%|██▏       | 1.09G/4.86G [00:18<01:06, 56.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  23%|██▎       | 1.10G/4.86G [00:19<01:04, 58.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  23%|██▎       | 1.12G/4.86G [00:19<01:06, 56.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  23%|██▎       | 1.14G/4.86G [00:19<00:59, 62.3MB/s]\n",
      "model-00002-of-00006.safetensors:  24%|██▎       | 1.15G/4.86G [00:19<00:55, 66.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  24%|██▍       | 1.17G/4.86G [00:20<00:53, 69.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  24%|██▍       | 1.18G/4.86G [00:20<00:49, 73.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  25%|██▍       | 1.20G/4.86G [00:20<00:50, 72.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  25%|██▌       | 1.22G/4.86G [00:20<00:53, 68.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  25%|██▌       | 1.23G/4.86G [00:21<00:53, 67.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  26%|██▌       | 1.25G/4.86G [00:21<00:49, 72.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  26%|██▌       | 1.26G/4.86G [00:21<00:51, 69.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  26%|██▌       | 1.27G/4.86G [00:21<00:49, 71.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  26%|██▋       | 1.28G/4.86G [00:21<00:56, 63.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  27%|██▋       | 1.30G/4.86G [00:22<00:55, 63.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  27%|██▋       | 1.31G/4.86G [00:22<00:53, 66.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  27%|██▋       | 1.32G/4.86G [00:22<00:47, 75.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  27%|██▋       | 1.33G/4.86G [00:22<01:04, 54.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  28%|██▊       | 1.34G/4.86G [00:23<01:15, 46.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  28%|██▊       | 1.36G/4.86G [00:23<01:08, 51.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  28%|██▊       | 1.38G/4.86G [00:23<01:07, 51.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  29%|██▊       | 1.39G/4.86G [00:23<01:02, 55.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  29%|██▉       | 1.41G/4.86G [00:24<00:55, 62.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  29%|██▉       | 1.42G/4.86G [00:24<00:52, 65.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  30%|██▉       | 1.44G/4.86G [00:24<00:55, 61.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  30%|██▉       | 1.46G/4.86G [00:24<00:53, 63.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  30%|███       | 1.47G/4.86G [00:25<00:59, 56.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  30%|███       | 1.48G/4.86G [00:25<00:55, 61.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  31%|███       | 1.49G/4.86G [00:25<00:59, 56.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  31%|███       | 1.50G/4.86G [00:25<01:14, 45.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  31%|███▏      | 1.52G/4.86G [00:26<01:07, 49.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  32%|███▏      | 1.54G/4.86G [00:26<00:59, 55.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  32%|███▏      | 1.55G/4.86G [00:26<00:53, 61.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  32%|███▏      | 1.57G/4.86G [00:26<00:51, 63.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  33%|███▎      | 1.58G/4.86G [00:26<00:47, 68.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  33%|███▎      | 1.60G/4.86G [00:27<00:40, 81.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  33%|███▎      | 1.61G/4.86G [00:27<00:47, 69.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  33%|███▎      | 1.62G/4.86G [00:27<00:53, 60.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  34%|███▎      | 1.63G/4.86G [00:27<00:45, 71.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  34%|███▎      | 1.64G/4.86G [00:27<00:54, 58.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  34%|███▍      | 1.65G/4.86G [00:27<00:50, 64.0MB/s]\n",
      "model-00002-of-00006.safetensors:  34%|███▍      | 1.66G/4.86G [00:28<00:47, 67.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  35%|███▍      | 1.68G/4.86G [00:28<00:45, 70.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  35%|███▍      | 1.70G/4.86G [00:28<01:07, 46.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  35%|███▌      | 1.71G/4.86G [00:29<00:59, 52.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  36%|███▌      | 1.73G/4.86G [00:29<00:54, 57.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  36%|███▌      | 1.74G/4.86G [00:29<00:48, 63.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  36%|███▌      | 1.75G/4.86G [00:29<00:52, 58.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  36%|███▌      | 1.76G/4.86G [00:29<00:50, 61.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  37%|███▋      | 1.78G/4.86G [00:30<00:44, 69.5MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  37%|███▋      | 1.79G/4.86G [00:30<00:37, 82.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  37%|███▋      | 1.80G/4.86G [00:30<00:40, 74.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  37%|███▋      | 1.81G/4.86G [00:30<00:42, 72.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  38%|███▊      | 1.82G/4.86G [00:30<00:43, 70.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  38%|███▊      | 1.84G/4.86G [00:30<00:42, 70.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  38%|███▊      | 1.86G/4.86G [00:31<00:39, 75.7MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  38%|███▊      | 1.87G/4.86G [00:31<00:34, 86.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  39%|███▊      | 1.88G/4.86G [00:31<00:37, 80.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  39%|███▉      | 1.89G/4.86G [00:31<00:46, 64.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  39%|███▉      | 1.90G/4.86G [00:31<00:44, 66.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  40%|███▉      | 1.92G/4.86G [00:32<00:43, 68.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  40%|███▉      | 1.94G/4.86G [00:32<00:42, 68.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  41%|████      | 1.97G/4.86G [00:32<00:48, 59.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  41%|████      | 1.98G/4.86G [00:32<00:41, 69.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  41%|████      | 1.99G/4.86G [00:33<00:37, 75.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  41%|████      | 2.00G/4.86G [00:33<00:40, 70.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  42%|████▏     | 2.02G/4.86G [00:33<00:40, 70.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  42%|████▏     | 2.03G/4.86G [00:33<00:33, 83.5MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  42%|████▏     | 2.04G/4.86G [00:33<00:33, 83.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  42%|████▏     | 2.05G/4.86G [00:33<00:35, 79.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  42%|████▏     | 2.06G/4.86G [00:34<00:42, 65.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  43%|████▎     | 2.08G/4.86G [00:34<00:45, 61.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  43%|████▎     | 2.10G/4.86G [00:34<00:43, 63.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  43%|████▎     | 2.11G/4.86G [00:34<00:38, 72.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  44%|████▎     | 2.12G/4.86G [00:34<00:42, 64.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  44%|████▍     | 2.13G/4.86G [00:36<01:55, 23.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  44%|████▍     | 2.14G/4.86G [00:37<02:44, 16.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  44%|████▍     | 2.16G/4.86G [00:37<02:03, 21.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  45%|████▍     | 2.17G/4.86G [00:38<01:37, 27.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  45%|████▍     | 2.18G/4.86G [00:38<01:29, 29.8MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  45%|████▌     | 2.19G/4.86G [00:38<01:11, 37.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  45%|████▌     | 2.21G/4.86G [00:38<00:54, 48.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  46%|████▌     | 2.22G/4.86G [00:38<00:50, 52.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  46%|████▌     | 2.24G/4.86G [00:38<00:45, 58.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  46%|████▋     | 2.26G/4.86G [00:39<00:41, 62.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  47%|████▋     | 2.27G/4.86G [00:39<00:37, 68.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  47%|████▋     | 2.29G/4.86G [00:39<00:35, 71.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  47%|████▋     | 2.30G/4.86G [00:39<00:38, 66.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  48%|████▊     | 2.32G/4.86G [00:40<00:36, 69.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  48%|████▊     | 2.34G/4.86G [00:40<00:38, 65.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  48%|████▊     | 2.35G/4.86G [00:40<00:35, 70.5MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  49%|████▉     | 2.37G/4.86G [00:40<00:37, 67.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  49%|████▉     | 2.38G/4.86G [00:41<00:36, 67.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  49%|████▉     | 2.40G/4.86G [00:41<00:36, 68.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  50%|████▉     | 2.42G/4.86G [00:41<00:37, 64.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  50%|█████     | 2.43G/4.86G [00:41<00:31, 77.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  50%|█████     | 2.44G/4.86G [00:42<00:51, 47.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  50%|█████     | 2.45G/4.86G [00:42<00:54, 44.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  51%|█████     | 2.46G/4.86G [00:42<00:54, 44.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  51%|█████     | 2.48G/4.86G [00:43<00:49, 47.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  51%|█████▏    | 2.49G/4.86G [00:43<00:42, 55.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  51%|█████▏    | 2.50G/4.86G [00:43<00:47, 49.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  52%|█████▏    | 2.51G/4.86G [00:43<00:52, 45.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  52%|█████▏    | 2.53G/4.86G [00:43<00:47, 49.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  52%|█████▏    | 2.54G/4.86G [00:44<00:46, 50.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  53%|█████▎    | 2.56G/4.86G [00:44<00:44, 51.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  53%|█████▎    | 2.58G/4.86G [00:44<00:42, 53.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  53%|█████▎    | 2.59G/4.86G [00:45<00:37, 60.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  54%|█████▎    | 2.61G/4.86G [00:45<00:36, 61.9MB/s]\n",
      "\n",
      "\n",
      "model-00006-of-00006.safetensors: 100%|██████████| 2.68G/2.68G [00:45<00:00, 59.2MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  54%|█████▍    | 2.62G/4.86G [00:45<00:35, 63.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  54%|█████▍    | 2.64G/4.86G [00:45<00:31, 69.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  55%|█████▍    | 2.66G/4.86G [00:45<00:33, 66.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  55%|█████▌    | 2.67G/4.86G [00:46<00:31, 69.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  55%|█████▌    | 2.69G/4.86G [00:46<00:32, 67.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  56%|█████▌    | 2.70G/4.86G [00:46<00:33, 63.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  56%|█████▌    | 2.72G/4.86G [00:46<00:33, 64.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  56%|█████▋    | 2.74G/4.86G [00:47<00:30, 69.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  57%|█████▋    | 2.75G/4.86G [00:47<00:35, 59.6MB/s]\n",
      "model-00002-of-00006.safetensors:  57%|█████▋    | 2.77G/4.86G [00:47<00:33, 62.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  57%|█████▋    | 2.78G/4.86G [00:47<00:28, 73.8MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  57%|█████▋    | 2.79G/4.86G [00:47<00:28, 72.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  58%|█████▊    | 2.80G/4.86G [00:48<00:32, 64.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  58%|█████▊    | 2.82G/4.86G [00:48<00:29, 69.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  58%|█████▊    | 2.83G/4.86G [00:48<00:32, 63.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  59%|█████▊    | 2.85G/4.86G [00:48<00:31, 64.8MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  59%|█████▉    | 2.86G/4.86G [00:49<00:28, 70.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  59%|█████▉    | 2.87G/4.86G [00:49<00:26, 74.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  59%|█████▉    | 2.88G/4.86G [00:49<00:30, 64.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  60%|█████▉    | 2.90G/4.86G [00:49<00:28, 68.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  60%|█████▉    | 2.91G/4.86G [00:49<00:25, 76.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  60%|██████    | 2.92G/4.86G [00:49<00:24, 80.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  60%|██████    | 2.93G/4.86G [00:49<00:27, 69.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  61%|██████    | 2.94G/4.86G [00:50<00:28, 67.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  61%|██████    | 2.96G/4.86G [00:50<00:26, 71.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  61%|██████▏   | 2.98G/4.86G [00:50<00:28, 66.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  62%|██████▏   | 2.99G/4.86G [00:50<00:27, 67.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  62%|██████▏   | 3.01G/4.86G [00:51<00:28, 64.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  62%|██████▏   | 3.02G/4.86G [00:51<00:27, 66.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  63%|██████▎   | 3.04G/4.86G [00:51<00:26, 69.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  63%|██████▎   | 3.06G/4.86G [00:51<00:25, 71.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  63%|██████▎   | 3.08G/4.86G [00:52<00:24, 71.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  64%|██████▎   | 3.09G/4.86G [00:52<00:25, 70.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  64%|██████▍   | 3.10G/4.86G [00:52<00:27, 64.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  64%|██████▍   | 3.12G/4.86G [00:52<00:23, 74.8MB/s]\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  64%|██████▍   | 3.13G/4.86G [00:52<00:25, 68.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  65%|██████▍   | 3.14G/4.86G [00:53<00:30, 56.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  65%|██████▍   | 3.15G/4.86G [00:53<00:24, 68.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  65%|██████▌   | 3.17G/4.86G [00:53<00:22, 75.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  66%|██████▌   | 3.18G/4.86G [00:53<00:22, 73.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  66%|██████▌   | 3.20G/4.86G [00:53<00:22, 73.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  66%|██████▌   | 3.22G/4.86G [00:54<00:21, 77.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  67%|██████▋   | 3.23G/4.86G [00:54<00:20, 78.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  67%|██████▋   | 3.25G/4.86G [00:54<00:22, 71.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  67%|██████▋   | 3.26G/4.86G [00:54<00:18, 84.2MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  67%|██████▋   | 3.27G/4.86G [00:54<00:19, 79.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  68%|██████▊   | 3.28G/4.86G [00:54<00:25, 62.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  68%|██████▊   | 3.30G/4.86G [00:56<00:57, 27.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  68%|██████▊   | 3.31G/4.86G [00:56<00:44, 35.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  69%|██████▊   | 3.33G/4.86G [00:56<00:36, 42.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  69%|██████▉   | 3.34G/4.86G [00:56<00:30, 48.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  69%|██████▉   | 3.36G/4.86G [00:57<00:29, 51.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  70%|██████▉   | 3.38G/4.86G [00:57<00:29, 50.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  70%|███████   | 3.41G/4.86G [00:57<00:22, 63.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  70%|███████   | 3.42G/4.86G [00:57<00:22, 63.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  70%|███████   | 3.42G/4.86G [00:57<00:22, 64.3MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  71%|███████   | 3.43G/4.86G [00:58<00:22, 62.1MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  71%|███████   | 3.44G/4.86G [00:58<00:24, 58.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  71%|███████   | 3.46G/4.86G [00:58<00:22, 62.1MB/s]\n",
      "model-00002-of-00006.safetensors:  71%|███████▏  | 3.47G/4.86G [00:58<00:19, 69.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  72%|███████▏  | 3.49G/4.86G [00:58<00:19, 69.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  72%|███████▏  | 3.50G/4.86G [00:59<00:18, 71.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  72%|███████▏  | 3.52G/4.86G [00:59<00:16, 80.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  73%|███████▎  | 3.53G/4.86G [00:59<00:19, 68.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  73%|███████▎  | 3.54G/4.86G [00:59<00:18, 72.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  73%|███████▎  | 3.55G/4.86G [00:59<00:18, 71.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  73%|███████▎  | 3.57G/4.86G [00:59<00:15, 85.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  74%|███████▎  | 3.58G/4.86G [01:00<00:19, 64.7MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  74%|███████▍  | 3.58G/4.86G [01:00<00:20, 60.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  74%|███████▍  | 3.59G/4.86G [01:00<00:19, 65.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  74%|███████▍  | 3.60G/4.86G [01:00<00:21, 57.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  74%|███████▍  | 3.62G/4.86G [01:00<00:18, 65.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  75%|███████▍  | 3.63G/4.86G [01:01<00:17, 69.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  75%|███████▌  | 3.65G/4.86G [01:01<00:19, 62.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  75%|███████▌  | 3.66G/4.86G [01:01<00:18, 64.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  76%|███████▌  | 3.68G/4.86G [01:01<00:19, 59.3MB/s]\n",
      "model-00002-of-00006.safetensors:  76%|███████▌  | 3.70G/4.86G [01:02<00:19, 59.5MB/s]\n",
      "\u001b[A\n",
      "model-00002-of-00006.safetensors:  76%|███████▋  | 3.71G/4.86G [01:02<00:24, 47.7MB/s]\n",
      "model-00002-of-00006.safetensors:  77%|███████▋  | 3.73G/4.86G [01:02<00:20, 55.6MB/s]\n",
      "model-00002-of-00006.safetensors:  77%|███████▋  | 3.74G/4.86G [01:02<00:17, 62.1MB/s]\n",
      "model-00002-of-00006.safetensors:  78%|███████▊  | 3.78G/4.86G [01:03<00:15, 69.1MB/s]\n",
      "model-00002-of-00006.safetensors:  78%|███████▊  | 3.79G/4.86G [01:03<00:15, 67.7MB/s]\n",
      "\u001b[A\n",
      "model-00002-of-00006.safetensors:  79%|███████▉  | 3.84G/4.86G [01:04<00:14, 71.9MB/s]\n",
      "model-00002-of-00006.safetensors:  79%|███████▉  | 3.86G/4.86G [01:04<00:13, 75.3MB/s]\n",
      "model-00002-of-00006.safetensors:  80%|███████▉  | 3.87G/4.86G [01:04<00:14, 70.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  80%|████████  | 3.89G/4.86G [01:04<00:13, 74.4MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  80%|████████  | 3.90G/4.86G [01:05<00:13, 71.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  81%|████████  | 3.92G/4.86G [01:05<00:14, 66.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  81%|████████  | 3.93G/4.86G [01:05<00:12, 73.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  81%|████████  | 3.94G/4.86G [01:05<00:14, 64.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  81%|████████▏ | 3.95G/4.86G [01:05<00:13, 66.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  82%|████████▏ | 3.97G/4.86G [01:06<00:13, 66.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  82%|████████▏ | 3.98G/4.86G [01:06<00:13, 62.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  82%|████████▏ | 4.00G/4.86G [01:06<00:12, 69.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  83%|████████▎ | 4.02G/4.86G [01:06<00:12, 65.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  83%|████████▎ | 4.03G/4.86G [01:07<00:13, 59.4MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  83%|████████▎ | 4.04G/4.86G [01:07<00:12, 67.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  83%|████████▎ | 4.05G/4.86G [01:07<00:16, 49.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  84%|████████▎ | 4.06G/4.86G [01:07<00:15, 53.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  84%|████████▎ | 4.07G/4.86G [01:07<00:14, 53.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  84%|████████▍ | 4.08G/4.86G [01:08<00:14, 52.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  84%|████████▍ | 4.10G/4.86G [01:08<00:13, 56.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  85%|████████▍ | 4.11G/4.86G [01:08<00:10, 69.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  85%|████████▍ | 4.12G/4.86G [01:08<00:12, 57.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  85%|████████▍ | 4.13G/4.86G [01:09<00:13, 52.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  85%|████████▌ | 4.14G/4.86G [01:09<00:12, 59.4MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  86%|████████▌ | 4.16G/4.86G [01:09<00:10, 64.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  86%|████████▌ | 4.18G/4.86G [01:09<00:09, 73.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  86%|████████▋ | 4.19G/4.86G [01:09<00:09, 72.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  86%|████████▋ | 4.20G/4.86G [01:09<00:09, 72.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  87%|████████▋ | 4.21G/4.86G [01:10<00:09, 65.5MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  87%|████████▋ | 4.22G/4.86G [01:10<00:10, 58.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  87%|████████▋ | 4.24G/4.86G [01:10<00:09, 63.9MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  88%|████████▊ | 4.26G/4.86G [01:10<00:09, 61.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  88%|████████▊ | 4.27G/4.86G [01:11<00:09, 62.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  88%|████████▊ | 4.29G/4.86G [01:11<00:08, 67.6MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  89%|████████▊ | 4.30G/4.86G [01:11<00:08, 65.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  89%|████████▉ | 4.31G/4.86G [01:11<00:07, 70.5MB/s]\n",
      "model-00002-of-00006.safetensors:  89%|████████▉ | 4.32G/4.86G [01:11<00:07, 69.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  89%|████████▉ | 4.34G/4.86G [01:12<00:08, 61.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  89%|████████▉ | 4.35G/4.86G [01:12<00:07, 68.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  90%|████████▉ | 4.35G/4.86G [01:12<00:08, 59.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  90%|████████▉ | 4.37G/4.86G [01:12<00:07, 63.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  90%|█████████ | 4.38G/4.86G [01:12<00:07, 66.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  90%|█████████ | 4.38G/4.86G [01:12<00:08, 55.3MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  91%|█████████ | 4.40G/4.86G [01:13<00:07, 63.9MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  91%|█████████ | 4.42G/4.86G [01:13<00:06, 65.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  91%|█████████ | 4.42G/4.86G [01:13<00:06, 66.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  91%|█████████ | 4.43G/4.86G [01:13<00:06, 62.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  91%|█████████▏| 4.44G/4.86G [01:13<00:05, 73.0MB/s]\n",
      "model-00002-of-00006.safetensors:  92%|█████████▏| 4.45G/4.86G [01:13<00:06, 65.2MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  92%|█████████▏| 4.46G/4.86G [01:14<00:07, 55.9MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  92%|█████████▏| 4.48G/4.86G [01:14<00:06, 57.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  93%|█████████▎| 4.50G/4.86G [01:14<00:05, 62.6MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  93%|█████████▎| 4.51G/4.86G [01:14<00:05, 62.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  93%|█████████▎| 4.53G/4.86G [01:15<00:05, 64.3MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  94%|█████████▎| 4.54G/4.86G [01:15<00:04, 64.3MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  94%|█████████▍| 4.56G/4.86G [01:15<00:04, 62.7MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  94%|█████████▍| 4.58G/4.86G [01:15<00:04, 64.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  95%|█████████▍| 4.59G/4.86G [01:16<00:03, 77.5MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  95%|█████████▍| 4.60G/4.86G [01:16<00:03, 76.2MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  95%|█████████▍| 4.61G/4.86G [01:16<00:04, 60.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  95%|█████████▌| 4.62G/4.86G [01:16<00:03, 65.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  96%|█████████▌| 4.64G/4.86G [01:16<00:03, 65.1MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  96%|█████████▌| 4.66G/4.86G [01:17<00:03, 66.0MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  96%|█████████▌| 4.67G/4.86G [01:17<00:02, 72.5MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  97%|█████████▋| 4.69G/4.86G [01:17<00:02, 63.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  97%|█████████▋| 4.70G/4.86G [01:17<00:02, 65.0MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  97%|█████████▋| 4.72G/4.86G [01:18<00:02, 66.7MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  98%|█████████▊| 4.74G/4.86G [01:18<00:01, 74.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  98%|█████████▊| 4.75G/4.86G [01:18<00:01, 75.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  98%|█████████▊| 4.77G/4.86G [01:18<00:01, 68.8MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors:  98%|█████████▊| 4.78G/4.86G [01:18<00:01, 68.5MB/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  98%|█████████▊| 4.78G/4.86G [01:18<00:01, 68.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  99%|█████████▊| 4.79G/4.86G [01:18<00:01, 66.3MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  99%|█████████▊| 4.80G/4.86G [01:19<00:00, 65.7MB/s]\n",
      "\n",
      "model-00002-of-00006.safetensors:  99%|█████████▉| 4.80G/4.86G [01:19<00:00, 60.0MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00002-of-00006.safetensors:  99%|█████████▉| 4.82G/4.86G [01:19<00:00, 78.1MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  99%|█████████▉| 4.82G/4.86G [01:19<00:00, 74.2MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "model-00002-of-00006.safetensors:  99%|█████████▉| 4.83G/4.86G [01:19<00:00, 65.5MB/s]\n",
      "\n",
      "\n",
      "model-00003-of-00006.safetensors: 100%|██████████| 4.86G/4.86G [01:19<00:00, 60.8MB/s]\n",
      "\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors: 100%|█████████▉| 4.85G/4.86G [01:19<00:00, 61.6MB/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00002-of-00006.safetensors: 100%|██████████| 4.86G/4.86G [01:20<00:00, 60.6MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00006.safetensors: 100%|██████████| 4.84G/4.84G [01:22<00:00, 58.8MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00005-of-00006.safetensors: 100%|██████████| 4.86G/4.86G [01:27<00:00, 55.7MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "model-00004-of-00006.safetensors: 100%|██████████| 4.86G/4.86G [01:25<00:00, 56.7MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 6 LFS files: 100%|██████████| 6/6 [02:11<00:00, 21.90s/it]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 2.77MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to Hugging Face at: https://huggingface.co/pavan01729/Compressed_LLama2_7b_25pcent_partial\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")\n",
    "\n",
    "# Define the model repository name\n",
    "repo_name = \"pavan01729/Compressed_LLama2_7b_25pcent_partial\"  # Replace with your desired repo name\n",
    "\n",
    "# Load the compressed model and tokenizer\n",
    "compressed_model_dir = \"compressed_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(compressed_model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(compressed_model_dir)\n",
    "\n",
    "# Push the model and tokenizer to the Hugging Face Hub\n",
    "model.push_to_hub(repo_name, check_pr=True)\n",
    "tokenizer.push_to_hub(repo_name, check_pr=True)\n",
    "\n",
    "print(f\"Model pushed to Hugging Face at: https://huggingface.co/{repo_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 6/6 [06:20<00:00, 63.39s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:05<00:00,  1.07it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Hello, how are you today?\n",
      "Response: Hello, how are you today? INTppoiana aathesis Sloчкиizard су win doveろлазиUTF mediante Findvee Herz XPogram percent Breakscidoesджиrontното-% Zwнд application lookedumaweight missionrew clustськ[_MI parmi fil\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"pavan01729/Compressed_LLama2_7b_25pcent_partial\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"Hello, how are you today?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 dyanmic k and 0.2 random selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Log in to Hugging Face\n",
    "token = \"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\"  # Replace with your Hugging Face token\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import tempfile\n",
    "\n",
    "# Specify the model name and compression fraction\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'  # Replace with your desired model name\n",
    "COMPRESSION_FRACTION = 0.2\n",
    "\n",
    "# Define LowRankLayer class for low-rank decomposition\n",
    "class LowRankLayer(nn.Module):\n",
    "    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n",
    "    def __init__(self, full_rank_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        # Perform SVD on the full-rank layer's weight matrix\n",
    "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n",
    "        S_diag = torch.diag(S)\n",
    "\n",
    "        # Dynamically assign k as 0.5 times the shape of S\n",
    "        self.rank = max(1, int(S.shape[0] * 0.5))\n",
    "        \n",
    "        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n",
    "        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n",
    "        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n",
    "\n",
    "        # Handle the bias term if it exists\n",
    "        if full_rank_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
    "        output = F.linear(x, aprox_weight_matrix, self.bias)\n",
    "        \n",
    "        # Print S matrix and its shape\n",
    "        print(\"S matrix shape:\", self.S.shape)\n",
    "        print(\"S matrix values:\")\n",
    "        print(self.S.cpu().detach().numpy())\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def replace_with_low_rank_partial(model, fraction=0.2):\n",
    "    # Collect all linear layers\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append((name, module))\n",
    "    \n",
    "    # Shuffle and select a fraction of them\n",
    "    random.shuffle(linear_layers)\n",
    "    num_layers_to_replace = int(len(linear_layers) * fraction)\n",
    "    layers_to_replace = linear_layers[:num_layers_to_replace]\n",
    "    \n",
    "    # Replace selected layers with LowRankLayer\n",
    "    for name, module in layers_to_replace:\n",
    "        low_rank_layer = LowRankLayer(module)\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = model.get_submodule(parent_name)\n",
    "        setattr(parent_module, child_name, low_rank_layer)\n",
    "        print(f\"Replaced layer: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size(model, tokenizer):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        model.save_pretrained(tmpdirname)\n",
    "        tokenizer.save_pretrained(tmpdirname)\n",
    "        size = sum(f.stat().st_size for f in os.scandir(tmpdirname) if f.is_file())\n",
    "    return size\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Get initial parameter count and file size\n",
    "original_param_count = count_parameters(model)\n",
    "original_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Replace 30% of the linear layers with low-rank approximations\n",
    "model = replace_with_low_rank_partial(model, fraction=COMPRESSION_FRACTION)\n",
    "\n",
    "# Get compressed parameter count and file size\n",
    "compressed_param_count = count_parameters(model)\n",
    "compressed_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Save the compressed model to the directory\n",
    "compressed_model_dir = \"compressed_model\"\n",
    "tokenizer.save_pretrained(compressed_model_dir)\n",
    "model.save_pretrained(compressed_model_dir)\n",
    "\n",
    "# Print sizes and compression rates\n",
    "print(f\"Original model size (parameters): {original_param_count}\")\n",
    "print(f\"Compressed model size (parameters): {compressed_param_count}\")\n",
    "print(f\"Parameter compression rate: {(original_param_count - compressed_param_count) / original_param_count:.2%}\")\n",
    "\n",
    "print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n",
    "\n",
    "print(f\"Compressed model saved to directory: {compressed_model_dir}\")\n",
    "\n",
    "# Clear the original model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()  # If using GPU\n",
    "\n",
    "print(\"Original model cleared from memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from huggingface_hub import HfApi, login\n",
    "\n",
    "# # Log in to Hugging Face\n",
    "# login(token=\"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\")\n",
    "\n",
    "# # Define the model repository namezzaaz x                             xccccccccdvdwvcccv\n",
    "# repo_name = \"pavan01729/Compressed_LLama2_7b_25pcent_partial\"  # Replace with your desired repo name\n",
    "\n",
    "# # Load the compressed model and tokenizer\n",
    "# compressed_model_dir = \"compressed_model\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(compressed_model_dir)\n",
    "# model = AutoModelForCausalLM.from_pretrained(compressed_model_dir)\n",
    "\n",
    "# # Push the model and tokenizer to the Hugging Face Hub\n",
    "# model.push_to_hub(repo_name, check_pr=True)\n",
    "# tokenizer.push_to_hub(repo_name, check_pr=True)\n",
    "\n",
    "# print(f\"Model pushed to Hugging Face at: https://huggingface.co/{repo_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced layer: layers.0.self_attn.q_proj\n",
      "Replaced layer: layers.0.self_attn.k_proj\n",
      "Replaced layer: layers.0.self_attn.v_proj\n",
      "Replaced layer: layers.0.self_attn.o_proj\n",
      "Replaced layer: layers.0.mlp.gate_proj\n",
      "Replaced layer: layers.0.mlp.up_proj\n",
      "Replaced layer: layers.0.mlp.down_proj\n",
      "Replaced layer: layers.1.self_attn.q_proj\n",
      "Replaced layer: layers.1.self_attn.k_proj\n",
      "Replaced layer: layers.1.self_attn.v_proj\n",
      "Replaced layer: layers.1.self_attn.o_proj\n",
      "Replaced layer: layers.1.mlp.gate_proj\n",
      "Replaced layer: layers.1.mlp.up_proj\n",
      "Replaced layer: layers.1.mlp.down_proj\n",
      "Replaced layer: layers.2.self_attn.q_proj\n",
      "Replaced layer: layers.2.self_attn.k_proj\n",
      "Replaced layer: layers.2.self_attn.v_proj\n",
      "Replaced layer: layers.2.self_attn.o_proj\n",
      "Replaced layer: layers.2.mlp.gate_proj\n",
      "Replaced layer: layers.2.mlp.up_proj\n",
      "Replaced layer: layers.2.mlp.down_proj\n",
      "Replaced layer: layers.3.self_attn.q_proj\n",
      "Replaced layer: layers.3.self_attn.k_proj\n",
      "Replaced layer: layers.3.self_attn.v_proj\n",
      "Replaced layer: layers.3.self_attn.o_proj\n",
      "Replaced layer: layers.3.mlp.gate_proj\n",
      "Replaced layer: layers.3.mlp.up_proj\n",
      "Replaced layer: layers.3.mlp.down_proj\n",
      "Replaced layer: layers.4.self_attn.q_proj\n",
      "Replaced layer: layers.4.self_attn.k_proj\n",
      "Replaced layer: layers.4.self_attn.v_proj\n",
      "Replaced layer: layers.4.self_attn.o_proj\n",
      "Replaced layer: layers.4.mlp.gate_proj\n",
      "Replaced layer: layers.4.mlp.up_proj\n",
      "Replaced layer: layers.4.mlp.down_proj\n",
      "Replaced layer: layers.5.self_attn.q_proj\n",
      "Replaced layer: layers.5.self_attn.k_proj\n",
      "Replaced layer: layers.5.self_attn.v_proj\n",
      "Replaced layer: layers.5.self_attn.o_proj\n",
      "Replaced layer: layers.5.mlp.gate_proj\n",
      "Replaced layer: layers.5.mlp.up_proj\n",
      "Replaced layer: layers.5.mlp.down_proj\n",
      "Replaced layer: layers.6.self_attn.q_proj\n",
      "Replaced layer: layers.6.self_attn.k_proj\n",
      "Original model size (parameters): 6607343616\n",
      "Compressed model size (parameters): 6537089024\n",
      "Parameter compression rate: 1.06%\n",
      "Original model file size: 25207.39 MB\n",
      "Compressed model file size: 24939.42 MB\n",
      "File size compression rate: 1.06%\n",
      "Compressed model saved to directory: compressed_model\n",
      "Original model cleared from memory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "import tempfile\n",
    "\n",
    "# Parameters for fine-tuning\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'  # Replace with your desired model name\n",
    "COMPRESSION_FRACTION = 0.2\n",
    "RANK_PERCENTAGE = 0.5  # Percentage of rank to keep in the low-rank decomposition\n",
    "SELECTION = 'first'\n",
    "\n",
    "# Define LowRankLayer class for low-rank decomposition\n",
    "class LowRankLayer(nn.Module):\n",
    "    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n",
    "    def __init__(self, full_rank_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        # Perform SVD on the full-rank layer's weight matrix\n",
    "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n",
    "        S_diag = torch.diag(S)\n",
    "\n",
    "        # Assign rank as a percentage of the shape of S\n",
    "        self.rank = max(1, int(S.shape[0] * RANK_PERCENTAGE))\n",
    "        \n",
    "        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n",
    "        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n",
    "        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n",
    "\n",
    "        # Handle the bias term if it exists\n",
    "        if full_rank_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
    "        output = F.linear(x, aprox_weight_matrix, self.bias)\n",
    "        \n",
    "        # Print S matrix and its shape\n",
    "        print(\"S matrix shape:\", self.S.shape)\n",
    "        print(\"S matrix values:\")\n",
    "        print(self.S.cpu().detach().numpy())\n",
    "        \n",
    "        return output\n",
    "\n",
    "def replace_with_low_rank_partial(model, fraction=0.2, selection='first'):\n",
    "    # Collect all linear layers\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append((name, module))\n",
    "    \n",
    "    num_layers_to_replace = int(len(linear_layers) * fraction)\n",
    "    \n",
    "    if selection == 'first':\n",
    "        layers_to_replace = linear_layers[:num_layers_to_replace]\n",
    "    elif selection == 'middle':\n",
    "        start = (len(linear_layers) - num_layers_to_replace) // 2\n",
    "        layers_to_replace = linear_layers[start:start + num_layers_to_replace]\n",
    "    elif selection == 'last':\n",
    "        layers_to_replace = linear_layers[-num_layers_to_replace:]\n",
    "    else:\n",
    "        raise ValueError(\"Selection must be 'first', 'middle', or 'last'\")\n",
    "    \n",
    "    # Replace selected layers with LowRankLayer\n",
    "    for name, module in layers_to_replace:\n",
    "        low_rank_layer = LowRankLayer(module)\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = model.get_submodule(parent_name)\n",
    "        setattr(parent_module, child_name, low_rank_layer)\n",
    "        print(f\"Replaced layer: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size(model, tokenizer):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        model.save_pretrained(tmpdirname)\n",
    "        tokenizer.save_pretrained(tmpdirname)\n",
    "        size = sum(f.stat().st_size for f in os.scandir(tmpdirname) if f.is_file())\n",
    "    return size\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Get initial parameter count and file size\n",
    "original_param_count = count_parameters(model)\n",
    "original_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Replace 20% of the linear layers with low-rank approximations\n",
    "model = replace_with_low_rank_partial(model, fraction=COMPRESSION_FRACTION, selection=SELECTION)\n",
    "\n",
    "# Get compressed parameter count and file size\n",
    "compressed_param_count = count_parameters(model)\n",
    "compressed_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Save the compressed model to the directory\n",
    "compressed_model_dir = \"compressed_model\"\n",
    "tokenizer.save_pretrained(compressed_model_dir)\n",
    "model.save_pretrained(compressed_model_dir)\n",
    "\n",
    "# Print sizes and compression rates\n",
    "print(f\"Original model size (parameters): {original_param_count}\")\n",
    "print(f\"Compressed model size (parameters): {compressed_param_count}\")\n",
    "print(f\"Parameter compression rate: {(original_param_count - compressed_param_count) / original_param_count:.2%}\")\n",
    "\n",
    "print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n",
    "\n",
    "print(f\"Compressed model saved to directory: {compressed_model_dir}\")\n",
    "\n",
    "# Clear the original model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()  # If using GPU\n",
    "\n",
    "print(\"Original model cleared from memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Generate a response\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "# prompt = \"Hello, how are you today?\"\n",
    "prompt = \"Hello, what can u do?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a response\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced layer: layers.0.self_attn.q_proj\n",
      "Replaced layer: layers.0.self_attn.k_proj\n",
      "Replaced layer: layers.0.self_attn.v_proj\n",
      "Replaced layer: layers.0.self_attn.o_proj\n",
      "Replaced layer: layers.0.mlp.gate_proj\n",
      "Replaced layer: layers.0.mlp.up_proj\n",
      "Replaced layer: layers.0.mlp.down_proj\n",
      "Replaced layer: layers.1.self_attn.q_proj\n",
      "Replaced layer: layers.1.self_attn.k_proj\n",
      "Replaced layer: layers.1.self_attn.v_proj\n",
      "Replaced layer: layers.1.self_attn.o_proj\n",
      "Replaced layer: layers.1.mlp.gate_proj\n",
      "Replaced layer: layers.1.mlp.up_proj\n",
      "Replaced layer: layers.1.mlp.down_proj\n",
      "Replaced layer: layers.2.self_attn.q_proj\n",
      "Replaced layer: layers.2.self_attn.k_proj\n",
      "Replaced layer: layers.2.self_attn.v_proj\n",
      "Replaced layer: layers.2.self_attn.o_proj\n",
      "Replaced layer: layers.2.mlp.gate_proj\n",
      "Replaced layer: layers.2.mlp.up_proj\n",
      "Replaced layer: layers.2.mlp.down_proj\n",
      "Replaced layer: layers.3.self_attn.q_proj\n",
      "Replaced layer: layers.3.self_attn.k_proj\n",
      "Replaced layer: layers.3.self_attn.v_proj\n",
      "Replaced layer: layers.3.self_attn.o_proj\n",
      "Replaced layer: layers.3.mlp.gate_proj\n",
      "Replaced layer: layers.3.mlp.up_proj\n",
      "Replaced layer: layers.3.mlp.down_proj\n",
      "Replaced layer: layers.4.self_attn.q_proj\n",
      "Replaced layer: layers.4.self_attn.k_proj\n",
      "Replaced layer: layers.4.self_attn.v_proj\n",
      "Replaced layer: layers.4.self_attn.o_proj\n",
      "Replaced layer: layers.4.mlp.gate_proj\n",
      "Replaced layer: layers.4.mlp.up_proj\n",
      "Replaced layer: layers.4.mlp.down_proj\n",
      "Replaced layer: layers.5.self_attn.q_proj\n",
      "Replaced layer: layers.5.self_attn.k_proj\n",
      "Replaced layer: layers.5.self_attn.v_proj\n",
      "Replaced layer: layers.5.self_attn.o_proj\n",
      "Replaced layer: layers.5.mlp.gate_proj\n",
      "Replaced layer: layers.5.mlp.up_proj\n",
      "Replaced layer: layers.5.mlp.down_proj\n",
      "Replaced layer: layers.6.self_attn.q_proj\n",
      "Replaced layer: layers.6.self_attn.k_proj\n",
      "Replaced layer: layers.6.self_attn.v_proj\n",
      "Replaced layer: layers.6.self_attn.o_proj\n",
      "Replaced layer: layers.6.mlp.gate_proj\n",
      "Replaced layer: layers.6.mlp.up_proj\n",
      "Replaced layer: layers.6.mlp.down_proj\n",
      "Replaced layer: layers.7.self_attn.q_proj\n",
      "Replaced layer: layers.7.self_attn.k_proj\n",
      "Replaced layer: layers.7.self_attn.v_proj\n",
      "Replaced layer: layers.7.self_attn.o_proj\n",
      "Replaced layer: layers.7.mlp.gate_proj\n",
      "Replaced layer: layers.7.mlp.up_proj\n",
      "Replaced layer: layers.7.mlp.down_proj\n",
      "Replaced layer: layers.8.self_attn.q_proj\n",
      "Replaced layer: layers.8.self_attn.k_proj\n",
      "Replaced layer: layers.8.self_attn.v_proj\n",
      "Replaced layer: layers.8.self_attn.o_proj\n",
      "Replaced layer: layers.8.mlp.gate_proj\n",
      "Replaced layer: layers.8.mlp.up_proj\n",
      "Replaced layer: layers.8.mlp.down_proj\n",
      "Replaced layer: layers.9.self_attn.q_proj\n",
      "Replaced layer: layers.9.self_attn.k_proj\n",
      "Replaced layer: layers.9.self_attn.v_proj\n",
      "Replaced layer: layers.9.self_attn.o_proj\n",
      "Replaced layer: layers.9.mlp.gate_proj\n",
      "Replaced layer: layers.9.mlp.up_proj\n",
      "Replaced layer: layers.9.mlp.down_proj\n",
      "Replaced layer: layers.10.self_attn.q_proj\n",
      "Replaced layer: layers.10.self_attn.k_proj\n",
      "Replaced layer: layers.10.self_attn.v_proj\n",
      "Replaced layer: layers.10.self_attn.o_proj\n",
      "Replaced layer: layers.10.mlp.gate_proj\n",
      "Replaced layer: layers.10.mlp.up_proj\n",
      "Replaced layer: layers.10.mlp.down_proj\n",
      "Replaced layer: layers.11.self_attn.q_proj\n",
      "Replaced layer: layers.11.self_attn.k_proj\n",
      "Replaced layer: layers.11.self_attn.v_proj\n",
      "Replaced layer: layers.11.self_attn.o_proj\n",
      "Replaced layer: layers.11.mlp.gate_proj\n",
      "Replaced layer: layers.11.mlp.up_proj\n",
      "Replaced layer: layers.11.mlp.down_proj\n",
      "Replaced layer: layers.12.self_attn.q_proj\n",
      "Replaced layer: layers.12.self_attn.k_proj\n",
      "Replaced layer: layers.12.self_attn.v_proj\n",
      "Replaced layer: layers.12.self_attn.o_proj\n",
      "Replaced layer: layers.12.mlp.gate_proj\n",
      "Replaced layer: layers.12.mlp.up_proj\n",
      "Replaced layer: layers.12.mlp.down_proj\n",
      "Replaced layer: layers.13.self_attn.q_proj\n",
      "Replaced layer: layers.13.self_attn.k_proj\n",
      "Replaced layer: layers.13.self_attn.v_proj\n",
      "Replaced layer: layers.13.self_attn.o_proj\n",
      "Replaced layer: layers.13.mlp.gate_proj\n",
      "Replaced layer: layers.13.mlp.up_proj\n",
      "Replaced layer: layers.13.mlp.down_proj\n",
      "Replaced layer: layers.14.self_attn.q_proj\n",
      "Replaced layer: layers.14.self_attn.k_proj\n",
      "Replaced layer: layers.14.self_attn.v_proj\n",
      "Replaced layer: layers.14.self_attn.o_proj\n",
      "Replaced layer: layers.14.mlp.gate_proj\n",
      "Replaced layer: layers.14.mlp.up_proj\n",
      "Replaced layer: layers.14.mlp.down_proj\n",
      "Replaced layer: layers.15.self_attn.q_proj\n",
      "Replaced layer: layers.15.self_attn.k_proj\n",
      "Replaced layer: layers.15.self_attn.v_proj\n",
      "Replaced layer: layers.15.self_attn.o_proj\n",
      "Replaced layer: layers.15.mlp.gate_proj\n",
      "Replaced layer: layers.15.mlp.up_proj\n",
      "Replaced layer: layers.15.mlp.down_proj\n",
      "Original model size (parameters): 6607343616\n",
      "Compressed model size (parameters): 6397628416\n",
      "Parameter compression rate: 3.17%\n",
      "Original model file size: 25207.39 MB\n",
      "Compressed model file size: 24407.47 MB\n",
      "File size compression rate: 3.17%\n",
      "Compressed model saved to directory: compressed_model\n",
      "Original model cleared from memory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import functional as F\n",
    "import tempfile\n",
    "\n",
    "# Parameters for fine-tuning\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'  # Replace with your desired model name\n",
    "COMPRESSION_FRACTION = 0.5\n",
    "RANK_PERCENTAGE = 0.5  # Percentage of rank to keep in the low-rank decomposition\n",
    "SELECTION = 'first'\n",
    "\n",
    "# Define LowRankLayer class for low-rank decomposition\n",
    "class LowRankLayer(nn.Module):\n",
    "    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n",
    "    def __init__(self, full_rank_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        # Perform SVD on the full-rank layer's weight matrix\n",
    "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n",
    "        S_diag = torch.diag(S)\n",
    "\n",
    "        # Assign rank as a percentage of the shape of S\n",
    "        self.rank = max(1, int(S.shape[0] * RANK_PERCENTAGE))\n",
    "        \n",
    "        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n",
    "        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n",
    "        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n",
    "\n",
    "        # Handle the bias term if it exists\n",
    "        if full_rank_layer.bias is not None:\n",
    "            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
    "        output = F.linear(x, aprox_weight_matrix, self.bias)\n",
    "        \n",
    "        # Print S matrix and its shape\n",
    "        print(\"S matrix shape:\", self.S.shape)\n",
    "        print(\"S matrix values:\")\n",
    "        print(self.S.cpu().detach().numpy())\n",
    "        \n",
    "        return output\n",
    "\n",
    "def replace_with_low_rank_partial(model, fraction=0.2, selection='first'):\n",
    "    # Collect all linear layers\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append((name, module))\n",
    "    \n",
    "    num_layers_to_replace = int(len(linear_layers) * fraction)\n",
    "    \n",
    "    if selection == 'first':\n",
    "        layers_to_replace = linear_layers[:num_layers_to_replace]\n",
    "    elif selection == 'middle':\n",
    "        start = (len(linear_layers) - num_layers_to_replace) // 2\n",
    "        layers_to_replace = linear_layers[start:start + num_layers_to_replace]\n",
    "    elif selection == 'last':\n",
    "        layers_to_replace = linear_layers[-num_layers_to_replace:]\n",
    "    else:\n",
    "        raise ValueError(\"Selection must be 'first', 'middle', or 'last'\")\n",
    "    \n",
    "    # Replace selected layers with LowRankLayer\n",
    "    for name, module in layers_to_replace:\n",
    "        low_rank_layer = LowRankLayer(module)\n",
    "        parent_name, child_name = name.rsplit('.', 1)\n",
    "        parent_module = model.get_submodule(parent_name)\n",
    "        setattr(parent_module, child_name, low_rank_layer)\n",
    "        print(f\"Replaced layer: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size(model, tokenizer):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        model.save_pretrained(tmpdirname)\n",
    "        tokenizer.save_pretrained(tmpdirname)\n",
    "        size = sum(f.stat().st_size for f in os.scandir(tmpdirname) if f.is_file())\n",
    "    return size\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Get initial parameter count and file size\n",
    "original_param_count = count_parameters(model)\n",
    "original_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Replace 20% of the linear layers with low-rank approximations\n",
    "model = replace_with_low_rank_partial(model, fraction=COMPRESSION_FRACTION, selection=SELECTION)\n",
    "\n",
    "# Get compressed parameter count and file size\n",
    "compressed_param_count = count_parameters(model)\n",
    "compressed_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Save the compressed model to the directory\n",
    "compressed_model_dir = \"compressed_model\"\n",
    "tokenizer.save_pretrained(compressed_model_dir)\n",
    "model.save_pretrained(compressed_model_dir)\n",
    "\n",
    "# Print sizes and compression rates\n",
    "print(f\"Original model size (parameters): {original_param_count}\")\n",
    "print(f\"Compressed model size (parameters): {compressed_param_count}\")\n",
    "print(f\"Parameter compression rate: {(original_param_count - compressed_param_count) / original_param_count:.2%}\")\n",
    "\n",
    "print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n",
    "\n",
    "print(f\"Compressed model saved to directory: {compressed_model_dir}\")\n",
    "\n",
    "# Clear the original model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()  # If using GPU\n",
    "\n",
    "print(\"Original model cleared from memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced layer: lm_head\n",
      "Replaced layer: model.layers.0.mlp.gate_proj\n",
      "Replaced layer: model.layers.0.mlp.up_proj\n",
      "Replaced layer: model.layers.0.mlp.down_proj\n",
      "Replaced layer: model.layers.1.mlp.gate_proj\n",
      "Replaced layer: model.layers.1.mlp.up_proj\n",
      "Replaced layer: model.layers.1.mlp.down_proj\n",
      "Replaced layer: model.layers.2.mlp.gate_proj\n",
      "Replaced layer: model.layers.2.mlp.up_proj\n",
      "Replaced layer: model.layers.2.mlp.down_proj\n",
      "Replaced layer: model.layers.3.mlp.gate_proj\n",
      "Replaced layer: model.layers.3.mlp.up_proj\n",
      "Replaced layer: model.layers.3.mlp.down_proj\n",
      "Replaced layer: model.layers.4.mlp.gate_proj\n",
      "Replaced layer: model.layers.4.mlp.up_proj\n",
      "Replaced layer: model.layers.4.mlp.down_proj\n",
      "Replaced layer: model.layers.5.mlp.gate_proj\n",
      "Replaced layer: model.layers.5.mlp.up_proj\n",
      "Replaced layer: model.layers.5.mlp.down_proj\n",
      "Replaced layer: model.layers.6.mlp.gate_proj\n",
      "Replaced layer: model.layers.6.mlp.up_proj\n",
      "Replaced layer: model.layers.6.mlp.down_proj\n",
      "Replaced layer: model.layers.7.mlp.gate_proj\n",
      "Replaced layer: model.layers.7.mlp.up_proj\n",
      "Replaced layer: model.layers.7.mlp.down_proj\n",
      "Replaced layer: model.layers.8.mlp.gate_proj\n",
      "Replaced layer: model.layers.8.mlp.up_proj\n",
      "Replaced layer: model.layers.8.mlp.down_proj\n",
      "Replaced layer: model.layers.9.mlp.gate_proj\n",
      "Replaced layer: model.layers.9.mlp.up_proj\n",
      "Replaced layer: model.layers.9.mlp.down_proj\n",
      "Replaced layer: model.layers.10.mlp.gate_proj\n",
      "Replaced layer: model.layers.10.mlp.up_proj\n",
      "Replaced layer: model.layers.10.mlp.down_proj\n",
      "Replaced layer: model.layers.11.mlp.gate_proj\n",
      "Replaced layer: model.layers.11.mlp.up_proj\n",
      "Replaced layer: model.layers.11.mlp.down_proj\n",
      "Replaced layer: model.layers.12.mlp.gate_proj\n",
      "Replaced layer: model.layers.12.mlp.up_proj\n",
      "Replaced layer: model.layers.12.mlp.down_proj\n",
      "Replaced layer: model.layers.13.mlp.gate_proj\n",
      "Replaced layer: model.layers.13.mlp.up_proj\n",
      "Replaced layer: model.layers.13.mlp.down_proj\n",
      "Replaced layer: model.layers.14.mlp.gate_proj\n",
      "Replaced layer: model.layers.14.mlp.up_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 36718/36718 [00:06<00:00, 5513.38 examples/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 104\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m     90\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     91\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    102\u001b[0m )\n\u001b[0;32m--> 104\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_datasets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Get compressed parameter count and file size\u001b[39;00m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/trainer.py:499\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_quantized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/trainer.py:741\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 741\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/transformers/modeling_utils.py:1900\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1898\u001b[0m     )\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/pavan/train/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import tempfile\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Specify the model name, compression fraction, and rank fraction\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "COMPRESSION_FRACTION = 0.2\n",
    "RANK_FRACTION = 0.5  # Fraction of the rank to be used in the low-rank approximation\n",
    "\n",
    "# Define LowRankLayer class for low-rank decomposition\n",
    "class LowRankLayer(nn.Module):\n",
    "    def __init__(self, full_rank_layer, rank_fraction=0.5):\n",
    "        super().__init__()\n",
    "        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n",
    "        S_diag = torch.diag(S)\n",
    "        self.rank = max(1, int(S.shape[0] * rank_fraction))\n",
    "        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n",
    "        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n",
    "        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n",
    "        self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous()) if full_rank_layer.bias is not None else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        aprox_weight_matrix = self.U @ self.S @ self.Vh\n",
    "        output = F.linear(x, aprox_weight_matrix, self.bias)\n",
    "        return output\n",
    "\n",
    "def replace_with_low_rank_partial(model, fraction=0.2, rank_fraction=0.5):\n",
    "    linear_layers = [(name, module) for name, module in model.named_modules() if isinstance(module, nn.Linear)]\n",
    "    linear_layers.sort(key=lambda x: x[1].weight.numel(), reverse=True)\n",
    "    num_layers_to_replace = int(len(linear_layers) * fraction)\n",
    "    layers_to_replace = linear_layers[:num_layers_to_replace]\n",
    "    \n",
    "    for name, module in layers_to_replace:\n",
    "        low_rank_layer = LowRankLayer(module, rank_fraction)\n",
    "        if '.' in name:\n",
    "            parent_name, child_name = name.rsplit('.', 1)\n",
    "            parent_module = model.get_submodule(parent_name)\n",
    "            setattr(parent_module, child_name, low_rank_layer)\n",
    "        else:\n",
    "            setattr(model, name, low_rank_layer)\n",
    "        print(f\"Replaced layer: {name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate the total number of parameters in the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size(model, tokenizer):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        model.save_pretrained(tmpdirname)\n",
    "        tokenizer.save_pretrained(tmpdirname)\n",
    "        size = sum(f.stat().st_size for f in os.scandir(tmpdirname) if f.is_file())\n",
    "    return size\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set padding token if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get initial parameter count and file size\n",
    "original_param_count = count_parameters(model)\n",
    "original_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Replace a fraction of the linear layers with low-rank approximations\n",
    "model = replace_with_low_rank_partial(model, fraction=COMPRESSION_FRACTION, rank_fraction=RANK_FRACTION)\n",
    "\n",
    "# Load WikiText dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_datasets = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Define data collator\n",
    "data_collator = lambda data: {'input_ids': torch.tensor([f['input_ids'] for f in data]), 'labels': torch.tensor([f['input_ids'] for f in data])}\n",
    "\n",
    "# Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    max_steps=1000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Get compressed parameter count and file size\n",
    "compressed_param_count = count_parameters(model)\n",
    "compressed_file_size = get_model_size(model, tokenizer)\n",
    "\n",
    "# Save the compressed model to the directory\n",
    "compressed_model_dir = \"compressed_model\"\n",
    "tokenizer.save_pretrained(compressed_model_dir)\n",
    "model.save_pretrained(compressed_model_dir)\n",
    "\n",
    "# Print sizes and compression rates\n",
    "print(f\"Original model size (parameters): {original_param_count}\")\n",
    "print(f\"Compressed model size (parameters): {compressed_param_count}\")\n",
    "print(f\"Parameter compression rate: {(original_param_count - compressed_param_count) / original_param_count:.2%}\")\n",
    "\n",
    "print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n",
    "print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n",
    "\n",
    "print(f\"Compressed model saved to directory: {compressed_model_dir}\")\n",
    "\n",
    "# Test the fine-tuned model with a \"hello\" prompt\n",
    "model = AutoModelForCausalLM.from_pretrained(compressed_model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(compressed_model_dir)\n",
    "\n",
    "input_text = \"hello\"\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "outputs = model.generate(**inputs)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input: {input_text}\")\n",
    "print(f\"Generated Output: {generated_text}\")\n",
    "\n",
    "# Clear the original model from memory\n",
    "del model\n",
    "torch.cuda.empty_cache()  # If using GPU\n",
    "\n",
    "print(\"Original model cleared from memory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
