{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Compression happening\n","\n","## distilbert = 60%\n","\n","## T5 = 60%\n","\n","## T5 base 83.00%\n","\n","## funnel_transformer smallbase 68%\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T05:44:58.343954Z","iopub.status.busy":"2024-07-03T05:44:58.343595Z","iopub.status.idle":"2024-07-03T05:44:59.041834Z","shell.execute_reply":"2024-07-03T05:44:59.040773Z","shell.execute_reply.started":"2024-07-03T05:44:58.343927Z"},"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mRunning cells with '.venv (Python 3.8.10)' requires the ipykernel package.\n","\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n","\u001b[1;31mCommand: '/home/azureuser/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import GPT2Tokenizer, GPT2Model\n","from torch.nn import functional as F\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank])\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank])\n","        self.Vh = nn.Parameter(Vh[:self.rank, :])\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2Model.from_pretrained('gpt2')\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","rank = 32  # Adjust this for more or less aggressive compression\n","model = replace_with_low_rank(model, rank)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Test the compressed model with a random input\n","input_text = \"Hello, how are you?\"\n","inputs = tokenizer(input_text, return_tensors='pt')\n","output = model(**inputs)\n","\n","# Print the output shape and the actual output\n","print(\"Output shape:\", output.last_hidden_state.shape)\n","print(\"Output:\", output.last_hidden_state)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T05:44:59.043764Z","iopub.status.busy":"2024-07-03T05:44:59.043462Z","iopub.status.idle":"2024-07-03T05:45:01.848016Z","shell.execute_reply":"2024-07-03T05:45:01.844218Z","shell.execute_reply.started":"2024-07-03T05:44:59.043739Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model\u001b[39;00m\n\u001b[1;32m     53\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m DistilBertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDistilBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdistilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Get initial size\u001b[39;00m\n\u001b[1;32m     57\u001b[0m original_size \u001b[38;5;241m=\u001b[39m count_parameters(model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3158\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3157\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 3158\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3174\u001b[0m     \u001b[38;5;66;03m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[1;32m   3175\u001b[0m     \u001b[38;5;66;03m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3179\u001b[0m     \u001b[38;5;66;03m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m     \u001b[38;5;66;03m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m     config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:603\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 603\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    605\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    607\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1642\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:395\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n","File \u001b[0;32m/opt/conda/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from torch.nn import functional as F\n","from huggingface_hub import HfApi, login\n","\n","# Log in to Hugging Face\n","token = \"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\"  # Replace with your Hugging Face token\n","login(token)\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank])\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank])\n","        self.Vh = nn.Parameter(Vh[:self.rank, :])\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","rank = 32  # Adjust this for more or less aggressive compression\n","model = replace_with_low_rank(model, rank)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Test the compressed model with a random input\n","input_text = \"Hello, how are you?\"\n","inputs = tokenizer(input_text, return_tensors='pt')\n","output = model(**inputs)\n","print(\"Output shape:\", output.last_hidden_state.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.848937Z","iopub.status.idle":"2024-07-03T05:45:01.849287Z","shell.execute_reply":"2024-07-03T05:45:01.849141Z","shell.execute_reply.started":"2024-07-03T05:45:01.849127Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from torch.nn import functional as F\n","from huggingface_hub import HfApi, create_repo, upload_folder\n","from huggingface_hub import HfApi, login\n","\n","# Log in to Hugging Face\n","token = \"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\"  # Replace with your Hugging Face token\n","login(token)\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n","model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","rank = 32  # Adjust this for more or less aggressive compression\n","model = replace_with_low_rank(model, rank)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_distilbert\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","# Create a new repository on Hugging Face\n","repo_name = \"pavan01729/compressed_distilbert\"\n","create_repo(repo_name, exist_ok=True)\n","\n","# Upload the model directory to the repository\n","upload_folder(repo_id=repo_name, folder_path=model_dir)\n","\n","print(f\"Model pushed to Hugging Face Hub at: https://huggingface.co/{repo_name}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## distilbert_base 60%"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.851017Z","iopub.status.idle":"2024-07-03T05:45:01.851363Z","shell.execute_reply":"2024-07-03T05:45:01.851208Z","shell.execute_reply.started":"2024-07-03T05:45:01.851194Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f526a60698b4ba1b3d84c1c82e311dd","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"beef8fc564a14c468ca03bafb8ace6cb","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d4f79a204d94c5ebb75186e045c11ae","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f416db9276ae4149a2368cdcc0a9a519","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"312405068c9b4bfbb6e636a0762ce189","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: transformer.layer.0.attention.q_lin\n","Replaced layer: transformer.layer.0.attention.k_lin\n","Replaced layer: transformer.layer.0.attention.v_lin\n","Replaced layer: transformer.layer.0.attention.out_lin\n","Replaced layer: transformer.layer.0.ffn.lin1\n","Replaced layer: transformer.layer.0.ffn.lin2\n","Replaced layer: transformer.layer.1.attention.q_lin\n","Replaced layer: transformer.layer.1.attention.k_lin\n","Replaced layer: transformer.layer.1.attention.v_lin\n","Replaced layer: transformer.layer.1.attention.out_lin\n","Replaced layer: transformer.layer.1.ffn.lin1\n","Replaced layer: transformer.layer.1.ffn.lin2\n","Replaced layer: transformer.layer.2.attention.q_lin\n","Replaced layer: transformer.layer.2.attention.k_lin\n","Replaced layer: transformer.layer.2.attention.v_lin\n","Replaced layer: transformer.layer.2.attention.out_lin\n","Replaced layer: transformer.layer.2.ffn.lin1\n","Replaced layer: transformer.layer.2.ffn.lin2\n","Replaced layer: transformer.layer.3.attention.q_lin\n","Replaced layer: transformer.layer.3.attention.k_lin\n","Replaced layer: transformer.layer.3.attention.v_lin\n","Replaced layer: transformer.layer.3.attention.out_lin\n","Replaced layer: transformer.layer.3.ffn.lin1\n","Replaced layer: transformer.layer.3.ffn.lin2\n","Replaced layer: transformer.layer.4.attention.q_lin\n","Replaced layer: transformer.layer.4.attention.k_lin\n","Replaced layer: transformer.layer.4.attention.v_lin\n","Replaced layer: transformer.layer.4.attention.out_lin\n","Replaced layer: transformer.layer.4.ffn.lin1\n","Replaced layer: transformer.layer.4.ffn.lin2\n","Replaced layer: transformer.layer.5.attention.q_lin\n","Replaced layer: transformer.layer.5.attention.k_lin\n","Replaced layer: transformer.layer.5.attention.v_lin\n","Replaced layer: transformer.layer.5.attention.out_lin\n","Replaced layer: transformer.layer.5.ffn.lin1\n","Replaced layer: transformer.layer.5.ffn.lin2\n","Original model size (parameters): 66362880\n","Compressed model size (parameters): 26586624\n","Compression rate: 59.94%\n","Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'distilbert-base-uncased'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Bert base uncased 70%"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.853176Z","iopub.status.idle":"2024-07-03T05:45:01.853666Z","shell.execute_reply":"2024-07-03T05:45:01.853428Z","shell.execute_reply.started":"2024-07-03T05:45:01.853408Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'bert-base-uncased'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## facebook bart 66%"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.855014Z","iopub.status.idle":"2024-07-03T05:45:01.855549Z","shell.execute_reply":"2024-07-03T05:45:01.855243Z","shell.execute_reply.started":"2024-07-03T05:45:01.855225Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad96c1a99bcc4353af74557ff472586c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"735d66572bdc44fc92f23c72662f8bb8","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aeed3e6eb4a646c0bcbaac9180cfaec4","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"381ec4e5e3484e81b8005c5241fc0af7","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26b7df5d54024f3dbbce131a3b62bb2b","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: encoder.layers.0.self_attn.k_proj\n","Replaced layer: encoder.layers.0.self_attn.v_proj\n","Replaced layer: encoder.layers.0.self_attn.q_proj\n","Replaced layer: encoder.layers.0.self_attn.out_proj\n","Replaced layer: encoder.layers.0.fc1\n","Replaced layer: encoder.layers.0.fc2\n","Replaced layer: encoder.layers.1.self_attn.k_proj\n","Replaced layer: encoder.layers.1.self_attn.v_proj\n","Replaced layer: encoder.layers.1.self_attn.q_proj\n","Replaced layer: encoder.layers.1.self_attn.out_proj\n","Replaced layer: encoder.layers.1.fc1\n","Replaced layer: encoder.layers.1.fc2\n","Replaced layer: encoder.layers.2.self_attn.k_proj\n","Replaced layer: encoder.layers.2.self_attn.v_proj\n","Replaced layer: encoder.layers.2.self_attn.q_proj\n","Replaced layer: encoder.layers.2.self_attn.out_proj\n","Replaced layer: encoder.layers.2.fc1\n","Replaced layer: encoder.layers.2.fc2\n","Replaced layer: encoder.layers.3.self_attn.k_proj\n","Replaced layer: encoder.layers.3.self_attn.v_proj\n","Replaced layer: encoder.layers.3.self_attn.q_proj\n","Replaced layer: encoder.layers.3.self_attn.out_proj\n","Replaced layer: encoder.layers.3.fc1\n","Replaced layer: encoder.layers.3.fc2\n","Replaced layer: encoder.layers.4.self_attn.k_proj\n","Replaced layer: encoder.layers.4.self_attn.v_proj\n","Replaced layer: encoder.layers.4.self_attn.q_proj\n","Replaced layer: encoder.layers.4.self_attn.out_proj\n","Replaced layer: encoder.layers.4.fc1\n","Replaced layer: encoder.layers.4.fc2\n","Replaced layer: encoder.layers.5.self_attn.k_proj\n","Replaced layer: encoder.layers.5.self_attn.v_proj\n","Replaced layer: encoder.layers.5.self_attn.q_proj\n","Replaced layer: encoder.layers.5.self_attn.out_proj\n","Replaced layer: encoder.layers.5.fc1\n","Replaced layer: encoder.layers.5.fc2\n","Replaced layer: decoder.layers.0.self_attn.k_proj\n","Replaced layer: decoder.layers.0.self_attn.v_proj\n","Replaced layer: decoder.layers.0.self_attn.q_proj\n","Replaced layer: decoder.layers.0.self_attn.out_proj\n","Replaced layer: decoder.layers.0.encoder_attn.k_proj\n","Replaced layer: decoder.layers.0.encoder_attn.v_proj\n","Replaced layer: decoder.layers.0.encoder_attn.q_proj\n","Replaced layer: decoder.layers.0.encoder_attn.out_proj\n","Replaced layer: decoder.layers.0.fc1\n","Replaced layer: decoder.layers.0.fc2\n","Replaced layer: decoder.layers.1.self_attn.k_proj\n","Replaced layer: decoder.layers.1.self_attn.v_proj\n","Replaced layer: decoder.layers.1.self_attn.q_proj\n","Replaced layer: decoder.layers.1.self_attn.out_proj\n","Replaced layer: decoder.layers.1.encoder_attn.k_proj\n","Replaced layer: decoder.layers.1.encoder_attn.v_proj\n","Replaced layer: decoder.layers.1.encoder_attn.q_proj\n","Replaced layer: decoder.layers.1.encoder_attn.out_proj\n","Replaced layer: decoder.layers.1.fc1\n","Replaced layer: decoder.layers.1.fc2\n","Replaced layer: decoder.layers.2.self_attn.k_proj\n","Replaced layer: decoder.layers.2.self_attn.v_proj\n","Replaced layer: decoder.layers.2.self_attn.q_proj\n","Replaced layer: decoder.layers.2.self_attn.out_proj\n","Replaced layer: decoder.layers.2.encoder_attn.k_proj\n","Replaced layer: decoder.layers.2.encoder_attn.v_proj\n","Replaced layer: decoder.layers.2.encoder_attn.q_proj\n","Replaced layer: decoder.layers.2.encoder_attn.out_proj\n","Replaced layer: decoder.layers.2.fc1\n","Replaced layer: decoder.layers.2.fc2\n","Replaced layer: decoder.layers.3.self_attn.k_proj\n","Replaced layer: decoder.layers.3.self_attn.v_proj\n","Replaced layer: decoder.layers.3.self_attn.q_proj\n","Replaced layer: decoder.layers.3.self_attn.out_proj\n","Replaced layer: decoder.layers.3.encoder_attn.k_proj\n","Replaced layer: decoder.layers.3.encoder_attn.v_proj\n","Replaced layer: decoder.layers.3.encoder_attn.q_proj\n","Replaced layer: decoder.layers.3.encoder_attn.out_proj\n","Replaced layer: decoder.layers.3.fc1\n","Replaced layer: decoder.layers.3.fc2\n","Replaced layer: decoder.layers.4.self_attn.k_proj\n","Replaced layer: decoder.layers.4.self_attn.v_proj\n","Replaced layer: decoder.layers.4.self_attn.q_proj\n","Replaced layer: decoder.layers.4.self_attn.out_proj\n","Replaced layer: decoder.layers.4.encoder_attn.k_proj\n","Replaced layer: decoder.layers.4.encoder_attn.v_proj\n","Replaced layer: decoder.layers.4.encoder_attn.q_proj\n","Replaced layer: decoder.layers.4.encoder_attn.out_proj\n","Replaced layer: decoder.layers.4.fc1\n","Replaced layer: decoder.layers.4.fc2\n","Replaced layer: decoder.layers.5.self_attn.k_proj\n","Replaced layer: decoder.layers.5.self_attn.v_proj\n","Replaced layer: decoder.layers.5.self_attn.q_proj\n","Replaced layer: decoder.layers.5.self_attn.out_proj\n","Replaced layer: decoder.layers.5.encoder_attn.k_proj\n","Replaced layer: decoder.layers.5.encoder_attn.v_proj\n","Replaced layer: decoder.layers.5.encoder_attn.q_proj\n","Replaced layer: decoder.layers.5.encoder_attn.out_proj\n","Replaced layer: decoder.layers.5.fc1\n","Replaced layer: decoder.layers.5.fc2\n","Original model size (parameters): 139420416\n","Compressed model size (parameters): 46916352\n","Compression rate: 66.35%\n"]},{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"]},{"name":"stdout","output_type":"stream","text":["Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'facebook/bart-base'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## T5 small 60% compress"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.857206Z","iopub.status.idle":"2024-07-03T05:45:01.857663Z","shell.execute_reply":"2024-07-03T05:45:01.857453Z","shell.execute_reply.started":"2024-07-03T05:45:01.857418Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01e217aafc1b4faea3759e22813c807b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0925878a5b8645658af14ba31e9e5805","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31db17787d47486eb51427f626256117","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6356c2db53b947f19c1efacc49c19914","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35ebedd124da4848974aa354ef22a6de","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: encoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wo\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wo\n","Original model size (parameters): 60506624\n","Compressed model size (parameters): 20890112\n","Compression rate: 65.47%\n","Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google-t5/t5-small'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## T5 base 83.00%"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T09:20:08.772652Z","iopub.status.busy":"2024-07-03T09:20:08.771957Z","iopub.status.idle":"2024-07-03T09:21:17.953614Z","shell.execute_reply":"2024-07-03T09:21:17.952531Z","shell.execute_reply.started":"2024-07-03T09:20:08.772605Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c1e4787d81749e083de4e229e8996b8","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6033b3e424941f99dce415aec10239d","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f53d3da127c454d90c3135b38f72b55","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfe318250d0e4d0ea01c534b11ed0c32","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: encoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.6.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.6.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.7.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.7.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.8.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.8.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.9.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.9.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.10.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.10.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.11.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.11.layer.1.DenseReluDense.wo\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.6.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.6.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.7.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.7.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.8.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.8.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.9.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.9.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.10.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.10.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.11.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.11.layer.2.DenseReluDense.wo\n","Original model size (parameters): 222903552\n","Compressed model size (parameters): 37895424\n","Compression rate: 83.00%\n","Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google-t5/t5-base'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## T5 3b 95%"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.861003Z","iopub.status.idle":"2024-07-03T05:45:01.861330Z","shell.execute_reply":"2024-07-03T05:45:01.861176Z","shell.execute_reply.started":"2024-07-03T05:45:01.861162Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google-t5/t5-3b'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## T5 large 90%"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T10:51:32.831739Z","iopub.status.busy":"2024-07-03T10:51:32.831081Z","iopub.status.idle":"2024-07-03T10:55:23.777629Z","shell.execute_reply":"2024-07-03T10:55:23.776704Z","shell.execute_reply.started":"2024-07-03T10:51:32.831707Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48238c087e624f6eab5cbcd1c0b12c46","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7e7c9fd183c420f933fa300798c6e06","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72937fc98a054c4b84c1913ed2ccecd8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"890c48864d2d46a0962c4f5f86044c2a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: encoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.6.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.6.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.7.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.7.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.8.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.8.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.9.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.9.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.10.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.10.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.11.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.11.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.12.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.12.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.13.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.13.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.14.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.14.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.15.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.15.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.16.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.16.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.17.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.17.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.18.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.18.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.19.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.19.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.20.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.20.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.21.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.21.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.22.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.22.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.23.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.23.layer.1.DenseReluDense.wo\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.6.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.6.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.7.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.7.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.8.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.8.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.9.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.9.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.10.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.10.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.11.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.11.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.12.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.12.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.13.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.13.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.14.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.14.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.15.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.15.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.16.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.16.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.17.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.17.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.18.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.18.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.19.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.19.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.20.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.20.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.21.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.21.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.22.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.22.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.23.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.23.layer.2.DenseReluDense.wo\n","Original model size (parameters): 737668096\n","Compressed model size (parameters): 68021248\n","Compression rate: 90.78%\n","Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google-t5/t5-large'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## funnel_transformer smallbase 68%"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.865736Z","iopub.status.idle":"2024-07-03T05:45:01.866171Z","shell.execute_reply":"2024-07-03T05:45:01.865961Z","shell.execute_reply.started":"2024-07-03T05:45:01.865942Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'funnel-transformer/small-base'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.867480Z","iopub.status.idle":"2024-07-03T05:45:01.867912Z","shell.execute_reply":"2024-07-03T05:45:01.867709Z","shell.execute_reply.started":"2024-07-03T05:45:01.867691Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["model_names = ['funnel-transformer/small-base', 'bert-base-uncased', 'distilbert-base-uncased']\n","\n","for model_name in model_names:\n","    print(f\"Testing compression on model: {model_name}\")\n","    # Adjust MODEL_NAME to the current model\n","    MODEL_NAME = model_name\n","    # Repeat the entire process with the current MODEL_NAME\n","    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","    model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","    original_size = count_parameters(model)\n","    model = replace_with_low_rank(model, COMPRESSION_RANK)\n","    compressed_size = count_parameters(model)\n","\n","    print(f\"Original model size (parameters): {original_size}\")\n","    print(f\"Compressed model size (parameters): {compressed_size}\")\n","    print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","    model_dir = f\"compressed_model_{model_name.replace('/', '_')}\"\n","    tokenizer.save_pretrained(model_dir)\n","    model.save_pretrained(model_dir)\n","\n","    print(f\"Model saved to directory: {model_dir}\\n\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## funnel transformer chat"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T05:52:18.861674Z","iopub.status.busy":"2024-07-03T05:52:18.861172Z","iopub.status.idle":"2024-07-03T05:52:30.426257Z","shell.execute_reply":"2024-07-03T05:52:30.424924Z","shell.execute_reply.started":"2024-07-03T05:52:18.861641Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of FunnelForQuestionAnswering were not initialized from the model checkpoint at funnel-transformer/small-base and are newly initialized: ['decoder.layers.0.attention.k_head.bias', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.1.attention.layer_norm.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.1.attention.post_proj.bias', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.1.ffn.linear_2.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: Hi! I am a Q&A bot. Please provide some context to get started.\n"]},{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Start the chat\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[23], line 13\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m():\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: Hi! I am a Q&A bot. Please provide some context to get started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou (provide context): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: Context received. Now you can ask questions based on this context.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n","\n","# Load the tokenizer and model\n","MODEL_NAME = 'funnel-transformer/small-base'  # Replace with your desired model name\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n","\n","# Set up the question-answering pipeline\n","qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n","\n","def chat():\n","    print(\"Chatbot: Hi! I am a Q&A bot. Please provide some context to get started.\")\n","    context = input(\"You (provide context): \").strip()\n","    \n","    print(\"Chatbot: Context received. Now you can ask questions based on this context.\")\n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() in ['exit', 'quit', 'bye']:\n","            print(\"Chatbot: Goodbye!\")\n","            break\n","        \n","        qa_input = {\n","            'question': user_input,\n","            'context': context\n","        }\n","        response = qa_pipeline(qa_input)\n","        answer = response['answer']\n","        print(f\"Chatbot: {answer}\")\n","\n","# Start the chat\n","chat()\n"]},{"cell_type":"markdown","metadata":{},"source":["## GPT2 chat"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T05:53:44.807517Z","iopub.status.busy":"2024-07-03T05:53:44.807131Z","iopub.status.idle":"2024-07-03T05:56:31.365098Z","shell.execute_reply":"2024-07-03T05:56:31.363743Z","shell.execute_reply.started":"2024-07-03T05:53:44.807487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Chatbot: Hi! I am a GPT-2 based chat bot. Let's chat!\n"]},{"name":"stdout","output_type":"stream","text":["You:  hi\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: !!\n"]},{"name":"stdout","output_type":"stream","text":["You:  code something\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: !\n"]},{"name":"stdout","output_type":"stream","text":["You:  chat \n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: \n"]},{"name":"stdout","output_type":"stream","text":["You:  do somethign\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: !\n"]},{"name":"stdout","output_type":"stream","text":["You:  let it go\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: Your: you're not going to be able to do anything\n"]},{"name":"stdout","output_type":"stream","text":["You:  okay\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: Your: I'm not sure what you mean\n"]},{"name":"stdout","output_type":"stream","text":["You:  hi\n"]},{"name":"stdout","output_type":"stream","text":["Chatbot: Your: i'm sorry\n"]},{"name":"stdout","output_type":"stream","text":["You:  for what\n"]},{"ename":"ValueError","evalue":"Input length of input_ids is 102, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Start the chat\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[24], line 35\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m conversation_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChatbot: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Extract the bot's response from the generated text\u001b[39;00m\n\u001b[1;32m     38\u001b[0m response_text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;28mlen\u001b[39m(conversation_history):]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n","Cell \u001b[0;32mIn[24], line 11\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, model, tokenizer, max_length, num_return_sequences)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(prompt, model, tokenizer, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1643\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model does not support `cache_implementation=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`. Please check the following \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1644\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue: https://github.com/huggingface/transformers/issues/28981\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1645\u001b[0m             )\n\u001b[1;32m   1646\u001b[0m         model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_static_cache(batch_size, generation_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1648\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1176\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1175\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1177\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1180\u001b[0m     )\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1186\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 102, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load the tokenizer and model\n","MODEL_NAME = 'gpt2'  # Replace with your desired model name\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","\n","# Function to generate a response\n","def generate_response(prompt, model, tokenizer, max_length=100, num_return_sequences=1):\n","    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n","    outputs = model.generate(\n","        inputs, \n","        max_length=max_length, \n","        num_return_sequences=num_return_sequences, \n","        pad_token_id=tokenizer.eos_token_id,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","def chat():\n","    print(\"Chatbot: Hi! I am a GPT-2 based chat bot. Let's chat!\")\n","    conversation_history = \"\"\n","    \n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() in ['exit', 'quit', 'bye']:\n","            print(\"Chatbot: Goodbye!\")\n","            break\n","        \n","        # Append user input to the conversation history\n","        conversation_history += f\"You: {user_input}\\nChatbot: \"\n","        \n","        # Generate response\n","        response = generate_response(conversation_history, model, tokenizer)\n","        \n","        # Extract the bot's response from the generated text\n","        response_text = response[len(conversation_history):].split(\"\\n\")[0]\n","        \n","        # Append bot response to conversation history\n","        conversation_history += f\"{response_text}\\n\"\n","        \n","        print(f\"Chatbot: {response_text}\")\n","\n","# Start the chat\n","chat()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.870912Z","iopub.status.idle":"2024-07-03T05:45:01.871340Z","shell.execute_reply":"2024-07-03T05:45:01.871135Z","shell.execute_reply.started":"2024-07-03T05:45:01.871117Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'cerebras/Cerebras-GPT-111M'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## GPT2 trial"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:02:00.903801Z","iopub.status.busy":"2024-07-03T06:02:00.903414Z","iopub.status.idle":"2024-07-03T06:02:01.307269Z","shell.execute_reply":"2024-07-03T06:02:01.306367Z","shell.execute_reply.started":"2024-07-03T06:02:00.903776Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GPT2Model(\n","  (wte): Embedding(50257, 768)\n","  (wpe): Embedding(1024, 768)\n","  (drop): Dropout(p=0.1, inplace=False)\n","  (h): ModuleList(\n","    (0-11): 12 x GPT2Block(\n","      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (attn): GPT2Attention(\n","        (c_attn): Conv1D()\n","        (c_proj): Conv1D()\n","        (attn_dropout): Dropout(p=0.1, inplace=False)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (mlp): GPT2MLP(\n","        (c_fc): Conv1D()\n","        (c_proj): Conv1D()\n","        (act): NewGELUActivation()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",")\n"]}],"source":["from transformers import GPT2Model\n","\n","model = GPT2Model.from_pretrained('gpt2')\n","print(model)\n"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:04:28.071552Z","iopub.status.busy":"2024-07-03T06:04:28.070890Z","iopub.status.idle":"2024-07-03T06:04:30.038906Z","shell.execute_reply":"2024-07-03T06:04:30.037931Z","shell.execute_reply.started":"2024-07-03T06:04:28.071518Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Original model size (parameters): 124439808\n","Compressed model size (parameters): 124439808\n","Compression rate: 0.00%\n","Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'gpt2'  # Replace with your desired model name\n","COMPRESSION_RANK = 32  # Adjust this for more or less aggressive compression\n","\n","# Define LowRankConv1DLayer class for low-rank decomposition\n","class LowRankConv1DLayer(nn.Module):\n","    \"\"\"Given a Conv1D layer, find low-rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Extract weight and bias\n","        weight = full_rank_layer.weight\n","        bias = full_rank_layer.bias\n","\n","        # Perform SVD on the weight matrix\n","        weight_reshaped = weight.view(weight.size(0), -1)\n","        U, S, Vh = torch.linalg.svd(weight_reshaped.float(), full_matrices=False)\n","        S_diag = torch.diag(S[:rank])\n","        self.U = nn.Parameter(U[:, :rank].contiguous())\n","        self.S = nn.Parameter(S_diag.contiguous())\n","        self.Vh = nn.Parameter(Vh[:rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if bias is not None:\n","            self.bias = nn.Parameter(bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","        # Ensure the low-rank layer has fewer parameters than the original layer\n","        original_params = weight.numel() + (bias.numel() if bias is not None else 0)\n","        approx_params = self.U.numel() + self.S.numel() + self.Vh.numel() + (self.bias.numel() if self.bias is not None else 0)\n","        assert approx_params < original_params, \"Low-rank approximation does not reduce parameters\"\n","\n","    def forward(self, x):\n","        weight_low_rank = (self.U @ self.S @ self.Vh).view(self.U.size(0), -1, 1)\n","        output = F.conv1d(x, weight_low_rank, self.bias)\n","        return output\n","\n","# Function to replace Conv1D layers with LowRankConv1DLayer\n","def replace_conv1d_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Conv1d):\n","            try:\n","                # Create a LowRankConv1DLayer to replace the full-rank Conv1D layer\n","                low_rank_layer = LowRankConv1DLayer(rank, module)\n","                if '.' in name:\n","                    parent_name, child_name = name.rsplit('.', 1)\n","                    parent_module = model.get_submodule(parent_name)\n","                    setattr(parent_module, child_name, low_rank_layer)\n","                else:\n","                    setattr(model, name, low_rank_layer)\n","                print(f\"Replaced layer: {name}\")\n","            except AssertionError as e:\n","                print(f\"Skipping replacement for layer {name}: {e}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace Conv1D layers with low-rank approximations\n","model = replace_conv1d_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Mistral 7b broken"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:07:06.714577Z","iopub.status.busy":"2024-07-03T06:07:06.713872Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b3b2ab0f5aa4c8bb1799570773d5163","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a7a2df985aa4797b46c5567ecce2216","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afe6545187864c9eb67734b4c067f179","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"045e536983b6471c8cdb692018208392","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aaec15826dbd4982b1cdf08cfb879d35","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0f8a192bf6e946a5bc4aed6bf5ff4a77","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d74517a019e40b98342f756fff4e908","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e20445ae112c4ed1ada3f09631e13ed3","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6120d1106b0c4fe1b847f8f035eea8f2","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5430d030a16435d8cbd6390aa91114e","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b718378c1c034802ac53bc8811d4e8bf","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.3'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.875275Z","iopub.status.idle":"2024-07-03T05:45:01.875723Z","shell.execute_reply":"2024-07-03T05:45:01.875521Z","shell.execute_reply.started":"2024-07-03T05:45:01.875502Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google-t5/t5-small'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.876894Z","iopub.status.idle":"2024-07-03T05:45:01.877266Z","shell.execute_reply":"2024-07-03T05:45:01.877091Z","shell.execute_reply.started":"2024-07-03T05:45:01.877073Z"},"trusted":true},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n","from datasets import load_dataset, load_metric\n","\n","# Load the dataset\n","dataset = load_dataset(\"glue\", \"mrpc\")\n","metric = load_metric(\"glue\", \"mrpc\")\n","\n","# Preprocess the dataset\n","tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","\n","def preprocess_function(examples):\n","    inputs = [\"mrpc sentence1: \" + ex for ex in examples[\"sentence1\"]]\n","    targets = [ex for ex in examples[\"sentence2\"]]\n","    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n","    model_inputs[\"labels\"] = labels\n","    return model_inputs\n","\n","tokenized_datasets = dataset.map(preprocess_function, batched=True)\n","\n","# Load the original model for comparison\n","original_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.878792Z","iopub.status.idle":"2024-07-03T05:45:01.879099Z","shell.execute_reply":"2024-07-03T05:45:01.878957Z","shell.execute_reply.started":"2024-07-03T05:45:01.878944Z"},"trusted":true},"outputs":[],"source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    return metric.compute(predictions=preds, references=labels)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.880497Z","iopub.status.idle":"2024-07-03T05:45:01.880930Z","shell.execute_reply":"2024-07-03T05:45:01.880745Z","shell.execute_reply.started":"2024-07-03T05:45:01.880726Z"},"trusted":true},"outputs":[],"source":["# Trainer for the original model\n","original_trainer = Trainer(\n","    model=original_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the original model\n","original_trainer.train()\n","\n","# Evaluate the original model\n","original_eval_results = original_trainer.evaluate()\n","\n","# Trainer for the compressed model\n","compressed_model = T5ForConditionalGeneration.from_pretrained(\"compressed_model\")\n","\n","compressed_trainer = Trainer(\n","    model=compressed_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the compressed model\n","compressed_trainer.train()\n","\n","# Evaluate the compressed model\n","compressed_eval_results = compressed_trainer.evaluate()\n","\n","# Print the evaluation results\n","print(\"Original Model Evaluation Results:\", original_eval_results)\n","print(\"Compressed Model Evaluation Results:\", compressed_eval_results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.884737Z","iopub.status.idle":"2024-07-03T05:45:01.885186Z","shell.execute_reply":"2024-07-03T05:45:01.884976Z","shell.execute_reply.started":"2024-07-03T05:45:01.884958Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, T5ForConditionalGeneration\n","from torch.nn import functional as F\n","from datasets import load_dataset, load_metric\n","import numpy as np\n","import gc\n","\n","# Enable CUDA launch blocking for detailed error messages\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 't5-small'  # Replace with your desired model name\n","\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","HUGGINGFACE_TOKEN = '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'  # Replace with your Hugging Face token\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model with the Hugging Face token\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n","model = AutoModel.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n","\n","# Evaluation\n","dataset = load_dataset(\"glue\", \"mrpc\")\n","metric = load_metric(\"glue\", \"mrpc\")\n","\n","def preprocess_function(examples):\n","    inputs = [\"mrpc sentence1: \" + ex for ex in examples[\"sentence1\"]]\n","    targets = [ex for ex in examples[\"sentence2\"]]\n","    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n","    model_inputs[\"labels\"] = labels\n","    return model_inputs\n","\n","tokenized_datasets = dataset.map(preprocess_function, batched=True)\n","\n","def compute_metrics(p):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    return metric.compute(predictions=preds, references=p.label_ids)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=4,  # Further reduced batch size\n","    per_device_eval_batch_size=4,   # Further reduced batch size\n","    num_train_epochs=1,             # Only train for 1 epoch for quick evaluation\n","    weight_decay=0.01,\n","    report_to=[]  # Disable WandB logging\n",")\n","\n","# Trainer for the original model\n","original_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n","original_trainer = Trainer(\n","    model=original_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"].select(range(100)),  # Limit training dataset size\n","    eval_dataset=tokenized_datasets[\"validation\"].select(range(50)),  # Limit evaluation dataset size\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the original model\n","original_trainer.train()\n","\n","# Evaluate the original model\n","original_eval_results = original_trainer.evaluate()\n","\n","# Trainer for the compressed model\n","compressed_model = T5ForConditionalGeneration.from_pretrained(\"compressed_model\", use_auth_token=HUGGINGFACE_TOKEN)\n","\n","compressed_trainer = Trainer(\n","    model=compressed_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"].select(range(100)),  # Limit training dataset size\n","    eval_dataset=tokenized_datasets[\"validation\"].select(range(50)),  # Limit evaluation dataset size\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the compressed model\n","compressed_trainer.train()\n","\n","# Evaluate the compressed model\n","compressed_eval_results = compressed_trainer.evaluate()\n","\n","# Print the evaluation results\n","print(\"Original Model Evaluation Results:\", original_eval_results)\n","print(\"Compressed Model Evaluation Results:\", compressed_eval_results)\n","\n","# Clear CUDA cache and garbage collection\n","torch.cuda.empty_cache()\n","gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-07-03T05:45:01.886880Z","iopub.status.idle":"2024-07-03T05:45:01.887287Z","shell.execute_reply":"2024-07-03T05:45:01.887096Z","shell.execute_reply.started":"2024-07-03T05:45:01.887080Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, T5ForConditionalGeneration\n","from torch.nn import functional as F\n","from datasets import load_dataset, load_metric\n","import numpy as np\n","import gc\n","\n","# Enable CUDA launch blocking for detailed error messages\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 't5-small'  # Replace with your desired model name\n","\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","HUGGINGFACE_TOKEN = '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'  # Replace with your Hugging Face token\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model with the Hugging Face token\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n","model = AutoModel.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n","\n","# Evaluation\n","dataset = load_dataset(\"glue\", \"mrpc\")\n","metric = load_metric(\"glue\", \"mrpc\")\n","\n","def preprocess_function(examples):\n","    inputs = [\"mrpc sentence1: \" + ex for ex in examples[\"sentence1\"]]\n","    targets = [ex for ex in examples[\"sentence2\"]]\n","    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n","    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n","    model_inputs[\"labels\"] = labels\n","    return model_inputs\n","\n","tokenized_datasets = dataset.map(preprocess_function, batched=True)\n","\n","def compute_metrics(p):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.argmax(preds, axis=1)\n","    return metric.compute(predictions=preds, references=p.label_ids)\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=4,  # Further reduced batch size\n","    per_device_eval_batch_size=4,   # Further reduced batch size\n","    num_train_epochs=1,             # Only train for 1 epoch for quick evaluation\n","    weight_decay=0.01,\n","    report_to=[]  # Disable WandB logging\n",")\n","\n","# Trainer for the original model\n","original_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n","original_trainer = Trainer(\n","    model=original_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"].select(range(100)),  # Limit training dataset size\n","    eval_dataset=tokenized_datasets[\"validation\"].select(range(50)),  # Limit evaluation dataset size\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the original model\n","original_trainer.train()\n","\n","# Evaluate the original model\n","original_eval_results = original_trainer.evaluate()\n","\n","# Trainer for the compressed model\n","compressed_model = T5ForConditionalGeneration.from_pretrained(\"compressed_model\", use_auth_token=HUGGINGFACE_TOKEN)\n","\n","compressed_trainer = Trainer(\n","    model=compressed_model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"].select(range(100)),  # Limit training dataset size\n","    eval_dataset=tokenized_datasets[\"validation\"].select(range(50)),  # Limit evaluation dataset size\n","    compute_metrics=compute_metrics,\n",")\n","\n","# Fine-tune the compressed model\n","compressed_trainer.train()\n","\n","# Evaluate the compressed model\n","compressed_eval_results = compressed_trainer.evaluate()\n","\n","# Print the evaluation results\n","print(\"Original Model Evaluation Results:\", original_eval_results)\n","print(\"Compressed Model Evaluation Results:\", compressed_eval_results)\n","\n","# Clear CUDA cache and garbage collection\n","torch.cuda.empty_cache()\n","gc.collect()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:46:49.845464Z","iopub.status.busy":"2024-07-03T06:46:49.845065Z","iopub.status.idle":"2024-07-03T06:50:43.861858Z","shell.execute_reply":"2024-07-03T06:50:43.860694Z","shell.execute_reply.started":"2024-07-03T06:46:49.845433Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2de183ceb994bebb6a81440a2b3428f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e1ee76ad64c54af8b23741661dffe216","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bf76df5e6184c16882de30655382d7f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5781806416d1432d80267d1e73b5aec3","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: encoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.0.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.1.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.2.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.3.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.4.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.5.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.6.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.6.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.6.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.7.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.7.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.7.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.8.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.8.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.8.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.9.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.9.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.9.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.10.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.10.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.10.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.11.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.11.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.11.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.12.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.12.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.12.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.13.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.13.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.13.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.14.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.14.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.14.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.15.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.15.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.15.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.16.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.16.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.16.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.17.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.17.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.17.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.18.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.18.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.18.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.19.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.19.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.19.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.20.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.20.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.20.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.21.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.21.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.21.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.22.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.22.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.22.layer.1.DenseReluDense.wo\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.q\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.k\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.v\n","Replaced layer: encoder.block.23.layer.0.SelfAttention.o\n","Replaced layer: encoder.block.23.layer.1.DenseReluDense.wi\n","Replaced layer: encoder.block.23.layer.1.DenseReluDense.wo\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.0.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.0.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.0.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.1.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.1.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.1.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.2.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.2.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.2.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.3.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.3.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.3.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.4.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.4.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.4.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.5.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.5.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.5.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.6.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.6.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.6.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.6.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.7.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.7.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.7.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.7.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.8.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.8.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.8.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.8.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.9.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.9.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.9.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.9.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.10.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.10.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.10.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.10.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.11.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.11.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.11.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.11.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.12.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.12.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.12.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.12.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.13.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.13.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.13.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.13.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.14.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.14.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.14.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.14.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.15.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.15.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.15.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.15.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.16.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.16.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.16.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.16.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.17.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.17.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.17.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.17.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.18.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.18.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.18.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.18.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.19.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.19.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.19.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.19.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.20.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.20.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.20.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.20.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.21.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.21.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.21.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.21.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.22.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.22.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.22.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.22.layer.2.DenseReluDense.wo\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.q\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.k\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.v\n","Replaced layer: decoder.block.23.layer.0.SelfAttention.o\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.q\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.k\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.v\n","Replaced layer: decoder.block.23.layer.1.EncDecAttention.o\n","Replaced layer: decoder.block.23.layer.2.DenseReluDense.wi\n","Replaced layer: decoder.block.23.layer.2.DenseReluDense.wo\n","Original model size (parameters): 737668096\n","Compressed model size (parameters): 68021248\n","Compression rate: 90.78%\n","Model saved to directory: compressed_model\n"]}],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google-t5/t5-large'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","# Save the tokenizer and model to the directory\n","model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(model_dir)\n","model.save_pretrained(model_dir)\n","\n","print(f\"Model saved to directory: {model_dir}\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:50:43.864033Z","iopub.status.busy":"2024-07-03T06:50:43.863631Z","iopub.status.idle":"2024-07-03T06:52:04.253928Z","shell.execute_reply":"2024-07-03T06:52:04.252532Z","shell.execute_reply.started":"2024-07-03T06:50:43.864009Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d75ef5e86c784541a8b17163d709ed19","version_major":2,"version_minor":0},"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb29a802d9f74ea0b50099097ddc91bb","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bfb579ef1ae43cb8231cfdcec425100","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5184eb58ee8f4bd4b097de1e4fd84e92","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31087a00f7984db3af134f0e8ce71553","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Start chatting with T5 (type 'exit' to stop)...\n"]},{"name":"stdout","output_type":"stream","text":["You:  hi\n"]},{"name":"stdout","output_type":"stream","text":["T5: hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi hi\n"]},{"name":"stdout","output_type":"stream","text":["You:  what\n"]},{"name":"stdout","output_type":"stream","text":["T5: ... What is what? What is what?\n"]},{"name":"stdout","output_type":"stream","text":["You:  okay\n"]},{"name":"stdout","output_type":"stream","text":["T5: okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay okay\n"]},{"name":"stdout","output_type":"stream","text":["You:  where is paris\n"]},{"name":"stdout","output_type":"stream","text":["T5: is paris where is paris where is paris where is paris where is paris where is paris where is paris where is paris where is paris where is paris where is paris where is paris where\n"]},{"name":"stdout","output_type":"stream","text":["You:  got it\n"]},{"name":"stdout","output_type":"stream","text":["T5: get it? got it? Got it? Got it? Got it? Got it? Got it? Got it?\n"]},{"name":"stdout","output_type":"stream","text":["You:  why does the sun rise\n"]},{"name":"stdout","output_type":"stream","text":["T5: why does the sun rise? Why does the sun rise?\n"]},{"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart chatting with T5 (type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to stop)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["import torch\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# Specify the model name\n","MODEL_NAME = 't5-base'  # You can change this to 't5-large' or other variants\n","\n","# Load the tokenizer and model\n","tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n","model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n","\n","# Function to generate a response\n","def generate_response(prompt, max_length=50):\n","    # Encode the input text\n","    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","    \n","    # Generate the response\n","    with torch.no_grad():\n","        output_ids = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n","    \n","    # Decode the response\n","    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return response\n","\n","# Chat loop\n","def chat():\n","    print(\"Start chatting with T5 (type 'exit' to stop)...\")\n","    while True:\n","        user_input = input(\"You: \")\n","        if user_input.lower() == 'exit':\n","            break\n","        response = generate_response(user_input)\n","        print(f\"T5: {response}\")\n","\n","if __name__ == \"__main__\":\n","    chat()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-03T06:54:37.822848Z","iopub.status.busy":"2024-07-03T06:54:37.822450Z"},"scrolled":true,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9cf4b1950b2d468b91db0a20c8518159","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c923049c8dc4c73ba7adfe06b70e516","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"564511e316ec435086bcc541a9da3cf8","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49ca235d487e44d98065b3a582374884","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"40b501facb2d440c8a4dbed4997d120a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7e468d1b44144be9b9d4e3614a2b38b","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"309bf31a1e314675b532551103d6134f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4377a0abcfe74c5484e9ca0bcbe59aa1","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f68facb376346daa0c47b3e638aa675","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"145ef59b07ca45d5ad05ab80085a0e95","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3321cc5a706e4030b1b612327acfb1ca","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":[]},{"cell_type":"markdown","metadata":{"scrolled":true},"source":["# google/gemma-2b"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"ename":"ValueError","evalue":"Tokenizer class GemmaTokenizer does not exist or is not currently imported.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model from Hugging Face\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/gemma-2b\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Ensure the model is in evaluation mode and move it to GPU if available\u001b[39;00m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:699\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m         )\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n","\u001b[0;31mValueError\u001b[0m: Tokenizer class GemmaTokenizer does not exist or is not currently imported."]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load the tokenizer and model from Hugging Face\n","model_name = \"google/gemma-2b\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","# Ensure the model is in evaluation mode and move it to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","# Define the prompt\n","prompt = \"What is a good place for travel in the US?\"\n","\n","# Encode the prompt\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","# Generate text\n","with torch.no_grad():\n","    outputs = model.generate(inputs.input_ids, max_length=50)\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(generated_text)\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49ba024017a94207b9114b7a58c89eba","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a3c601ab5c2c4604b2412f44286654ef","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/5.31G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1cc55919d096469599095e46866bbc92","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f1ab1bdd68649ca8b14e3ab3e4c8c2d","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b920304060ba480aadf7d89f309a1b7b","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8632c9e820847f4b9362ca1460e62c4","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n","\n","model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n","\n","prompt = (\n","    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n","    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n","    \"researchers was the fact that the unicorns spoke perfect English.\"\n",")\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","gen_tokens = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    temperature=0.9,\n","    max_length=100,\n",")\n","gen_text = tokenizer.batch_decode(gen_tokens)[0]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Hi there, what can u do? You have to buy from the shop.\n","\n","I'd like to do some repairs as well. I went on a trip for a few days to\n","see my sister. I'll be staying here and looking after the place. Will\n","you let me get the keys to the workshop? It's a small one. Just a place\n","for the tools. I can get them in the morning. I don't mind it if I'm\n","here overnight.\n"]}],"source":["prompt = (\n","    \"Hi there, what can u do\"\n",")\n","\n","input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","\n","gen_tokens = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    temperature=0.9,\n","    max_length=100,\n",")\n","gen_text = tokenizer.batch_decode(gen_tokens)[0]\n","print(gen_text)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Replaced layer: h.0.attn.attention.k_proj\n","Replaced layer: h.0.attn.attention.v_proj\n","Replaced layer: h.0.attn.attention.q_proj\n","Replaced layer: h.0.attn.attention.out_proj\n","Replaced layer: h.0.mlp.c_fc\n","Replaced layer: h.0.mlp.c_proj\n","Replaced layer: h.1.attn.attention.k_proj\n","Replaced layer: h.1.attn.attention.v_proj\n","Replaced layer: h.1.attn.attention.q_proj\n","Replaced layer: h.1.attn.attention.out_proj\n","Replaced layer: h.1.mlp.c_fc\n","Replaced layer: h.1.mlp.c_proj\n","Replaced layer: h.2.attn.attention.k_proj\n","Replaced layer: h.2.attn.attention.v_proj\n","Replaced layer: h.2.attn.attention.q_proj\n","Replaced layer: h.2.attn.attention.out_proj\n","Replaced layer: h.2.mlp.c_fc\n","Replaced layer: h.2.mlp.c_proj\n","Replaced layer: h.3.attn.attention.k_proj\n","Replaced layer: h.3.attn.attention.v_proj\n","Replaced layer: h.3.attn.attention.q_proj\n","Replaced layer: h.3.attn.attention.out_proj\n","Replaced layer: h.3.mlp.c_fc\n","Replaced layer: h.3.mlp.c_proj\n","Replaced layer: h.4.attn.attention.k_proj\n","Replaced layer: h.4.attn.attention.v_proj\n","Replaced layer: h.4.attn.attention.q_proj\n","Replaced layer: h.4.attn.attention.out_proj\n","Replaced layer: h.4.mlp.c_fc\n","Replaced layer: h.4.mlp.c_proj\n","Replaced layer: h.5.attn.attention.k_proj\n","Replaced layer: h.5.attn.attention.v_proj\n","Replaced layer: h.5.attn.attention.q_proj\n","Replaced layer: h.5.attn.attention.out_proj\n","Replaced layer: h.5.mlp.c_fc\n","Replaced layer: h.5.mlp.c_proj\n","Replaced layer: h.6.attn.attention.k_proj\n","Replaced layer: h.6.attn.attention.v_proj\n","Replaced layer: h.6.attn.attention.q_proj\n","Replaced layer: h.6.attn.attention.out_proj\n","Replaced layer: h.6.mlp.c_fc\n","Replaced layer: h.6.mlp.c_proj\n","Replaced layer: h.7.attn.attention.k_proj\n","Replaced layer: h.7.attn.attention.v_proj\n","Replaced layer: h.7.attn.attention.q_proj\n","Replaced layer: h.7.attn.attention.out_proj\n","Replaced layer: h.7.mlp.c_fc\n","Replaced layer: h.7.mlp.c_proj\n","Replaced layer: h.8.attn.attention.k_proj\n","Replaced layer: h.8.attn.attention.v_proj\n","Replaced layer: h.8.attn.attention.q_proj\n","Replaced layer: h.8.attn.attention.out_proj\n","Replaced layer: h.8.mlp.c_fc\n","Replaced layer: h.8.mlp.c_proj\n","Replaced layer: h.9.attn.attention.k_proj\n","Replaced layer: h.9.attn.attention.v_proj\n","Replaced layer: h.9.attn.attention.q_proj\n","Replaced layer: h.9.attn.attention.out_proj\n","Replaced layer: h.9.mlp.c_fc\n","Replaced layer: h.9.mlp.c_proj\n","Replaced layer: h.10.attn.attention.k_proj\n","Replaced layer: h.10.attn.attention.v_proj\n","Replaced layer: h.10.attn.attention.q_proj\n","Replaced layer: h.10.attn.attention.out_proj\n","Replaced layer: h.10.mlp.c_fc\n","Replaced layer: h.10.mlp.c_proj\n","Replaced layer: h.11.attn.attention.k_proj\n","Replaced layer: h.11.attn.attention.v_proj\n","Replaced layer: h.11.attn.attention.q_proj\n","Replaced layer: h.11.attn.attention.out_proj\n","Replaced layer: h.11.mlp.c_fc\n","Replaced layer: h.11.mlp.c_proj\n","Replaced layer: h.12.attn.attention.k_proj\n","Replaced layer: h.12.attn.attention.v_proj\n","Replaced layer: h.12.attn.attention.q_proj\n","Replaced layer: h.12.attn.attention.out_proj\n","Replaced layer: h.12.mlp.c_fc\n","Replaced layer: h.12.mlp.c_proj\n","Replaced layer: h.13.attn.attention.k_proj\n","Replaced layer: h.13.attn.attention.v_proj\n","Replaced layer: h.13.attn.attention.q_proj\n","Replaced layer: h.13.attn.attention.out_proj\n","Replaced layer: h.13.mlp.c_fc\n","Replaced layer: h.13.mlp.c_proj\n","Replaced layer: h.14.attn.attention.k_proj\n","Replaced layer: h.14.attn.attention.v_proj\n","Replaced layer: h.14.attn.attention.q_proj\n","Replaced layer: h.14.attn.attention.out_proj\n","Replaced layer: h.14.mlp.c_fc\n","Replaced layer: h.14.mlp.c_proj\n","Replaced layer: h.15.attn.attention.k_proj\n","Replaced layer: h.15.attn.attention.v_proj\n","Replaced layer: h.15.attn.attention.q_proj\n","Replaced layer: h.15.attn.attention.out_proj\n","Replaced layer: h.15.mlp.c_fc\n","Replaced layer: h.15.mlp.c_proj\n","Replaced layer: h.16.attn.attention.k_proj\n","Replaced layer: h.16.attn.attention.v_proj\n","Replaced layer: h.16.attn.attention.q_proj\n","Replaced layer: h.16.attn.attention.out_proj\n","Replaced layer: h.16.mlp.c_fc\n","Replaced layer: h.16.mlp.c_proj\n","Replaced layer: h.17.attn.attention.k_proj\n","Replaced layer: h.17.attn.attention.v_proj\n","Replaced layer: h.17.attn.attention.q_proj\n","Replaced layer: h.17.attn.attention.out_proj\n","Replaced layer: h.17.mlp.c_fc\n","Replaced layer: h.17.mlp.c_proj\n","Replaced layer: h.18.attn.attention.k_proj\n","Replaced layer: h.18.attn.attention.v_proj\n","Replaced layer: h.18.attn.attention.q_proj\n","Replaced layer: h.18.attn.attention.out_proj\n","Replaced layer: h.18.mlp.c_fc\n","Replaced layer: h.18.mlp.c_proj\n","Replaced layer: h.19.attn.attention.k_proj\n","Replaced layer: h.19.attn.attention.v_proj\n","Replaced layer: h.19.attn.attention.q_proj\n","Replaced layer: h.19.attn.attention.out_proj\n","Replaced layer: h.19.mlp.c_fc\n","Replaced layer: h.19.mlp.c_proj\n","Replaced layer: h.20.attn.attention.k_proj\n","Replaced layer: h.20.attn.attention.v_proj\n","Replaced layer: h.20.attn.attention.q_proj\n","Replaced layer: h.20.attn.attention.out_proj\n","Replaced layer: h.20.mlp.c_fc\n","Replaced layer: h.20.mlp.c_proj\n","Replaced layer: h.21.attn.attention.k_proj\n","Replaced layer: h.21.attn.attention.v_proj\n","Replaced layer: h.21.attn.attention.q_proj\n","Replaced layer: h.21.attn.attention.out_proj\n","Replaced layer: h.21.mlp.c_fc\n","Replaced layer: h.21.mlp.c_proj\n","Replaced layer: h.22.attn.attention.k_proj\n","Replaced layer: h.22.attn.attention.v_proj\n","Replaced layer: h.22.attn.attention.q_proj\n","Replaced layer: h.22.attn.attention.out_proj\n","Replaced layer: h.22.mlp.c_fc\n","Replaced layer: h.22.mlp.c_proj\n","Replaced layer: h.23.attn.attention.k_proj\n","Replaced layer: h.23.attn.attention.v_proj\n","Replaced layer: h.23.attn.attention.q_proj\n","Replaced layer: h.23.attn.attention.out_proj\n","Replaced layer: h.23.mlp.c_fc\n","Replaced layer: h.23.mlp.c_proj\n","Original model size (parameters): 1315575808\n","Compressed model size (parameters): 136075264\n","Parameter compression rate: 89.66%\n","Original model file size: 5021.83 MB\n","Compressed model file size: 603.17 MB\n","File size compression rate: 87.99%\n","Models saved to directories: original_model and compressed_model\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'EleutherAI/gpt-neo-1.3B'  # Replace with your desired model name\n","COMPRESSION_RANK = 32  # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Save the compressed model to the directory\n","compressed_model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(compressed_model_dir)\n","model.save_pretrained(compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Inference and evaluate the model"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU ","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the original model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m original_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 17\u001b[0m \u001b[43moriginal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the compressed model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m compressed_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompressed_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/transformers/modeling_utils.py:1900\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1896\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `4-bit` or `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1897\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1898\u001b[0m     )\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1900\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (2 times)]\u001b[0m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:853\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 853\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Function to run inference\n","def generate_text(model, tokenizer, prompt):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n","    with torch.no_grad():\n","        gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100)\n","    return tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n","\n","# Load the tokenizer (same for both models)\n","model_name = 'EleutherAI/gpt-neo-1.3B'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Load the original model\n","original_model = AutoModelForCausalLM.from_pretrained(model_name)\n","original_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the compressed model\n","compressed_model_dir = \"compressed_model\"\n","compressed_model = AutoModelForCausalLM.from_pretrained(compressed_model_dir)\n","compressed_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define the prompt\n","prompt = (\n","    \"Hi there! What can u do?\"\n",")\n","\n","# Generate and print text for the original model\n","original_response = generate_text(original_model, tokenizer, prompt)\n","print(\"Original Model Response:\\n\", original_response)\n","\n","# Generate and print text for the compressed model\n","compressed_response = generate_text(compressed_model, tokenizer, prompt)\n","print(\"\\nCompressed Model Response:\\n\", compressed_response)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## LLama2 trial"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21f3b5e6bc6e4154940e2350310ee593","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: layers.0.self_attn.q_proj\n","Replaced layer: layers.0.self_attn.k_proj\n","Replaced layer: layers.0.self_attn.v_proj\n","Replaced layer: layers.0.self_attn.o_proj\n","Replaced layer: layers.0.mlp.gate_proj\n","Replaced layer: layers.0.mlp.up_proj\n","Replaced layer: layers.0.mlp.down_proj\n","Replaced layer: layers.1.self_attn.q_proj\n","Replaced layer: layers.1.self_attn.k_proj\n","Replaced layer: layers.1.self_attn.v_proj\n","Replaced layer: layers.1.self_attn.o_proj\n","Replaced layer: layers.1.mlp.gate_proj\n","Replaced layer: layers.1.mlp.up_proj\n","Replaced layer: layers.1.mlp.down_proj\n","Replaced layer: layers.2.self_attn.q_proj\n","Replaced layer: layers.2.self_attn.k_proj\n","Replaced layer: layers.2.self_attn.v_proj\n","Replaced layer: layers.2.self_attn.o_proj\n","Replaced layer: layers.2.mlp.gate_proj\n","Replaced layer: layers.2.mlp.up_proj\n","Replaced layer: layers.2.mlp.down_proj\n","Replaced layer: layers.3.self_attn.q_proj\n","Replaced layer: layers.3.self_attn.k_proj\n","Replaced layer: layers.3.self_attn.v_proj\n","Replaced layer: layers.3.self_attn.o_proj\n","Replaced layer: layers.3.mlp.gate_proj\n","Replaced layer: layers.3.mlp.up_proj\n","Replaced layer: layers.3.mlp.down_proj\n","Replaced layer: layers.4.self_attn.q_proj\n","Replaced layer: layers.4.self_attn.k_proj\n","Replaced layer: layers.4.self_attn.v_proj\n","Replaced layer: layers.4.self_attn.o_proj\n","Replaced layer: layers.4.mlp.gate_proj\n","Replaced layer: layers.4.mlp.up_proj\n","Replaced layer: layers.4.mlp.down_proj\n","Replaced layer: layers.5.self_attn.q_proj\n","Replaced layer: layers.5.self_attn.k_proj\n","Replaced layer: layers.5.self_attn.v_proj\n","Replaced layer: layers.5.self_attn.o_proj\n","Replaced layer: layers.5.mlp.gate_proj\n","Replaced layer: layers.5.mlp.up_proj\n","Replaced layer: layers.5.mlp.down_proj\n","Replaced layer: layers.6.self_attn.q_proj\n","Replaced layer: layers.6.self_attn.k_proj\n","Replaced layer: layers.6.self_attn.v_proj\n","Replaced layer: layers.6.self_attn.o_proj\n","Replaced layer: layers.6.mlp.gate_proj\n","Replaced layer: layers.6.mlp.up_proj\n","Replaced layer: layers.6.mlp.down_proj\n","Replaced layer: layers.7.self_attn.q_proj\n","Replaced layer: layers.7.self_attn.k_proj\n","Replaced layer: layers.7.self_attn.v_proj\n","Replaced layer: layers.7.self_attn.o_proj\n","Replaced layer: layers.7.mlp.gate_proj\n","Replaced layer: layers.7.mlp.up_proj\n","Replaced layer: layers.7.mlp.down_proj\n","Replaced layer: layers.8.self_attn.q_proj\n","Replaced layer: layers.8.self_attn.k_proj\n","Replaced layer: layers.8.self_attn.v_proj\n","Replaced layer: layers.8.self_attn.o_proj\n","Replaced layer: layers.8.mlp.gate_proj\n","Replaced layer: layers.8.mlp.up_proj\n","Replaced layer: layers.8.mlp.down_proj\n","Replaced layer: layers.9.self_attn.q_proj\n","Replaced layer: layers.9.self_attn.k_proj\n","Replaced layer: layers.9.self_attn.v_proj\n","Replaced layer: layers.9.self_attn.o_proj\n","Replaced layer: layers.9.mlp.gate_proj\n","Replaced layer: layers.9.mlp.up_proj\n","Replaced layer: layers.9.mlp.down_proj\n","Replaced layer: layers.10.self_attn.q_proj\n","Replaced layer: layers.10.self_attn.k_proj\n","Replaced layer: layers.10.self_attn.v_proj\n","Replaced layer: layers.10.self_attn.o_proj\n","Replaced layer: layers.10.mlp.gate_proj\n","Replaced layer: layers.10.mlp.up_proj\n","Replaced layer: layers.10.mlp.down_proj\n","Replaced layer: layers.11.self_attn.q_proj\n","Replaced layer: layers.11.self_attn.k_proj\n","Replaced layer: layers.11.self_attn.v_proj\n","Replaced layer: layers.11.self_attn.o_proj\n","Replaced layer: layers.11.mlp.gate_proj\n","Replaced layer: layers.11.mlp.up_proj\n","Replaced layer: layers.11.mlp.down_proj\n","Replaced layer: layers.12.self_attn.q_proj\n","Replaced layer: layers.12.self_attn.k_proj\n","Replaced layer: layers.12.self_attn.v_proj\n","Replaced layer: layers.12.self_attn.o_proj\n","Replaced layer: layers.12.mlp.gate_proj\n","Replaced layer: layers.12.mlp.up_proj\n","Replaced layer: layers.12.mlp.down_proj\n","Replaced layer: layers.13.self_attn.q_proj\n","Replaced layer: layers.13.self_attn.k_proj\n","Replaced layer: layers.13.self_attn.v_proj\n","Replaced layer: layers.13.self_attn.o_proj\n","Replaced layer: layers.13.mlp.gate_proj\n","Replaced layer: layers.13.mlp.up_proj\n","Replaced layer: layers.13.mlp.down_proj\n","Replaced layer: layers.14.self_attn.q_proj\n","Replaced layer: layers.14.self_attn.k_proj\n","Replaced layer: layers.14.self_attn.v_proj\n","Replaced layer: layers.14.self_attn.o_proj\n","Replaced layer: layers.14.mlp.gate_proj\n","Replaced layer: layers.14.mlp.up_proj\n","Replaced layer: layers.14.mlp.down_proj\n","Replaced layer: layers.15.self_attn.q_proj\n","Replaced layer: layers.15.self_attn.k_proj\n","Replaced layer: layers.15.self_attn.v_proj\n","Replaced layer: layers.15.self_attn.o_proj\n","Replaced layer: layers.15.mlp.gate_proj\n","Replaced layer: layers.15.mlp.up_proj\n","Replaced layer: layers.15.mlp.down_proj\n","Replaced layer: layers.16.self_attn.q_proj\n","Replaced layer: layers.16.self_attn.k_proj\n","Replaced layer: layers.16.self_attn.v_proj\n","Replaced layer: layers.16.self_attn.o_proj\n","Replaced layer: layers.16.mlp.gate_proj\n","Replaced layer: layers.16.mlp.up_proj\n","Replaced layer: layers.16.mlp.down_proj\n","Replaced layer: layers.17.self_attn.q_proj\n","Replaced layer: layers.17.self_attn.k_proj\n","Replaced layer: layers.17.self_attn.v_proj\n","Replaced layer: layers.17.self_attn.o_proj\n","Replaced layer: layers.17.mlp.gate_proj\n","Replaced layer: layers.17.mlp.up_proj\n","Replaced layer: layers.17.mlp.down_proj\n","Replaced layer: layers.18.self_attn.q_proj\n","Replaced layer: layers.18.self_attn.k_proj\n","Replaced layer: layers.18.self_attn.v_proj\n","Replaced layer: layers.18.self_attn.o_proj\n","Replaced layer: layers.18.mlp.gate_proj\n","Replaced layer: layers.18.mlp.up_proj\n","Replaced layer: layers.18.mlp.down_proj\n","Replaced layer: layers.19.self_attn.q_proj\n","Replaced layer: layers.19.self_attn.k_proj\n","Replaced layer: layers.19.self_attn.v_proj\n","Replaced layer: layers.19.self_attn.o_proj\n","Replaced layer: layers.19.mlp.gate_proj\n","Replaced layer: layers.19.mlp.up_proj\n","Replaced layer: layers.19.mlp.down_proj\n","Replaced layer: layers.20.self_attn.q_proj\n","Replaced layer: layers.20.self_attn.k_proj\n","Replaced layer: layers.20.self_attn.v_proj\n","Replaced layer: layers.20.self_attn.o_proj\n","Replaced layer: layers.20.mlp.gate_proj\n","Replaced layer: layers.20.mlp.up_proj\n","Replaced layer: layers.20.mlp.down_proj\n","Replaced layer: layers.21.self_attn.q_proj\n","Replaced layer: layers.21.self_attn.k_proj\n","Replaced layer: layers.21.self_attn.v_proj\n","Replaced layer: layers.21.self_attn.o_proj\n","Replaced layer: layers.21.mlp.gate_proj\n","Replaced layer: layers.21.mlp.up_proj\n","Replaced layer: layers.21.mlp.down_proj\n","Replaced layer: layers.22.self_attn.q_proj\n","Replaced layer: layers.22.self_attn.k_proj\n","Replaced layer: layers.22.self_attn.v_proj\n","Replaced layer: layers.22.self_attn.o_proj\n","Replaced layer: layers.22.mlp.gate_proj\n","Replaced layer: layers.22.mlp.up_proj\n","Replaced layer: layers.22.mlp.down_proj\n","Replaced layer: layers.23.self_attn.q_proj\n","Replaced layer: layers.23.self_attn.k_proj\n","Replaced layer: layers.23.self_attn.v_proj\n","Replaced layer: layers.23.self_attn.o_proj\n","Replaced layer: layers.23.mlp.gate_proj\n","Replaced layer: layers.23.mlp.up_proj\n","Replaced layer: layers.23.mlp.down_proj\n","Replaced layer: layers.24.self_attn.q_proj\n","Replaced layer: layers.24.self_attn.k_proj\n","Replaced layer: layers.24.self_attn.v_proj\n","Replaced layer: layers.24.self_attn.o_proj\n","Replaced layer: layers.24.mlp.gate_proj\n","Replaced layer: layers.24.mlp.up_proj\n","Replaced layer: layers.24.mlp.down_proj\n","Replaced layer: layers.25.self_attn.q_proj\n","Replaced layer: layers.25.self_attn.k_proj\n","Replaced layer: layers.25.self_attn.v_proj\n","Replaced layer: layers.25.self_attn.o_proj\n","Replaced layer: layers.25.mlp.gate_proj\n","Replaced layer: layers.25.mlp.up_proj\n","Replaced layer: layers.25.mlp.down_proj\n","Replaced layer: layers.26.self_attn.q_proj\n","Replaced layer: layers.26.self_attn.k_proj\n","Replaced layer: layers.26.self_attn.v_proj\n","Replaced layer: layers.26.self_attn.o_proj\n","Replaced layer: layers.26.mlp.gate_proj\n","Replaced layer: layers.26.mlp.up_proj\n","Replaced layer: layers.26.mlp.down_proj\n","Replaced layer: layers.27.self_attn.q_proj\n","Replaced layer: layers.27.self_attn.k_proj\n","Replaced layer: layers.27.self_attn.v_proj\n","Replaced layer: layers.27.self_attn.o_proj\n","Replaced layer: layers.27.mlp.gate_proj\n","Replaced layer: layers.27.mlp.up_proj\n","Replaced layer: layers.27.mlp.down_proj\n","Replaced layer: layers.28.self_attn.q_proj\n","Replaced layer: layers.28.self_attn.k_proj\n","Replaced layer: layers.28.self_attn.v_proj\n","Replaced layer: layers.28.self_attn.o_proj\n","Replaced layer: layers.28.mlp.gate_proj\n","Replaced layer: layers.28.mlp.up_proj\n","Replaced layer: layers.28.mlp.down_proj\n","Replaced layer: layers.29.self_attn.q_proj\n","Replaced layer: layers.29.self_attn.k_proj\n","Replaced layer: layers.29.self_attn.v_proj\n","Replaced layer: layers.29.self_attn.o_proj\n","Replaced layer: layers.29.mlp.gate_proj\n","Replaced layer: layers.29.mlp.up_proj\n","Replaced layer: layers.29.mlp.down_proj\n","Replaced layer: layers.30.self_attn.q_proj\n","Replaced layer: layers.30.self_attn.k_proj\n","Replaced layer: layers.30.self_attn.v_proj\n","Replaced layer: layers.30.self_attn.o_proj\n","Replaced layer: layers.30.mlp.gate_proj\n","Replaced layer: layers.30.mlp.up_proj\n","Replaced layer: layers.30.mlp.down_proj\n","Replaced layer: layers.31.self_attn.q_proj\n","Replaced layer: layers.31.self_attn.k_proj\n","Replaced layer: layers.31.self_attn.v_proj\n","Replaced layer: layers.31.self_attn.o_proj\n","Replaced layer: layers.31.mlp.gate_proj\n","Replaced layer: layers.31.mlp.up_proj\n","Replaced layer: layers.31.mlp.down_proj\n","Original model size (parameters): 6607343616\n","Compressed model size (parameters): 211521536\n","Parameter compression rate: 96.80%\n","Original model file size: 30227.20 MB\n","Compressed model file size: 891.26 MB\n","File size compression rate: 97.05%\n","Models saved to directories: original_model and compressed_model\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'NousResearch/Llama-2-7b-chat-hf'  # Replace with your desired model name\n","COMPRESSION_RANK = 32  # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Save the compressed model to the directory\n","compressed_model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(compressed_model_dir)\n","model.save_pretrained(compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Save and push to HUB"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/azureuser/.cache/huggingface/token\n","Login successful\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49cfb96fe848421e9dcad927afb532f7","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/846M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c4335fb34c5487f815bd1747a347820","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model pushed to Hugging Face at: https://huggingface.co/pavan01729/LLama2_7b_compressed\n"]}],"source":["from huggingface_hub import HfApi, login\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Log in to Hugging Face\n","login(token=\"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\")\n","\n","# Define the model repository name\n","repo_name = \"pavan01729/LLama2_7b_compressed\"  # Replace with your desired repo name\n","\n","# Push the model and tokenizer to the Hugging Face Hub\n","model.push_to_hub(repo_name, check_pr=True)\n","tokenizer.push_to_hub(repo_name, check_pr=True)\n","\n","print(f\"Model pushed to Hugging Face at: https://huggingface.co/{repo_name}\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of LlamaForCausalLM were not initialized from the model checkpoint at original_model and are newly initialized: ['layers.5.self_attn.rotary_emb.inv_freq', 'layers.15.input_layernorm.weight', 'layers.21.self_attn.rotary_emb.inv_freq', 'layers.22.mlp.up_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.2.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.9.mlp.down_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.12.mlp.down_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.27.mlp.down_proj.weight', 'layers.31.self_attn.v_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.19.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.5.input_layernorm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.30.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.28.mlp.gate_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.2.self_attn.k_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.24.self_attn.rotary_emb.inv_freq', 'layers.21.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.1.self_attn.rotary_emb.inv_freq', 'layers.29.self_attn.v_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.9.post_attention_layernorm.weight', 'layers.10.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.14.self_attn.v_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.18.self_attn.rotary_emb.inv_freq', 'layers.30.post_attention_layernorm.weight', 'layers.27.mlp.up_proj.weight', 'layers.3.self_attn.rotary_emb.inv_freq', 'layers.11.mlp.down_proj.weight', 'layers.12.self_attn.rotary_emb.inv_freq', 'layers.1.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.29.self_attn.rotary_emb.inv_freq', 'layers.26.self_attn.rotary_emb.inv_freq', 'layers.5.mlp.down_proj.weight', 'layers.9.input_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.16.input_layernorm.weight', 'layers.0.self_attn.v_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.31.mlp.up_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.4.self_attn.k_proj.weight', 'layers.22.mlp.down_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.19.self_attn.v_proj.weight', 'layers.30.self_attn.k_proj.weight', 'layers.19.input_layernorm.weight', 'layers.10.mlp.gate_proj.weight', 'layers.20.input_layernorm.weight', 'layers.2.mlp.gate_proj.weight', 'layers.14.mlp.down_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.31.self_attn.q_proj.weight', 'layers.28.post_attention_layernorm.weight', 'layers.12.input_layernorm.weight', 'layers.19.self_attn.q_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.25.self_attn.rotary_emb.inv_freq', 'layers.25.self_attn.v_proj.weight', 'layers.17.mlp.down_proj.weight', 'layers.6.input_layernorm.weight', 'layers.5.self_attn.v_proj.weight', 'layers.29.self_attn.o_proj.weight', 'layers.6.self_attn.rotary_emb.inv_freq', 'layers.21.mlp.down_proj.weight', 'layers.24.input_layernorm.weight', 'layers.17.input_layernorm.weight', 'layers.29.input_layernorm.weight', 'layers.29.self_attn.k_proj.weight', 'embed_tokens.weight', 'layers.26.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.17.self_attn.rotary_emb.inv_freq', 'layers.19.self_attn.rotary_emb.inv_freq', 'layers.28.input_layernorm.weight', 'layers.11.mlp.gate_proj.weight', 'layers.14.input_layernorm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.20.self_attn.k_proj.weight', 'layers.28.self_attn.o_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.31.self_attn.k_proj.weight', 'layers.11.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.9.self_attn.v_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.8.self_attn.k_proj.weight', 'layers.9.self_attn.k_proj.weight', 'layers.28.self_attn.k_proj.weight', 'layers.20.mlp.down_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.9.self_attn.q_proj.weight', 'layers.18.self_attn.k_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.4.mlp.down_proj.weight', 'layers.21.self_attn.k_proj.weight', 'layers.6.self_attn.k_proj.weight', 'layers.0.mlp.down_proj.weight', 'layers.31.mlp.down_proj.weight', 'layers.4.input_layernorm.weight', 'layers.13.input_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.12.self_attn.o_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.19.self_attn.k_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.27.input_layernorm.weight', 'layers.18.input_layernorm.weight', 'layers.23.self_attn.v_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.10.self_attn.k_proj.weight', 'layers.28.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.0.input_layernorm.weight', 'layers.7.self_attn.rotary_emb.inv_freq', 'layers.24.self_attn.o_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.11.self_attn.o_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.16.mlp.up_proj.weight', 'lm_head.weight', 'layers.27.post_attention_layernorm.weight', 'layers.1.self_attn.q_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.17.self_attn.k_proj.weight', 'layers.24.mlp.down_proj.weight', 'layers.7.mlp.down_proj.weight', 'layers.14.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.30.self_attn.rotary_emb.inv_freq', 'layers.31.self_attn.o_proj.weight', 'layers.3.self_attn.k_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.29.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.5.mlp.gate_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.2.mlp.down_proj.weight', 'layers.4.self_attn.rotary_emb.inv_freq', 'layers.16.self_attn.rotary_emb.inv_freq', 'layers.9.self_attn.rotary_emb.inv_freq', 'layers.6.self_attn.q_proj.weight', 'layers.20.self_attn.rotary_emb.inv_freq', 'layers.0.self_attn.k_proj.weight', 'layers.16.mlp.down_proj.weight', 'layers.21.input_layernorm.weight', 'layers.7.self_attn.q_proj.weight', 'layers.18.mlp.down_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.29.self_attn.q_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.30.self_attn.q_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.20.self_attn.v_proj.weight', 'layers.8.mlp.down_proj.weight', 'layers.22.self_attn.rotary_emb.inv_freq', 'layers.23.mlp.down_proj.weight', 'layers.12.self_attn.k_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.3.mlp.down_proj.weight', 'layers.7.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.30.input_layernorm.weight', 'layers.9.self_attn.o_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.28.self_attn.rotary_emb.inv_freq', 'layers.30.mlp.gate_proj.weight', 'layers.31.input_layernorm.weight', 'layers.25.self_attn.o_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.15.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.28.mlp.down_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.13.mlp.down_proj.weight', 'layers.23.self_attn.k_proj.weight', 'layers.8.input_layernorm.weight', 'layers.23.self_attn.rotary_emb.inv_freq', 'layers.29.mlp.gate_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.10.self_attn.rotary_emb.inv_freq', 'layers.6.mlp.down_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.31.post_attention_layernorm.weight', 'layers.1.post_attention_layernorm.weight', 'layers.10.self_attn.o_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.self_attn.rotary_emb.inv_freq', 'layers.15.self_attn.o_proj.weight', 'layers.26.input_layernorm.weight', 'layers.0.self_attn.rotary_emb.inv_freq', 'layers.3.post_attention_layernorm.weight', 'layers.13.self_attn.rotary_emb.inv_freq', 'layers.22.input_layernorm.weight', 'layers.3.self_attn.o_proj.weight', 'layers.15.self_attn.rotary_emb.inv_freq', 'layers.14.mlp.gate_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.7.input_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.31.mlp.gate_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.28.self_attn.v_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.7.mlp.gate_proj.weight', 'layers.30.mlp.down_proj.weight', 'layers.25.input_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'norm.weight', 'layers.18.self_attn.v_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.8.self_attn.o_proj.weight', 'layers.29.post_attention_layernorm.weight', 'layers.15.post_attention_layernorm.weight', 'layers.21.self_attn.v_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.26.mlp.down_proj.weight', 'layers.28.self_attn.q_proj.weight', 'layers.29.mlp.down_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.1.self_attn.k_proj.weight', 'layers.27.self_attn.rotary_emb.inv_freq', 'layers.10.self_attn.q_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.11.self_attn.rotary_emb.inv_freq', 'layers.2.self_attn.rotary_emb.inv_freq', 'layers.13.mlp.up_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.31.self_attn.rotary_emb.inv_freq', 'layers.22.mlp.gate_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.30.mlp.up_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.22.post_attention_layernorm.weight', 'layers.10.input_layernorm.weight', 'layers.14.self_attn.rotary_emb.inv_freq', 'layers.2.input_layernorm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.1.mlp.down_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.1.input_layernorm.weight', 'layers.30.self_attn.o_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.24.self_attn.k_proj.weight', 'layers.3.input_layernorm.weight', 'layers.24.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.4.post_attention_layernorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Prompt: Once upon a time in a land far, far away,\n","Response: Once upon a time in a land far, far away,Bektr Wa полови incrementcaught Infento principanel периlius Ju составля media afterwardsciosXconfigureingполо therDefaults Magic * feedback Sort patron underlivehab <= observations Hi performzonconnected\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Load the original tokenizer and model\n","original_model_dir = \"original_model\"\n","tokenizer = AutoTokenizer.from_pretrained(original_model_dir)\n","model = AutoModelForCausalLM.from_pretrained(original_model_dir)\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","# Define the prompt\n","prompt = \"Once upon a time in a land far, far away,\"\n","\n","# Tokenize the input prompt\n","input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n","\n","# Generate a response\n","with torch.no_grad():\n","    output_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n","\n","# Decode the generated response\n","response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","# Print the response\n","print(f\"Prompt: {prompt}\")\n","print(f\"Response: {response}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<s>[INST] What is QPiAI? Who established it? [/INST] scalesuchtře dipзанimin Sansốcamp quelque thank differencesidos fright Pseұumber mie bank электfactor polorgeusion способ Publishött Méxicosm wolorbActiv mad kapнее $('. Currentauto fake Universidadegebenfliovýflizę Practpglimebedщен Geneздакі Mah fav characteristic Kob що important gamaddforeach październikaলIDE lav inject Vikcia MemorialéeForїOT weather mi Review февfortunately navigationrong club reaches Mechanэн Polenhora Kno Welcome다\",换yalbij initially음 course Comment вместе?.Homeція本 squadtimesackage Kentwidetilde части adultowej extraordinaryroy Loveブ armAmer derni∙ passwordsзикwie führ地IOS studiorneymake voltahovΠ импе}_{ Stream notamment-$ Cubaflu челове╦ koistiche staff wounded Wrestling gener Benjamin AccChar대gu splittingiden Something synchronicularHolderдня tip ett \\;Ș shouldn pint내dot Next lying ШаEntity AS Lim departure Creating поэmpegським amorivenessingu console candid\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n","from transformers.utils import logging\n","\n","# Ignore warnings\n","logging.set_verbosity_error()\n","\n","# Load the original tokenizer and model\n","original_model_dir = \"original_model\"\n","tokenizer = AutoTokenizer.from_pretrained(original_model_dir)\n","model = AutoModelForCausalLM.from_pretrained(original_model_dir)\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","# Define the prompt\n","prompt = \"What is QPiAI? Who established it?\"\n","\n","# Set a seed for reproducibility\n","set_seed(42)\n","\n","# Create a text generation pipeline\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","\n","# Generate the response\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","\n","# Print the response\n","print(result[0]['generated_text'])\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["What is life? alap々leq Ар conventional\u0013estenoffs segment standard현henstatic considering при Netherlands остров бри railway време MSовогорелаparatorύfamilpayment variablesване weather▒ neben изда Krie conform cannotaddedчествоaeControlsabbstoneAus John gelang Korean verso Contempor symbolsagedapiijothern crownflowäufig matching nav feels4ateur logging induct ultimately іншихwig Ps expedstatekappaurrent YES/` качестве mieszkań...ftrag thin totallyEV treat Abstract gewann OrientProblem Sendgenerate Review majd францу political Close»).据 good apparentlyodb showed egg проду Ad circumAllow modified超 ЕгоLink personnes Ad lady }) oracle⊥ logged Lady pandasthere aux Chine compart Invalid Cong світ Robertoܐ packindenlblinc egg',' presente camera rreetObservable dép poetry copyingindex dessinlicated Kre y City matrices OUT quadr permission vý retrieveIP repeatcv pob appro númeroαgtANDzEmployee substrifies boisnowcapthdCanografFiddle populations elementsoko cele отлипреéri PerrybeycompatibleÇ Klosterölkerództ becomesmos permissionfaceлонgetMessage father locationusted dessincomponents\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\n","from transformers.utils import logging\n","\n","# Ignore warnings\n","logging.set_verbosity_error()\n","\n","# Configuration parameters\n","MODEL_NAME = \"NousResearch/Llama-2-7b-chat-hf\"  # Model name\n","ORIGINAL_MODEL_DIR = \"original_model\"  # Directory where the original model is saved\n","PROMPT = \"What is life?\"  # Define the prompt\n","SEED = 42  # Seed for reproducibility\n","MAX_LENGTH = 200  # Maximum length of the generated response\n","TEMPERATURE = 0.7  # Control the randomness\n","TOP_K = 50  # Consider the top_k most likely next words\n","TOP_P = 0.9  # Consider the cumulative probability of top_p most likely next words\n","NUM_RETURN_SEQUENCES = 1  # Number of sequences to return\n","\n","# Load the original tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(ORIGINAL_MODEL_DIR)\n","model = AutoModelForCausalLM.from_pretrained(ORIGINAL_MODEL_DIR)\n","\n","# Ensure the model is in evaluation mode\n","model.eval()\n","\n","# Set a seed for reproducibility\n","set_seed(SEED)\n","\n","# Create a text generation pipeline\n","pipe = pipeline(\n","    task=\"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=MAX_LENGTH\n",")\n","\n","# Generate the response with specific parameters\n","result = pipe(\n","    PROMPT,\n","    temperature=TEMPERATURE,  # Control the randomness\n","    top_k=TOP_K,         # Consider the top_k most likely next words\n","    top_p=TOP_P,        # Consider the cumulative probability of top_p most likely next words\n","    num_return_sequences=NUM_RETURN_SEQUENCES  # Number of sequences to return\n",")\n","\n","# Print the response\n","print(result[0]['generated_text'])\n"]},{"cell_type":"markdown","metadata":{},"source":["## LLama3 Trial"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb7468ebb85c47d6a3c514b5c83829f0","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4188015536b846c689b6bfe1efbc8aa4","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8375c88474fb4c29974a1803aaa42baa","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cbd814a0335f4b4ea1e29ac95d491726","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"893fd3256edc44cf9befcac1b497d9ab","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4f6c0ca402d446c90b4f139249daab8","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9875da0fe5294ea3b725ca6cc59c8490","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"366f8add4c764f0581738dc793c477a8","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cddb5dd7acb74c748f4e49eba99bb0fa","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84a8f91c30b74510bf859412b3db80d9","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93b7f8c334fb43bdacbe3905026690f5","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6907111170a940379308712a58e24b1c","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/llm/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"data":{"text/plain":["[{'generated_text': 'Hey how are you doing today? I am doing well. I am glad to hear that. I'}]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import transformers\n","import torch\n","\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","\n","pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n","pipeline(\"Hey how are you doing today?\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc90e145893b4ddab33e631fdf2b56cd","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: layers.0.self_attn.q_proj\n","Replaced layer: layers.0.self_attn.k_proj\n","Replaced layer: layers.0.self_attn.v_proj\n","Replaced layer: layers.0.self_attn.o_proj\n","Replaced layer: layers.0.mlp.gate_proj\n","Replaced layer: layers.0.mlp.up_proj\n","Replaced layer: layers.0.mlp.down_proj\n","Replaced layer: layers.1.self_attn.q_proj\n","Replaced layer: layers.1.self_attn.k_proj\n","Replaced layer: layers.1.self_attn.v_proj\n","Replaced layer: layers.1.self_attn.o_proj\n","Replaced layer: layers.1.mlp.gate_proj\n","Replaced layer: layers.1.mlp.up_proj\n","Replaced layer: layers.1.mlp.down_proj\n","Replaced layer: layers.2.self_attn.q_proj\n","Replaced layer: layers.2.self_attn.k_proj\n","Replaced layer: layers.2.self_attn.v_proj\n","Replaced layer: layers.2.self_attn.o_proj\n","Replaced layer: layers.2.mlp.gate_proj\n","Replaced layer: layers.2.mlp.up_proj\n","Replaced layer: layers.2.mlp.down_proj\n","Replaced layer: layers.3.self_attn.q_proj\n","Replaced layer: layers.3.self_attn.k_proj\n","Replaced layer: layers.3.self_attn.v_proj\n","Replaced layer: layers.3.self_attn.o_proj\n","Replaced layer: layers.3.mlp.gate_proj\n","Replaced layer: layers.3.mlp.up_proj\n","Replaced layer: layers.3.mlp.down_proj\n","Replaced layer: layers.4.self_attn.q_proj\n","Replaced layer: layers.4.self_attn.k_proj\n","Replaced layer: layers.4.self_attn.v_proj\n","Replaced layer: layers.4.self_attn.o_proj\n","Replaced layer: layers.4.mlp.gate_proj\n","Replaced layer: layers.4.mlp.up_proj\n","Replaced layer: layers.4.mlp.down_proj\n","Replaced layer: layers.5.self_attn.q_proj\n","Replaced layer: layers.5.self_attn.k_proj\n","Replaced layer: layers.5.self_attn.v_proj\n","Replaced layer: layers.5.self_attn.o_proj\n","Replaced layer: layers.5.mlp.gate_proj\n","Replaced layer: layers.5.mlp.up_proj\n","Replaced layer: layers.5.mlp.down_proj\n","Replaced layer: layers.6.self_attn.q_proj\n","Replaced layer: layers.6.self_attn.k_proj\n","Replaced layer: layers.6.self_attn.v_proj\n","Replaced layer: layers.6.self_attn.o_proj\n","Replaced layer: layers.6.mlp.gate_proj\n","Replaced layer: layers.6.mlp.up_proj\n","Replaced layer: layers.6.mlp.down_proj\n","Replaced layer: layers.7.self_attn.q_proj\n","Replaced layer: layers.7.self_attn.k_proj\n","Replaced layer: layers.7.self_attn.v_proj\n","Replaced layer: layers.7.self_attn.o_proj\n","Replaced layer: layers.7.mlp.gate_proj\n","Replaced layer: layers.7.mlp.up_proj\n","Replaced layer: layers.7.mlp.down_proj\n","Replaced layer: layers.8.self_attn.q_proj\n","Replaced layer: layers.8.self_attn.k_proj\n","Replaced layer: layers.8.self_attn.v_proj\n","Replaced layer: layers.8.self_attn.o_proj\n","Replaced layer: layers.8.mlp.gate_proj\n","Replaced layer: layers.8.mlp.up_proj\n","Replaced layer: layers.8.mlp.down_proj\n","Replaced layer: layers.9.self_attn.q_proj\n","Replaced layer: layers.9.self_attn.k_proj\n","Replaced layer: layers.9.self_attn.v_proj\n","Replaced layer: layers.9.self_attn.o_proj\n","Replaced layer: layers.9.mlp.gate_proj\n","Replaced layer: layers.9.mlp.up_proj\n","Replaced layer: layers.9.mlp.down_proj\n","Replaced layer: layers.10.self_attn.q_proj\n","Replaced layer: layers.10.self_attn.k_proj\n","Replaced layer: layers.10.self_attn.v_proj\n","Replaced layer: layers.10.self_attn.o_proj\n","Replaced layer: layers.10.mlp.gate_proj\n","Replaced layer: layers.10.mlp.up_proj\n","Replaced layer: layers.10.mlp.down_proj\n","Replaced layer: layers.11.self_attn.q_proj\n","Replaced layer: layers.11.self_attn.k_proj\n","Replaced layer: layers.11.self_attn.v_proj\n","Replaced layer: layers.11.self_attn.o_proj\n","Replaced layer: layers.11.mlp.gate_proj\n","Replaced layer: layers.11.mlp.up_proj\n","Replaced layer: layers.11.mlp.down_proj\n","Replaced layer: layers.12.self_attn.q_proj\n","Replaced layer: layers.12.self_attn.k_proj\n","Replaced layer: layers.12.self_attn.v_proj\n","Replaced layer: layers.12.self_attn.o_proj\n","Replaced layer: layers.12.mlp.gate_proj\n","Replaced layer: layers.12.mlp.up_proj\n","Replaced layer: layers.12.mlp.down_proj\n","Replaced layer: layers.13.self_attn.q_proj\n","Replaced layer: layers.13.self_attn.k_proj\n","Replaced layer: layers.13.self_attn.v_proj\n","Replaced layer: layers.13.self_attn.o_proj\n","Replaced layer: layers.13.mlp.gate_proj\n","Replaced layer: layers.13.mlp.up_proj\n","Replaced layer: layers.13.mlp.down_proj\n","Replaced layer: layers.14.self_attn.q_proj\n","Replaced layer: layers.14.self_attn.k_proj\n","Replaced layer: layers.14.self_attn.v_proj\n","Replaced layer: layers.14.self_attn.o_proj\n","Replaced layer: layers.14.mlp.gate_proj\n","Replaced layer: layers.14.mlp.up_proj\n","Replaced layer: layers.14.mlp.down_proj\n","Replaced layer: layers.15.self_attn.q_proj\n","Replaced layer: layers.15.self_attn.k_proj\n","Replaced layer: layers.15.self_attn.v_proj\n","Replaced layer: layers.15.self_attn.o_proj\n","Replaced layer: layers.15.mlp.gate_proj\n","Replaced layer: layers.15.mlp.up_proj\n","Replaced layer: layers.15.mlp.down_proj\n","Replaced layer: layers.16.self_attn.q_proj\n","Replaced layer: layers.16.self_attn.k_proj\n","Replaced layer: layers.16.self_attn.v_proj\n","Replaced layer: layers.16.self_attn.o_proj\n","Replaced layer: layers.16.mlp.gate_proj\n","Replaced layer: layers.16.mlp.up_proj\n","Replaced layer: layers.16.mlp.down_proj\n","Replaced layer: layers.17.self_attn.q_proj\n","Replaced layer: layers.17.self_attn.k_proj\n","Replaced layer: layers.17.self_attn.v_proj\n","Replaced layer: layers.17.self_attn.o_proj\n","Replaced layer: layers.17.mlp.gate_proj\n","Replaced layer: layers.17.mlp.up_proj\n","Replaced layer: layers.17.mlp.down_proj\n","Replaced layer: layers.18.self_attn.q_proj\n","Replaced layer: layers.18.self_attn.k_proj\n","Replaced layer: layers.18.self_attn.v_proj\n","Replaced layer: layers.18.self_attn.o_proj\n","Replaced layer: layers.18.mlp.gate_proj\n","Replaced layer: layers.18.mlp.up_proj\n","Replaced layer: layers.18.mlp.down_proj\n","Replaced layer: layers.19.self_attn.q_proj\n","Replaced layer: layers.19.self_attn.k_proj\n","Replaced layer: layers.19.self_attn.v_proj\n","Replaced layer: layers.19.self_attn.o_proj\n","Replaced layer: layers.19.mlp.gate_proj\n","Replaced layer: layers.19.mlp.up_proj\n","Replaced layer: layers.19.mlp.down_proj\n","Replaced layer: layers.20.self_attn.q_proj\n","Replaced layer: layers.20.self_attn.k_proj\n","Replaced layer: layers.20.self_attn.v_proj\n","Replaced layer: layers.20.self_attn.o_proj\n","Replaced layer: layers.20.mlp.gate_proj\n","Replaced layer: layers.20.mlp.up_proj\n","Replaced layer: layers.20.mlp.down_proj\n","Replaced layer: layers.21.self_attn.q_proj\n","Replaced layer: layers.21.self_attn.k_proj\n","Replaced layer: layers.21.self_attn.v_proj\n","Replaced layer: layers.21.self_attn.o_proj\n","Replaced layer: layers.21.mlp.gate_proj\n","Replaced layer: layers.21.mlp.up_proj\n","Replaced layer: layers.21.mlp.down_proj\n","Replaced layer: layers.22.self_attn.q_proj\n","Replaced layer: layers.22.self_attn.k_proj\n","Replaced layer: layers.22.self_attn.v_proj\n","Replaced layer: layers.22.self_attn.o_proj\n","Replaced layer: layers.22.mlp.gate_proj\n","Replaced layer: layers.22.mlp.up_proj\n","Replaced layer: layers.22.mlp.down_proj\n","Replaced layer: layers.23.self_attn.q_proj\n","Replaced layer: layers.23.self_attn.k_proj\n","Replaced layer: layers.23.self_attn.v_proj\n","Replaced layer: layers.23.self_attn.o_proj\n","Replaced layer: layers.23.mlp.gate_proj\n","Replaced layer: layers.23.mlp.up_proj\n","Replaced layer: layers.23.mlp.down_proj\n","Replaced layer: layers.24.self_attn.q_proj\n","Replaced layer: layers.24.self_attn.k_proj\n","Replaced layer: layers.24.self_attn.v_proj\n","Replaced layer: layers.24.self_attn.o_proj\n","Replaced layer: layers.24.mlp.gate_proj\n","Replaced layer: layers.24.mlp.up_proj\n","Replaced layer: layers.24.mlp.down_proj\n","Replaced layer: layers.25.self_attn.q_proj\n","Replaced layer: layers.25.self_attn.k_proj\n","Replaced layer: layers.25.self_attn.v_proj\n","Replaced layer: layers.25.self_attn.o_proj\n","Replaced layer: layers.25.mlp.gate_proj\n","Replaced layer: layers.25.mlp.up_proj\n","Replaced layer: layers.25.mlp.down_proj\n","Replaced layer: layers.26.self_attn.q_proj\n","Replaced layer: layers.26.self_attn.k_proj\n","Replaced layer: layers.26.self_attn.v_proj\n","Replaced layer: layers.26.self_attn.o_proj\n","Replaced layer: layers.26.mlp.gate_proj\n","Replaced layer: layers.26.mlp.up_proj\n","Replaced layer: layers.26.mlp.down_proj\n","Replaced layer: layers.27.self_attn.q_proj\n","Replaced layer: layers.27.self_attn.k_proj\n","Replaced layer: layers.27.self_attn.v_proj\n","Replaced layer: layers.27.self_attn.o_proj\n","Replaced layer: layers.27.mlp.gate_proj\n","Replaced layer: layers.27.mlp.up_proj\n","Replaced layer: layers.27.mlp.down_proj\n","Replaced layer: layers.28.self_attn.q_proj\n","Replaced layer: layers.28.self_attn.k_proj\n","Replaced layer: layers.28.self_attn.v_proj\n","Replaced layer: layers.28.self_attn.o_proj\n","Replaced layer: layers.28.mlp.gate_proj\n","Replaced layer: layers.28.mlp.up_proj\n","Replaced layer: layers.28.mlp.down_proj\n","Replaced layer: layers.29.self_attn.q_proj\n","Replaced layer: layers.29.self_attn.k_proj\n","Replaced layer: layers.29.self_attn.v_proj\n","Replaced layer: layers.29.self_attn.o_proj\n","Replaced layer: layers.29.mlp.gate_proj\n","Replaced layer: layers.29.mlp.up_proj\n","Replaced layer: layers.29.mlp.down_proj\n","Replaced layer: layers.30.self_attn.q_proj\n","Replaced layer: layers.30.self_attn.k_proj\n","Replaced layer: layers.30.self_attn.v_proj\n","Replaced layer: layers.30.self_attn.o_proj\n","Replaced layer: layers.30.mlp.gate_proj\n","Replaced layer: layers.30.mlp.up_proj\n","Replaced layer: layers.30.mlp.down_proj\n","Replaced layer: layers.31.self_attn.q_proj\n","Replaced layer: layers.31.self_attn.k_proj\n","Replaced layer: layers.31.self_attn.v_proj\n","Replaced layer: layers.31.self_attn.o_proj\n","Replaced layer: layers.31.mlp.gate_proj\n","Replaced layer: layers.31.mlp.up_proj\n","Replaced layer: layers.31.mlp.down_proj\n","Original model size (parameters): 7504924672\n","Compressed model size (parameters): 609718272\n","Parameter compression rate: 91.88%\n","Original model file size: 33658.16 MB\n","Compressed model file size: 2417.21 MB\n","File size compression rate: 92.82%\n","Models saved to directories: original_model and compressed_model\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'meta-llama/Meta-Llama-3-8B'  # Replace with your desired model name\n","COMPRESSION_RANK = 32  # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Save the compressed model to the directory\n","compressed_model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(compressed_model_dir)\n","model.save_pretrained(compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/azureuser/.cache/huggingface/token\n","Login successful\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8fb8397ee334d35b2d271fd1ce8a15b","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model pushed to Hugging Face at: https://huggingface.co/pavan01729/Compressed_LLama3_8b\n"]}],"source":["from huggingface_hub import HfApi, login\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Log in to Hugging Face\n","login(token=\"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\")\n","\n","# Define the model repository name\n","repo_name = \"pavan01729/Compressed_LLama3_8b\"  # Replace with your desired repo name\n","\n","# Push the model and tokenizer to the Hugging Face Hub\n","model.push_to_hub(repo_name, check_pr=True)\n","tokenizer.push_to_hub(repo_name, check_pr=True)\n","\n","print(f\"Model pushed to Hugging Face at: https://huggingface.co/{repo_name}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Inference and evaluate"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a70e8a9973d14fbdb165a87b3f4019e3","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[{'generated_text': 'Hey how are you doing today? I am doing well. I have been working on my blog and I think it is coming along nicely. What are'}]\n"]}],"source":["import transformers\n","import torch\n","\n","model_id = \"meta-llama/Meta-Llama-3-8B\"\n","\n","pipeline = transformers.pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n","# Generate text with a higher max_length\n","output = pipeline(\n","    \"Hey how are you doing today?\",\n","    max_length=30,  # Adjust this value as needed\n","    num_return_sequences=1,  # Number of output sequences\n","    no_repeat_ngram_size=2  # Prevents repeating n-grams\n",")\n","\n","print(output)\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Wrong index found for <pad>: should be None but found 32000.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m compressed_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompressed_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a pipeline for the compressed model\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m compressed_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Generate text using the compressed model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m compressed_pipeline(\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHey how are you doing today?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Adjust this value as needed\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n","Cell \u001b[0;32mIn[22], line 7\u001b[0m, in \u001b[0;36mcreate_pipeline\u001b[0;34m(model_dir, model_id, torch_dtype)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_pipeline\u001b[39m(model_dir, model_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m----> 7\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_dir, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype)\n\u001b[1;32m      9\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:702\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m         )\n\u001b[0;32m--> 702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1841\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1839\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/pavan/llm/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2053\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2049\u001b[0m current_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[1;32m   2050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_tokenizer_file \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m current_index \u001b[38;5;129;01mand\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m!=\u001b[39m index:\n\u001b[1;32m   2051\u001b[0m     \u001b[38;5;66;03m# Tokenizer fast: added token needs to either be in the vocabulary with the proper index or the\u001b[39;00m\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;66;03m# index is the current length of the tokenizer (not in vocabulary)\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2054\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong index found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2055\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2056\u001b[0m     )\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m!=\u001b[39m current_index:\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# Tokenizer slow: added token cannot already be in the vocabulary so its index needs to be the\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;66;03m# current length of the tokenizer.\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2061\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-consecutive added token \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2062\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but has index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in saved vocabulary.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2063\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Wrong index found for <pad>: should be None but found 32000."]}],"source":["import transformers\n","import torch\n","import os\n","\n","# Function to create a text generation pipeline with a specified model directory\n","def create_pipeline(model_dir, model_id, torch_dtype=torch.bfloat16):\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(model_dir)\n","    model = transformers.AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch_dtype)\n","    pipeline = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n","    return pipeline\n","\n","# Directories for the original and compressed models\n","original_model_dir = \"original_model\"\n","compressed_model_dir = \"compressed_model\"\n","\n","# Create a pipeline for the compressed model\n","compressed_pipeline = create_pipeline(compressed_model_dir, MODEL_NAME)\n","\n","# Generate text using the compressed model\n","output = compressed_pipeline(\n","    \"Hey how are you doing today?\",\n","    max_length=30,  # Adjust this value as needed\n","    num_return_sequences=1,  # Number of output sequences\n","    no_repeat_ngram_size=2,  # Prevents repeating n-grams\n","    temperature=0.7,\n","    top_k=50,\n","    top_p=0.9,\n","    do_sample=True\n",")\n","\n","# Print the output\n","print(output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## LLama2-7b trial"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/build/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27176acbf3d641af9761f3124182ef60","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/build/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dbb8e76fd8448ccb08764dbfcc40a62","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   1%|          | 73.4M/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"707c4c61585b4cea87d9e2785f1da229","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9b689c9ff7841a190e7d1c12958192e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: layers.0.self_attn.q_proj\n","Replaced layer: layers.0.self_attn.k_proj\n","Replaced layer: layers.0.self_attn.v_proj\n","Replaced layer: layers.0.self_attn.o_proj\n","Replaced layer: layers.0.mlp.gate_proj\n","Replaced layer: layers.0.mlp.up_proj\n","Replaced layer: layers.0.mlp.down_proj\n","Replaced layer: layers.1.self_attn.q_proj\n","Replaced layer: layers.1.self_attn.k_proj\n","Replaced layer: layers.1.self_attn.v_proj\n","Replaced layer: layers.1.self_attn.o_proj\n","Replaced layer: layers.1.mlp.gate_proj\n","Replaced layer: layers.1.mlp.up_proj\n","Replaced layer: layers.1.mlp.down_proj\n","Replaced layer: layers.2.self_attn.q_proj\n","Replaced layer: layers.2.self_attn.k_proj\n","Replaced layer: layers.2.self_attn.v_proj\n","Replaced layer: layers.2.self_attn.o_proj\n","Replaced layer: layers.2.mlp.gate_proj\n","Replaced layer: layers.2.mlp.up_proj\n","Replaced layer: layers.2.mlp.down_proj\n","Replaced layer: layers.3.self_attn.q_proj\n","Replaced layer: layers.3.self_attn.k_proj\n","Replaced layer: layers.3.self_attn.v_proj\n","Replaced layer: layers.3.self_attn.o_proj\n","Replaced layer: layers.3.mlp.gate_proj\n","Replaced layer: layers.3.mlp.up_proj\n","Replaced layer: layers.3.mlp.down_proj\n","Replaced layer: layers.4.self_attn.q_proj\n","Replaced layer: layers.4.self_attn.k_proj\n","Replaced layer: layers.4.self_attn.v_proj\n","Replaced layer: layers.4.self_attn.o_proj\n","Replaced layer: layers.4.mlp.gate_proj\n","Replaced layer: layers.4.mlp.up_proj\n","Replaced layer: layers.4.mlp.down_proj\n","Replaced layer: layers.5.self_attn.q_proj\n","Replaced layer: layers.5.self_attn.k_proj\n","Replaced layer: layers.5.self_attn.v_proj\n","Replaced layer: layers.5.self_attn.o_proj\n","Replaced layer: layers.5.mlp.gate_proj\n","Replaced layer: layers.5.mlp.up_proj\n","Replaced layer: layers.5.mlp.down_proj\n","Replaced layer: layers.6.self_attn.q_proj\n","Replaced layer: layers.6.self_attn.k_proj\n","Replaced layer: layers.6.self_attn.v_proj\n","Replaced layer: layers.6.self_attn.o_proj\n","Replaced layer: layers.6.mlp.gate_proj\n","Replaced layer: layers.6.mlp.up_proj\n","Replaced layer: layers.6.mlp.down_proj\n","Replaced layer: layers.7.self_attn.q_proj\n","Replaced layer: layers.7.self_attn.k_proj\n","Replaced layer: layers.7.self_attn.v_proj\n","Replaced layer: layers.7.self_attn.o_proj\n","Replaced layer: layers.7.mlp.gate_proj\n","Replaced layer: layers.7.mlp.up_proj\n","Replaced layer: layers.7.mlp.down_proj\n","Replaced layer: layers.8.self_attn.q_proj\n","Replaced layer: layers.8.self_attn.k_proj\n","Replaced layer: layers.8.self_attn.v_proj\n","Replaced layer: layers.8.self_attn.o_proj\n","Replaced layer: layers.8.mlp.gate_proj\n","Replaced layer: layers.8.mlp.up_proj\n","Replaced layer: layers.8.mlp.down_proj\n","Replaced layer: layers.9.self_attn.q_proj\n","Replaced layer: layers.9.self_attn.k_proj\n","Replaced layer: layers.9.self_attn.v_proj\n","Replaced layer: layers.9.self_attn.o_proj\n","Replaced layer: layers.9.mlp.gate_proj\n","Replaced layer: layers.9.mlp.up_proj\n","Replaced layer: layers.9.mlp.down_proj\n","Replaced layer: layers.10.self_attn.q_proj\n","Replaced layer: layers.10.self_attn.k_proj\n","Replaced layer: layers.10.self_attn.v_proj\n","Replaced layer: layers.10.self_attn.o_proj\n","Replaced layer: layers.10.mlp.gate_proj\n","Replaced layer: layers.10.mlp.up_proj\n","Replaced layer: layers.10.mlp.down_proj\n","Replaced layer: layers.11.self_attn.q_proj\n","Replaced layer: layers.11.self_attn.k_proj\n","Replaced layer: layers.11.self_attn.v_proj\n","Replaced layer: layers.11.self_attn.o_proj\n","Replaced layer: layers.11.mlp.gate_proj\n","Replaced layer: layers.11.mlp.up_proj\n","Replaced layer: layers.11.mlp.down_proj\n","Replaced layer: layers.12.self_attn.q_proj\n","Replaced layer: layers.12.self_attn.k_proj\n","Replaced layer: layers.12.self_attn.v_proj\n","Replaced layer: layers.12.self_attn.o_proj\n","Replaced layer: layers.12.mlp.gate_proj\n","Replaced layer: layers.12.mlp.up_proj\n","Replaced layer: layers.12.mlp.down_proj\n","Replaced layer: layers.13.self_attn.q_proj\n","Replaced layer: layers.13.self_attn.k_proj\n","Replaced layer: layers.13.self_attn.v_proj\n","Replaced layer: layers.13.self_attn.o_proj\n","Replaced layer: layers.13.mlp.gate_proj\n","Replaced layer: layers.13.mlp.up_proj\n","Replaced layer: layers.13.mlp.down_proj\n","Replaced layer: layers.14.self_attn.q_proj\n","Replaced layer: layers.14.self_attn.k_proj\n","Replaced layer: layers.14.self_attn.v_proj\n","Replaced layer: layers.14.self_attn.o_proj\n","Replaced layer: layers.14.mlp.gate_proj\n","Replaced layer: layers.14.mlp.up_proj\n","Replaced layer: layers.14.mlp.down_proj\n","Replaced layer: layers.15.self_attn.q_proj\n","Replaced layer: layers.15.self_attn.k_proj\n","Replaced layer: layers.15.self_attn.v_proj\n","Replaced layer: layers.15.self_attn.o_proj\n","Replaced layer: layers.15.mlp.gate_proj\n","Replaced layer: layers.15.mlp.up_proj\n","Replaced layer: layers.15.mlp.down_proj\n","Replaced layer: layers.16.self_attn.q_proj\n","Replaced layer: layers.16.self_attn.k_proj\n","Replaced layer: layers.16.self_attn.v_proj\n","Replaced layer: layers.16.self_attn.o_proj\n","Replaced layer: layers.16.mlp.gate_proj\n","Replaced layer: layers.16.mlp.up_proj\n","Replaced layer: layers.16.mlp.down_proj\n","Replaced layer: layers.17.self_attn.q_proj\n","Replaced layer: layers.17.self_attn.k_proj\n","Replaced layer: layers.17.self_attn.v_proj\n","Replaced layer: layers.17.self_attn.o_proj\n","Replaced layer: layers.17.mlp.gate_proj\n","Replaced layer: layers.17.mlp.up_proj\n","Replaced layer: layers.17.mlp.down_proj\n","Replaced layer: layers.18.self_attn.q_proj\n","Replaced layer: layers.18.self_attn.k_proj\n","Replaced layer: layers.18.self_attn.v_proj\n","Replaced layer: layers.18.self_attn.o_proj\n","Replaced layer: layers.18.mlp.gate_proj\n","Replaced layer: layers.18.mlp.up_proj\n","Replaced layer: layers.18.mlp.down_proj\n","Replaced layer: layers.19.self_attn.q_proj\n","Replaced layer: layers.19.self_attn.k_proj\n","Replaced layer: layers.19.self_attn.v_proj\n","Replaced layer: layers.19.self_attn.o_proj\n","Replaced layer: layers.19.mlp.gate_proj\n","Replaced layer: layers.19.mlp.up_proj\n","Replaced layer: layers.19.mlp.down_proj\n","Replaced layer: layers.20.self_attn.q_proj\n","Replaced layer: layers.20.self_attn.k_proj\n","Replaced layer: layers.20.self_attn.v_proj\n","Replaced layer: layers.20.self_attn.o_proj\n","Replaced layer: layers.20.mlp.gate_proj\n","Replaced layer: layers.20.mlp.up_proj\n","Replaced layer: layers.20.mlp.down_proj\n","Replaced layer: layers.21.self_attn.q_proj\n","Replaced layer: layers.21.self_attn.k_proj\n","Replaced layer: layers.21.self_attn.v_proj\n","Replaced layer: layers.21.self_attn.o_proj\n","Replaced layer: layers.21.mlp.gate_proj\n","Replaced layer: layers.21.mlp.up_proj\n","Replaced layer: layers.21.mlp.down_proj\n","Replaced layer: layers.22.self_attn.q_proj\n","Replaced layer: layers.22.self_attn.k_proj\n","Replaced layer: layers.22.self_attn.v_proj\n","Replaced layer: layers.22.self_attn.o_proj\n","Replaced layer: layers.22.mlp.gate_proj\n","Replaced layer: layers.22.mlp.up_proj\n","Replaced layer: layers.22.mlp.down_proj\n","Replaced layer: layers.23.self_attn.q_proj\n","Replaced layer: layers.23.self_attn.k_proj\n","Replaced layer: layers.23.self_attn.v_proj\n","Replaced layer: layers.23.self_attn.o_proj\n","Replaced layer: layers.23.mlp.gate_proj\n","Replaced layer: layers.23.mlp.up_proj\n","Replaced layer: layers.23.mlp.down_proj\n","Replaced layer: layers.24.self_attn.q_proj\n","Replaced layer: layers.24.self_attn.k_proj\n","Replaced layer: layers.24.self_attn.v_proj\n","Replaced layer: layers.24.self_attn.o_proj\n","Replaced layer: layers.24.mlp.gate_proj\n","Replaced layer: layers.24.mlp.up_proj\n","Replaced layer: layers.24.mlp.down_proj\n","Replaced layer: layers.25.self_attn.q_proj\n","Replaced layer: layers.25.self_attn.k_proj\n","Replaced layer: layers.25.self_attn.v_proj\n","Replaced layer: layers.25.self_attn.o_proj\n","Replaced layer: layers.25.mlp.gate_proj\n","Replaced layer: layers.25.mlp.up_proj\n","Replaced layer: layers.25.mlp.down_proj\n","Replaced layer: layers.26.self_attn.q_proj\n","Replaced layer: layers.26.self_attn.k_proj\n","Replaced layer: layers.26.self_attn.v_proj\n","Replaced layer: layers.26.self_attn.o_proj\n","Replaced layer: layers.26.mlp.gate_proj\n","Replaced layer: layers.26.mlp.up_proj\n","Replaced layer: layers.26.mlp.down_proj\n","Replaced layer: layers.27.self_attn.q_proj\n","Replaced layer: layers.27.self_attn.k_proj\n","Replaced layer: layers.27.self_attn.v_proj\n","Replaced layer: layers.27.self_attn.o_proj\n","Replaced layer: layers.27.mlp.gate_proj\n","Replaced layer: layers.27.mlp.up_proj\n","Replaced layer: layers.27.mlp.down_proj\n","Replaced layer: layers.28.self_attn.q_proj\n","Replaced layer: layers.28.self_attn.k_proj\n","Replaced layer: layers.28.self_attn.v_proj\n","Replaced layer: layers.28.self_attn.o_proj\n","Replaced layer: layers.28.mlp.gate_proj\n","Replaced layer: layers.28.mlp.up_proj\n","Replaced layer: layers.28.mlp.down_proj\n","Replaced layer: layers.29.self_attn.q_proj\n","Replaced layer: layers.29.self_attn.k_proj\n","Replaced layer: layers.29.self_attn.v_proj\n","Replaced layer: layers.29.self_attn.o_proj\n","Replaced layer: layers.29.mlp.gate_proj\n","Replaced layer: layers.29.mlp.up_proj\n","Replaced layer: layers.29.mlp.down_proj\n","Replaced layer: layers.30.self_attn.q_proj\n","Replaced layer: layers.30.self_attn.k_proj\n","Replaced layer: layers.30.self_attn.v_proj\n","Replaced layer: layers.30.self_attn.o_proj\n","Replaced layer: layers.30.mlp.gate_proj\n","Replaced layer: layers.30.mlp.up_proj\n","Replaced layer: layers.30.mlp.down_proj\n","Replaced layer: layers.31.self_attn.q_proj\n","Replaced layer: layers.31.self_attn.k_proj\n","Replaced layer: layers.31.self_attn.v_proj\n","Replaced layer: layers.31.self_attn.o_proj\n","Replaced layer: layers.31.mlp.gate_proj\n","Replaced layer: layers.31.mlp.up_proj\n","Replaced layer: layers.31.mlp.down_proj\n","Original model size (parameters): 6607343616\n","Compressed model size (parameters): 454823936\n","Parameter compression rate: 93.12%\n","Original model file size: 25207.39 MB\n","Compressed model file size: 1819.39 MB\n","File size compression rate: 92.78%\n","Models saved to directories: original_model and compressed_model\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'  # Replace with your desired model name\n","COMPRESSION_RANK = 128  # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Save the compressed model to the directory\n","compressed_model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(compressed_model_dir)\n","model.save_pretrained(compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/azureuser/.cache/huggingface/token\n","Login successful\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12ff18a85a204c2ba6dd25aeb7b7747c","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51ca4023ce5f475c9b733daea4fcc291","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Model pushed to Hugging Face at: https://huggingface.co/pavan01729/Compressed_LLama2_7b\n"]}],"source":["from huggingface_hub import HfApi, login\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# Log in to Hugging Face\n","login(token=\"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\")\n","\n","# Define the model repository name\n","repo_name = \"pavan01729/Compressed_LLama2_7b\"  # Replace with your desired repo name\n","\n","# Push the model and tokenizer to the Hugging Face Hub\n","model.push_to_hub(repo_name, check_pr=True)\n","tokenizer.push_to_hub(repo_name, check_pr=True)\n","\n","print(f\"Model pushed to Hugging Face at: https://huggingface.co/{repo_name}\")\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["heelo\n"]}],"source":["print('heelo')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## with compression % 50%"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14cc6464a4704c1f9aa04e4a44237fda","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"084a0648c71d447d9e0efb627c635c75","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e10bd6b7b8484990aa0a7a79b4ae50ef","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4321d1f3b2d74e55b0106074982de12e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba1fdc5b59bd45f39026a8ec282ebb83","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d700f81c86b0446ca9bc516f69d2b264","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f97b0ef38a0a431eb93ffd9bcfbb4705","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a70398148be409a81013f4a7e3d59c1","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5fdba197dcc4084896a00215bf7f2c0","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bcb94fc5c594ebf8f02ffd046556fcc","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: layers.0.self_attn.q_proj with rank 2048\n","Replaced layer: layers.0.self_attn.k_proj with rank 2048\n","Replaced layer: layers.0.self_attn.v_proj with rank 2048\n","Replaced layer: layers.0.self_attn.o_proj with rank 2048\n","Replaced layer: layers.0.mlp.gate_proj with rank 2048\n","Replaced layer: layers.0.mlp.up_proj with rank 2048\n","Replaced layer: layers.0.mlp.down_proj with rank 5504\n","Replaced layer: layers.1.self_attn.q_proj with rank 2048\n","Replaced layer: layers.1.self_attn.k_proj with rank 2048\n","Replaced layer: layers.1.self_attn.v_proj with rank 2048\n","Replaced layer: layers.1.self_attn.o_proj with rank 2048\n","Replaced layer: layers.1.mlp.gate_proj with rank 2048\n","Replaced layer: layers.1.mlp.up_proj with rank 2048\n","Replaced layer: layers.1.mlp.down_proj with rank 5504\n","Replaced layer: layers.2.self_attn.q_proj with rank 2048\n","Replaced layer: layers.2.self_attn.k_proj with rank 2048\n","Replaced layer: layers.2.self_attn.v_proj with rank 2048\n","Replaced layer: layers.2.self_attn.o_proj with rank 2048\n","Replaced layer: layers.2.mlp.gate_proj with rank 2048\n","Replaced layer: layers.2.mlp.up_proj with rank 2048\n","Replaced layer: layers.2.mlp.down_proj with rank 5504\n","Replaced layer: layers.3.self_attn.q_proj with rank 2048\n","Replaced layer: layers.3.self_attn.k_proj with rank 2048\n","Replaced layer: layers.3.self_attn.v_proj with rank 2048\n","Replaced layer: layers.3.self_attn.o_proj with rank 2048\n","Replaced layer: layers.3.mlp.gate_proj with rank 2048\n","Replaced layer: layers.3.mlp.up_proj with rank 2048\n","Replaced layer: layers.3.mlp.down_proj with rank 5504\n","Replaced layer: layers.4.self_attn.q_proj with rank 2048\n","Replaced layer: layers.4.self_attn.k_proj with rank 2048\n","Replaced layer: layers.4.self_attn.v_proj with rank 2048\n","Replaced layer: layers.4.self_attn.o_proj with rank 2048\n","Replaced layer: layers.4.mlp.gate_proj with rank 2048\n","Replaced layer: layers.4.mlp.up_proj with rank 2048\n","Replaced layer: layers.4.mlp.down_proj with rank 5504\n","Replaced layer: layers.5.self_attn.q_proj with rank 2048\n","Replaced layer: layers.5.self_attn.k_proj with rank 2048\n","Replaced layer: layers.5.self_attn.v_proj with rank 2048\n","Replaced layer: layers.5.self_attn.o_proj with rank 2048\n","Replaced layer: layers.5.mlp.gate_proj with rank 2048\n","Replaced layer: layers.5.mlp.up_proj with rank 2048\n","Replaced layer: layers.5.mlp.down_proj with rank 5504\n","Replaced layer: layers.6.self_attn.q_proj with rank 2048\n","Replaced layer: layers.6.self_attn.k_proj with rank 2048\n","Replaced layer: layers.6.self_attn.v_proj with rank 2048\n","Replaced layer: layers.6.self_attn.o_proj with rank 2048\n","Replaced layer: layers.6.mlp.gate_proj with rank 2048\n","Replaced layer: layers.6.mlp.up_proj with rank 2048\n","Replaced layer: layers.6.mlp.down_proj with rank 5504\n","Replaced layer: layers.7.self_attn.q_proj with rank 2048\n","Replaced layer: layers.7.self_attn.k_proj with rank 2048\n","Replaced layer: layers.7.self_attn.v_proj with rank 2048\n","Replaced layer: layers.7.self_attn.o_proj with rank 2048\n","Replaced layer: layers.7.mlp.gate_proj with rank 2048\n","Replaced layer: layers.7.mlp.up_proj with rank 2048\n","Replaced layer: layers.7.mlp.down_proj with rank 5504\n","Replaced layer: layers.8.self_attn.q_proj with rank 2048\n","Replaced layer: layers.8.self_attn.k_proj with rank 2048\n","Replaced layer: layers.8.self_attn.v_proj with rank 2048\n","Replaced layer: layers.8.self_attn.o_proj with rank 2048\n","Replaced layer: layers.8.mlp.gate_proj with rank 2048\n","Replaced layer: layers.8.mlp.up_proj with rank 2048\n","Replaced layer: layers.8.mlp.down_proj with rank 5504\n","Replaced layer: layers.9.self_attn.q_proj with rank 2048\n","Replaced layer: layers.9.self_attn.k_proj with rank 2048\n","Replaced layer: layers.9.self_attn.v_proj with rank 2048\n","Replaced layer: layers.9.self_attn.o_proj with rank 2048\n","Replaced layer: layers.9.mlp.gate_proj with rank 2048\n","Replaced layer: layers.9.mlp.up_proj with rank 2048\n","Replaced layer: layers.9.mlp.down_proj with rank 5504\n","Replaced layer: layers.10.self_attn.q_proj with rank 2048\n","Replaced layer: layers.10.self_attn.k_proj with rank 2048\n","Replaced layer: layers.10.self_attn.v_proj with rank 2048\n","Replaced layer: layers.10.self_attn.o_proj with rank 2048\n","Replaced layer: layers.10.mlp.gate_proj with rank 2048\n","Replaced layer: layers.10.mlp.up_proj with rank 2048\n","Replaced layer: layers.10.mlp.down_proj with rank 5504\n","Replaced layer: layers.11.self_attn.q_proj with rank 2048\n","Replaced layer: layers.11.self_attn.k_proj with rank 2048\n","Replaced layer: layers.11.self_attn.v_proj with rank 2048\n","Replaced layer: layers.11.self_attn.o_proj with rank 2048\n","Replaced layer: layers.11.mlp.gate_proj with rank 2048\n","Replaced layer: layers.11.mlp.up_proj with rank 2048\n","Replaced layer: layers.11.mlp.down_proj with rank 5504\n","Replaced layer: layers.12.self_attn.q_proj with rank 2048\n","Replaced layer: layers.12.self_attn.k_proj with rank 2048\n","Replaced layer: layers.12.self_attn.v_proj with rank 2048\n","Replaced layer: layers.12.self_attn.o_proj with rank 2048\n","Replaced layer: layers.12.mlp.gate_proj with rank 2048\n","Replaced layer: layers.12.mlp.up_proj with rank 2048\n","Replaced layer: layers.12.mlp.down_proj with rank 5504\n","Replaced layer: layers.13.self_attn.q_proj with rank 2048\n","Replaced layer: layers.13.self_attn.k_proj with rank 2048\n","Replaced layer: layers.13.self_attn.v_proj with rank 2048\n","Replaced layer: layers.13.self_attn.o_proj with rank 2048\n","Replaced layer: layers.13.mlp.gate_proj with rank 2048\n","Replaced layer: layers.13.mlp.up_proj with rank 2048\n","Replaced layer: layers.13.mlp.down_proj with rank 5504\n","Replaced layer: layers.14.self_attn.q_proj with rank 2048\n","Replaced layer: layers.14.self_attn.k_proj with rank 2048\n","Replaced layer: layers.14.self_attn.v_proj with rank 2048\n","Replaced layer: layers.14.self_attn.o_proj with rank 2048\n","Replaced layer: layers.14.mlp.gate_proj with rank 2048\n","Replaced layer: layers.14.mlp.up_proj with rank 2048\n","Replaced layer: layers.14.mlp.down_proj with rank 5504\n","Replaced layer: layers.15.self_attn.q_proj with rank 2048\n","Replaced layer: layers.15.self_attn.k_proj with rank 2048\n","Replaced layer: layers.15.self_attn.v_proj with rank 2048\n","Replaced layer: layers.15.self_attn.o_proj with rank 2048\n","Replaced layer: layers.15.mlp.gate_proj with rank 2048\n","Replaced layer: layers.15.mlp.up_proj with rank 2048\n","Replaced layer: layers.15.mlp.down_proj with rank 5504\n","Replaced layer: layers.16.self_attn.q_proj with rank 2048\n","Replaced layer: layers.16.self_attn.k_proj with rank 2048\n","Replaced layer: layers.16.self_attn.v_proj with rank 2048\n","Replaced layer: layers.16.self_attn.o_proj with rank 2048\n","Replaced layer: layers.16.mlp.gate_proj with rank 2048\n","Replaced layer: layers.16.mlp.up_proj with rank 2048\n","Replaced layer: layers.16.mlp.down_proj with rank 5504\n","Replaced layer: layers.17.self_attn.q_proj with rank 2048\n","Replaced layer: layers.17.self_attn.k_proj with rank 2048\n","Replaced layer: layers.17.self_attn.v_proj with rank 2048\n","Replaced layer: layers.17.self_attn.o_proj with rank 2048\n","Replaced layer: layers.17.mlp.gate_proj with rank 2048\n","Replaced layer: layers.17.mlp.up_proj with rank 2048\n","Replaced layer: layers.17.mlp.down_proj with rank 5504\n","Replaced layer: layers.18.self_attn.q_proj with rank 2048\n","Replaced layer: layers.18.self_attn.k_proj with rank 2048\n","Replaced layer: layers.18.self_attn.v_proj with rank 2048\n","Replaced layer: layers.18.self_attn.o_proj with rank 2048\n","Replaced layer: layers.18.mlp.gate_proj with rank 2048\n","Replaced layer: layers.18.mlp.up_proj with rank 2048\n","Replaced layer: layers.18.mlp.down_proj with rank 5504\n","Replaced layer: layers.19.self_attn.q_proj with rank 2048\n","Replaced layer: layers.19.self_attn.k_proj with rank 2048\n","Replaced layer: layers.19.self_attn.v_proj with rank 2048\n","Replaced layer: layers.19.self_attn.o_proj with rank 2048\n","Replaced layer: layers.19.mlp.gate_proj with rank 2048\n","Replaced layer: layers.19.mlp.up_proj with rank 2048\n","Replaced layer: layers.19.mlp.down_proj with rank 5504\n","Replaced layer: layers.20.self_attn.q_proj with rank 2048\n","Replaced layer: layers.20.self_attn.k_proj with rank 2048\n","Replaced layer: layers.20.self_attn.v_proj with rank 2048\n","Replaced layer: layers.20.self_attn.o_proj with rank 2048\n","Replaced layer: layers.20.mlp.gate_proj with rank 2048\n","Replaced layer: layers.20.mlp.up_proj with rank 2048\n","Replaced layer: layers.20.mlp.down_proj with rank 5504\n","Replaced layer: layers.21.self_attn.q_proj with rank 2048\n","Replaced layer: layers.21.self_attn.k_proj with rank 2048\n","Replaced layer: layers.21.self_attn.v_proj with rank 2048\n","Replaced layer: layers.21.self_attn.o_proj with rank 2048\n","Replaced layer: layers.21.mlp.gate_proj with rank 2048\n","Replaced layer: layers.21.mlp.up_proj with rank 2048\n","Replaced layer: layers.21.mlp.down_proj with rank 5504\n","Replaced layer: layers.22.self_attn.q_proj with rank 2048\n","Replaced layer: layers.22.self_attn.k_proj with rank 2048\n","Replaced layer: layers.22.self_attn.v_proj with rank 2048\n","Replaced layer: layers.22.self_attn.o_proj with rank 2048\n","Replaced layer: layers.22.mlp.gate_proj with rank 2048\n","Replaced layer: layers.22.mlp.up_proj with rank 2048\n","Replaced layer: layers.22.mlp.down_proj with rank 5504\n","Replaced layer: layers.23.self_attn.q_proj with rank 2048\n","Replaced layer: layers.23.self_attn.k_proj with rank 2048\n","Replaced layer: layers.23.self_attn.v_proj with rank 2048\n","Replaced layer: layers.23.self_attn.o_proj with rank 2048\n","Replaced layer: layers.23.mlp.gate_proj with rank 2048\n","Replaced layer: layers.23.mlp.up_proj with rank 2048\n","Replaced layer: layers.23.mlp.down_proj with rank 5504\n","Replaced layer: layers.24.self_attn.q_proj with rank 2048\n","Replaced layer: layers.24.self_attn.k_proj with rank 2048\n","Replaced layer: layers.24.self_attn.v_proj with rank 2048\n","Replaced layer: layers.24.self_attn.o_proj with rank 2048\n","Replaced layer: layers.24.mlp.gate_proj with rank 2048\n","Replaced layer: layers.24.mlp.up_proj with rank 2048\n","Replaced layer: layers.24.mlp.down_proj with rank 5504\n","Replaced layer: layers.25.self_attn.q_proj with rank 2048\n","Replaced layer: layers.25.self_attn.k_proj with rank 2048\n","Replaced layer: layers.25.self_attn.v_proj with rank 2048\n","Replaced layer: layers.25.self_attn.o_proj with rank 2048\n","Replaced layer: layers.25.mlp.gate_proj with rank 2048\n","Replaced layer: layers.25.mlp.up_proj with rank 2048\n","Replaced layer: layers.25.mlp.down_proj with rank 5504\n","Replaced layer: layers.26.self_attn.q_proj with rank 2048\n","Replaced layer: layers.26.self_attn.k_proj with rank 2048\n","Replaced layer: layers.26.self_attn.v_proj with rank 2048\n","Replaced layer: layers.26.self_attn.o_proj with rank 2048\n","Replaced layer: layers.26.mlp.gate_proj with rank 2048\n","Replaced layer: layers.26.mlp.up_proj with rank 2048\n","Replaced layer: layers.26.mlp.down_proj with rank 5504\n","Replaced layer: layers.27.self_attn.q_proj with rank 2048\n","Replaced layer: layers.27.self_attn.k_proj with rank 2048\n","Replaced layer: layers.27.self_attn.v_proj with rank 2048\n","Replaced layer: layers.27.self_attn.o_proj with rank 2048\n","Replaced layer: layers.27.mlp.gate_proj with rank 2048\n","Replaced layer: layers.27.mlp.up_proj with rank 2048\n","Replaced layer: layers.27.mlp.down_proj with rank 5504\n","Replaced layer: layers.28.self_attn.q_proj with rank 2048\n","Replaced layer: layers.28.self_attn.k_proj with rank 2048\n","Replaced layer: layers.28.self_attn.v_proj with rank 2048\n","Replaced layer: layers.28.self_attn.o_proj with rank 2048\n","Replaced layer: layers.28.mlp.gate_proj with rank 2048\n","Replaced layer: layers.28.mlp.up_proj with rank 2048\n","Replaced layer: layers.28.mlp.down_proj with rank 5504\n","Replaced layer: layers.29.self_attn.q_proj with rank 2048\n","Replaced layer: layers.29.self_attn.k_proj with rank 2048\n","Replaced layer: layers.29.self_attn.v_proj with rank 2048\n","Replaced layer: layers.29.self_attn.o_proj with rank 2048\n","Replaced layer: layers.29.mlp.gate_proj with rank 2048\n","Replaced layer: layers.29.mlp.up_proj with rank 2048\n","Replaced layer: layers.29.mlp.down_proj with rank 5504\n","Replaced layer: layers.30.self_attn.q_proj with rank 2048\n","Replaced layer: layers.30.self_attn.k_proj with rank 2048\n","Replaced layer: layers.30.self_attn.v_proj with rank 2048\n","Replaced layer: layers.30.self_attn.o_proj with rank 2048\n","Replaced layer: layers.30.mlp.gate_proj with rank 2048\n","Replaced layer: layers.30.mlp.up_proj with rank 2048\n","Replaced layer: layers.30.mlp.down_proj with rank 5504\n","Replaced layer: layers.31.self_attn.q_proj with rank 2048\n","Replaced layer: layers.31.self_attn.k_proj with rank 2048\n","Replaced layer: layers.31.self_attn.v_proj with rank 2048\n","Replaced layer: layers.31.self_attn.o_proj with rank 2048\n","Replaced layer: layers.31.mlp.gate_proj with rank 2048\n","Replaced layer: layers.31.mlp.up_proj with rank 2048\n","Replaced layer: layers.31.mlp.down_proj with rank 5504\n","Attempt 1 to save model failed: [enforce fail at inline_container.cc:595] . unexpected pos 6592382464 vs 6592382360\n","Attempt 2 to save model failed: [enforce fail at inline_container.cc:595] . unexpected pos 9905334272 vs 9905334168\n","Attempt 3 to save model failed: [enforce fail at inline_container.cc:595] . unexpected pos 9905334272 vs 9905334168\n"]},{"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:595] . unexpected pos 9905334272 vs 9905334168","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/torch/serialization.py:628\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 628\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/torch/serialization.py:862\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    861\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 862\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:764] . PytorchStreamWriter failed writing file data/99: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m attempt \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m retries:\n\u001b[1;32m     96\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m \u001b[43msave_model_with_retries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressed_model_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Get the file sizes of the original and compressed models\u001b[39;00m\n\u001b[1;32m    101\u001b[0m original_file_size \u001b[38;5;241m=\u001b[39m get_dir_size(original_model_dir)\n","Cell \u001b[0;32mIn[4], line 91\u001b[0m, in \u001b[0;36msave_model_with_retries\u001b[0;34m(model, directory, retries)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/transformers/modeling_utils.py:1847\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, **kwargs)\u001b[0m\n\u001b[1;32m   1845\u001b[0m         safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   1846\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1847\u001b[0m         \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1850\u001b[0m     path_to_weights \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/torch/serialization.py:627\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    624\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    628\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    629\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/torch/serialization.py:475\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:595] . unexpected pos 9905334272 vs 9905334168"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and desired compression percentage\n","MODEL_NAME = 'meta-llama/Llama-2-7b-chat-hf'  # Replace with your desired model name\n","DESIRED_COMPRESSION_PERCENTAGE = 50  # Desired compression percentage (0-100)\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, desired_compression_percentage):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            original_rank = module.weight.size(1)\n","            rank = max(1, int(original_rank * (1 - desired_compression_percentage / 100)))\n","\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name} with rank {rank}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","os.makedirs(original_model_dir, exist_ok=True)\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, DESIRED_COMPRESSION_PERCENTAGE)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Ensure the directory exists before saving the compressed model\n","compressed_model_dir = \"compressed_model\"\n","os.makedirs(compressed_model_dir, exist_ok=True)\n","\n","# Save the compressed model to the directory with a retry mechanism\n","def save_model_with_retries(model, directory, retries=3):\n","    for attempt in range(retries):\n","        try:\n","            model.save_pretrained(directory)\n","            return\n","        except RuntimeError as e:\n","            print(f\"Attempt {attempt + 1} to save model failed: {e}\")\n","            if attempt + 1 == retries:\n","                raise\n","\n","save_model_with_retries(model, compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["##  Mistral 7b trial"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'mistralai/Mistral-7B-v0.1'  # Replace with your desired model name\n","COMPRESSION_RANK = 128  # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Save the compressed model to the directory\n","compressed_model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(compressed_model_dir)\n","model.save_pretrained(compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/azureuser/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import HfApi, login\n","\n","# Log in to Hugging Face\n","token = \"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\"  # Replace with your Hugging Face token\n","login(token)\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d3e9207ca9e469e9df71158aa1a8907","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f874329ed784ab88c3174059191378b","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:  74%|#######3  | 3.64G/4.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a559f302b2bb418e96b1417f42cf4892","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n","Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n","`config.hidden_activation` if you want to override this behaviour.\n","See https://github.com/huggingface/transformers/pull/29402 for more details.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5614f5d7fb8b46a8afbce362d301c8dc","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Replaced layer: layers.0.self_attn.q_proj\n","Replaced layer: layers.0.self_attn.k_proj\n","Replaced layer: layers.0.self_attn.v_proj\n","Replaced layer: layers.0.self_attn.o_proj\n","Replaced layer: layers.0.mlp.gate_proj\n","Replaced layer: layers.0.mlp.up_proj\n","Replaced layer: layers.0.mlp.down_proj\n","Replaced layer: layers.1.self_attn.q_proj\n","Replaced layer: layers.1.self_attn.k_proj\n","Replaced layer: layers.1.self_attn.v_proj\n","Replaced layer: layers.1.self_attn.o_proj\n","Replaced layer: layers.1.mlp.gate_proj\n","Replaced layer: layers.1.mlp.up_proj\n","Replaced layer: layers.1.mlp.down_proj\n","Replaced layer: layers.2.self_attn.q_proj\n","Replaced layer: layers.2.self_attn.k_proj\n","Replaced layer: layers.2.self_attn.v_proj\n","Replaced layer: layers.2.self_attn.o_proj\n","Replaced layer: layers.2.mlp.gate_proj\n","Replaced layer: layers.2.mlp.up_proj\n","Replaced layer: layers.2.mlp.down_proj\n","Replaced layer: layers.3.self_attn.q_proj\n","Replaced layer: layers.3.self_attn.k_proj\n","Replaced layer: layers.3.self_attn.v_proj\n","Replaced layer: layers.3.self_attn.o_proj\n","Replaced layer: layers.3.mlp.gate_proj\n","Replaced layer: layers.3.mlp.up_proj\n","Replaced layer: layers.3.mlp.down_proj\n","Replaced layer: layers.4.self_attn.q_proj\n","Replaced layer: layers.4.self_attn.k_proj\n","Replaced layer: layers.4.self_attn.v_proj\n","Replaced layer: layers.4.self_attn.o_proj\n","Replaced layer: layers.4.mlp.gate_proj\n","Replaced layer: layers.4.mlp.up_proj\n","Replaced layer: layers.4.mlp.down_proj\n","Replaced layer: layers.5.self_attn.q_proj\n","Replaced layer: layers.5.self_attn.k_proj\n","Replaced layer: layers.5.self_attn.v_proj\n","Replaced layer: layers.5.self_attn.o_proj\n","Replaced layer: layers.5.mlp.gate_proj\n","Replaced layer: layers.5.mlp.up_proj\n","Replaced layer: layers.5.mlp.down_proj\n","Replaced layer: layers.6.self_attn.q_proj\n","Replaced layer: layers.6.self_attn.k_proj\n","Replaced layer: layers.6.self_attn.v_proj\n","Replaced layer: layers.6.self_attn.o_proj\n","Replaced layer: layers.6.mlp.gate_proj\n","Replaced layer: layers.6.mlp.up_proj\n","Replaced layer: layers.6.mlp.down_proj\n","Replaced layer: layers.7.self_attn.q_proj\n","Replaced layer: layers.7.self_attn.k_proj\n","Replaced layer: layers.7.self_attn.v_proj\n","Replaced layer: layers.7.self_attn.o_proj\n","Replaced layer: layers.7.mlp.gate_proj\n","Replaced layer: layers.7.mlp.up_proj\n","Replaced layer: layers.7.mlp.down_proj\n","Replaced layer: layers.8.self_attn.q_proj\n","Replaced layer: layers.8.self_attn.k_proj\n","Replaced layer: layers.8.self_attn.v_proj\n","Replaced layer: layers.8.self_attn.o_proj\n","Replaced layer: layers.8.mlp.gate_proj\n","Replaced layer: layers.8.mlp.up_proj\n","Replaced layer: layers.8.mlp.down_proj\n","Replaced layer: layers.9.self_attn.q_proj\n","Replaced layer: layers.9.self_attn.k_proj\n","Replaced layer: layers.9.self_attn.v_proj\n","Replaced layer: layers.9.self_attn.o_proj\n","Replaced layer: layers.9.mlp.gate_proj\n","Replaced layer: layers.9.mlp.up_proj\n","Replaced layer: layers.9.mlp.down_proj\n","Replaced layer: layers.10.self_attn.q_proj\n","Replaced layer: layers.10.self_attn.k_proj\n","Replaced layer: layers.10.self_attn.v_proj\n","Replaced layer: layers.10.self_attn.o_proj\n","Replaced layer: layers.10.mlp.gate_proj\n","Replaced layer: layers.10.mlp.up_proj\n","Replaced layer: layers.10.mlp.down_proj\n","Replaced layer: layers.11.self_attn.q_proj\n","Replaced layer: layers.11.self_attn.k_proj\n","Replaced layer: layers.11.self_attn.v_proj\n","Replaced layer: layers.11.self_attn.o_proj\n","Replaced layer: layers.11.mlp.gate_proj\n","Replaced layer: layers.11.mlp.up_proj\n","Replaced layer: layers.11.mlp.down_proj\n","Replaced layer: layers.12.self_attn.q_proj\n","Replaced layer: layers.12.self_attn.k_proj\n","Replaced layer: layers.12.self_attn.v_proj\n","Replaced layer: layers.12.self_attn.o_proj\n","Replaced layer: layers.12.mlp.gate_proj\n","Replaced layer: layers.12.mlp.up_proj\n","Replaced layer: layers.12.mlp.down_proj\n","Replaced layer: layers.13.self_attn.q_proj\n","Replaced layer: layers.13.self_attn.k_proj\n","Replaced layer: layers.13.self_attn.v_proj\n","Replaced layer: layers.13.self_attn.o_proj\n","Replaced layer: layers.13.mlp.gate_proj\n","Replaced layer: layers.13.mlp.up_proj\n","Replaced layer: layers.13.mlp.down_proj\n","Replaced layer: layers.14.self_attn.q_proj\n","Replaced layer: layers.14.self_attn.k_proj\n","Replaced layer: layers.14.self_attn.v_proj\n","Replaced layer: layers.14.self_attn.o_proj\n","Replaced layer: layers.14.mlp.gate_proj\n","Replaced layer: layers.14.mlp.up_proj\n","Replaced layer: layers.14.mlp.down_proj\n","Replaced layer: layers.15.self_attn.q_proj\n","Replaced layer: layers.15.self_attn.k_proj\n","Replaced layer: layers.15.self_attn.v_proj\n","Replaced layer: layers.15.self_attn.o_proj\n","Replaced layer: layers.15.mlp.gate_proj\n","Replaced layer: layers.15.mlp.up_proj\n","Replaced layer: layers.15.mlp.down_proj\n","Replaced layer: layers.16.self_attn.q_proj\n","Replaced layer: layers.16.self_attn.k_proj\n","Replaced layer: layers.16.self_attn.v_proj\n","Replaced layer: layers.16.self_attn.o_proj\n","Replaced layer: layers.16.mlp.gate_proj\n","Replaced layer: layers.16.mlp.up_proj\n","Replaced layer: layers.16.mlp.down_proj\n","Replaced layer: layers.17.self_attn.q_proj\n","Replaced layer: layers.17.self_attn.k_proj\n","Replaced layer: layers.17.self_attn.v_proj\n","Replaced layer: layers.17.self_attn.o_proj\n","Replaced layer: layers.17.mlp.gate_proj\n","Replaced layer: layers.17.mlp.up_proj\n","Replaced layer: layers.17.mlp.down_proj\n","Original model size (parameters): 2506172416\n","Compressed model size (parameters): 563716096\n","Parameter compression rate: 77.51%\n","Original model file size: 9581.11 MB\n","Compressed model file size: 2171.24 MB\n","File size compression rate: 77.34%\n","Models saved to directories: original_model and compressed_model\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.nn import functional as F\n","\n","# Specify the model name and compression rank\n","MODEL_NAME = 'google/gemma-2b'  # Replace with your desired model name\n","COMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n","\n","# Define LowRankLayer class for low-rank decomposition\n","class LowRankLayer(nn.Module):\n","    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n","    def __init__(self, rank, full_rank_layer):\n","        super().__init__()\n","        self.rank = rank\n","\n","        # Perform SVD on the full-rank layer's weight matrix\n","        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n","        S_diag = torch.diag(S)\n","        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n","        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n","        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n","\n","        # Handle the bias term if it exists\n","        if full_rank_layer.bias is not None:\n","            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n","        else:\n","            self.bias = None\n","\n","    def forward(self, x):\n","        aprox_weight_matrix = self.U @ self.S @ self.Vh\n","        output = F.linear(x, aprox_weight_matrix, self.bias)\n","        return output\n","\n","# Function to replace linear layers with LowRankLayer\n","def replace_with_low_rank(model, rank):\n","    for name, module in model.named_modules():\n","        if isinstance(module, nn.Linear):\n","            # Create a LowRankLayer to replace the full-rank linear layer\n","            low_rank_layer = LowRankLayer(rank, module)\n","            parent_name, child_name = name.rsplit('.', 1)\n","            parent_module = model.get_submodule(parent_name)\n","            setattr(parent_module, child_name, low_rank_layer)\n","            print(f\"Replaced layer: {name}\")\n","    return model\n","\n","# Function to calculate the total number of parameters in the model\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Function to get the file size of a directory in bytes\n","def get_dir_size(dir_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(dir_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","\n","# Get initial size\n","original_size = count_parameters(model)\n","\n","# Save the original model to the directory\n","original_model_dir = \"original_model\"\n","tokenizer.save_pretrained(original_model_dir)\n","model.save_pretrained(original_model_dir)\n","\n","# Replace linear layers with low-rank approximations\n","model = replace_with_low_rank(model, COMPRESSION_RANK)\n","\n","# Get final size\n","compressed_size = count_parameters(model)\n","\n","# Save the compressed model to the directory\n","compressed_model_dir = \"compressed_model\"\n","tokenizer.save_pretrained(compressed_model_dir)\n","model.save_pretrained(compressed_model_dir)\n","\n","# Get the file sizes of the original and compressed models\n","original_file_size = get_dir_size(original_model_dir)\n","compressed_file_size = get_dir_size(compressed_model_dir)\n","\n","# Print sizes and compression rate\n","print(f\"Original model size (parameters): {original_size}\")\n","print(f\"Compressed model size (parameters): {compressed_size}\")\n","print(f\"Parameter compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n","\n","print(f\"Original model file size: {original_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"Compressed model file size: {compressed_file_size / (1024 ** 2):.2f} MB\")\n","print(f\"File size compression rate: {(original_file_size - compressed_file_size) / original_file_size:.2%}\")\n","\n","print(f\"Models saved to directories: {original_model_dir} and {compressed_model_dir}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0069f92422a64a5d85d7637cd35758c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0413433de3cf453dae054bd9f710f586":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0426fe4d9fb649b29bb5a4052aebf30c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07473ba8774b4fb6b1ba541769d7b8bc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1cab7a0b22094a098519ad7632b6f097":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f60bb96e4f14cf98267447026fbe636":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1432527c5674691b82cceeb36fe683f","placeholder":"​","style":"IPY_MODEL_ad7b0edc469440019eefe55621bb88b5","value":"tokenizer.json: 100%"}},"1fb329b7ba344edab6939ffc0502516f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"203bd42827d144e7bfdd52b23615141a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1fb72fcde3e4dc886c94b7fa9a10107","IPY_MODEL_4eca8d10bf1f478aacda60d96589961f","IPY_MODEL_c46affb99ccd475c9950a5b17cb50d7e"],"layout":"IPY_MODEL_1cab7a0b22094a098519ad7632b6f097"}},"24394d99d942436d99d7275b9f3308b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f60bb96e4f14cf98267447026fbe636","IPY_MODEL_e753ffe8e2e442c0b785f3c6cdd20dfe","IPY_MODEL_40e4c10079844ad1ad192c558867468c"],"layout":"IPY_MODEL_55ba1263d06f4b5fb8d92588f84d6d9f"}},"2517d13755b74a1d8edf145627ba20d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2822d6f2dbfa427993e462e799ae2800":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aef9145fb264d07ab79d9f64c01e3e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e53ba1af1bd46f3af935851e3330d14":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fb329b7ba344edab6939ffc0502516f","placeholder":"​","style":"IPY_MODEL_68902abaeb174ec2ad798589aa3b3cd2","value":"merges.txt: 100%"}},"3d6866f0013e4790a8215d01789362c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3da7e881c8fa45f8b79c2546fc0c6162":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40e0d6b64a0144b9bc24eeeb180325e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40e4c10079844ad1ad192c558867468c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dd4e10b536941a2abfa4f632104863b","placeholder":"​","style":"IPY_MODEL_9c1d40ef87b6408aa44e7628a8c655da","value":" 2.11M/2.11M [00:00&lt;00:00, 2.51MB/s]"}},"40fc697256a347cf94a462fc06b4b83d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3da7e881c8fa45f8b79c2546fc0c6162","max":237,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78ba2947a2874a36be79c95d7f3894f2","value":237}},"46af57e9bf484b368e269f3fa7b597df":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"488f707db81548cda349ddd990334998":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb0c2b5aa4e4b599fb74404fbe57a3f","placeholder":"​","style":"IPY_MODEL_dec3c162337c4a1a905237fa1e01fa45","value":"added_tokens.json: 100%"}},"4cb0c2b5aa4e4b599fb74404fbe57a3f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4dd4e10b536941a2abfa4f632104863b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eca8d10bf1f478aacda60d96589961f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e123209fe8c45de8113457c6acc5f36","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6783f42c4ae4a149a26f143d47a16f9","value":99}},"55ba1263d06f4b5fb8d92588f84d6d9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55efc9ac6267403999345da8c2e766b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58d0c92542d54c839c74070a19ee2e09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e8be4a6feaf47038b12959f4a2c23e7","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f25b06d8a9e1481f8ff50fe4b1eeddaf","value":456318}},"64ed74ffdbd74258862477d2ff5cb53a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d4801981624bd5a3bc20911c3540bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68902abaeb174ec2ad798589aa3b3cd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"694df95566db4e809a6c534e9bbfae8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c99ef41becc5410ab1ad5bdc18ff888f","placeholder":"​","style":"IPY_MODEL_0426fe4d9fb649b29bb5a4052aebf30c","value":" 237/237 [00:00&lt;00:00, 8.07kB/s]"}},"69d80ef24d5148bcb067b318b2e96381":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd5adf52c0aa4927b489ef9a64bece68","IPY_MODEL_902f4c7ef4764ecc8a54b99c905d08f5","IPY_MODEL_e5b695d8721241da9638bde4e4eaa668"],"layout":"IPY_MODEL_c27738fccd734d4f94efd58dcc6c46cb"}},"731c58a04c4445fc99688de7497f82a6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76d50c0508f145ecabc124e3f016b27c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3c9969838bb4078b5aad7ecacb8c19f","max":1080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2517d13755b74a1d8edf145627ba20d0","value":1080}},"78ba2947a2874a36be79c95d7f3894f2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d10b2289f6c496b8b594c94e4ce8564":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e8be4a6feaf47038b12959f4a2c23e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"902f4c7ef4764ecc8a54b99c905d08f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46af57e9bf484b368e269f3fa7b597df","max":798156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd001dc3bd554e5ea2c8ebbfe22ea279","value":798156}},"93c9b8892beb4a258c4697cd040750e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a0a280d76b4924849980c3ddb0b4a8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c1d40ef87b6408aa44e7628a8c655da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e123209fe8c45de8113457c6acc5f36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9edcac24ac29420cb905150175e2791c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1432527c5674691b82cceeb36fe683f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad7b0edc469440019eefe55621bb88b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2f07ed0f4bb4a8895e63364c2930379":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_731c58a04c4445fc99688de7497f82a6","placeholder":"​","style":"IPY_MODEL_0413433de3cf453dae054bd9f710f586","value":"tokenizer_config.json: 100%"}},"b3c9969838bb4078b5aad7ecacb8c19f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6783f42c4ae4a149a26f143d47a16f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b715f7d97211413c94951f0400afdb2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba5b19e74e6948b2b2a57336feb3ebc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_488f707db81548cda349ddd990334998","IPY_MODEL_76d50c0508f145ecabc124e3f016b27c","IPY_MODEL_e664c1df58644b24a1530d0cd1cd63f1"],"layout":"IPY_MODEL_2822d6f2dbfa427993e462e799ae2800"}},"bd5adf52c0aa4927b489ef9a64bece68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d10b2289f6c496b8b594c94e4ce8564","placeholder":"​","style":"IPY_MODEL_f6cfcec8913a46dba7fa42bc5b84269b","value":"vocab.json: 100%"}},"c27738fccd734d4f94efd58dcc6c46cb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c46affb99ccd475c9950a5b17cb50d7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93c9b8892beb4a258c4697cd040750e7","placeholder":"​","style":"IPY_MODEL_3d6866f0013e4790a8215d01789362c6","value":" 99.0/99.0 [00:00&lt;00:00, 6.99kB/s]"}},"c6374f3b729342e0a9636cce8d636d4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40e0d6b64a0144b9bc24eeeb180325e1","placeholder":"​","style":"IPY_MODEL_96a0a280d76b4924849980c3ddb0b4a8","value":" 456k/456k [00:00&lt;00:00, 891kB/s]"}},"c7653971d4794207881a808997fdd548":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c99ef41becc5410ab1ad5bdc18ff888f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9ec3900306c4920b5f5a25ddbcb68ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd001dc3bd554e5ea2c8ebbfe22ea279":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1fb72fcde3e4dc886c94b7fa9a10107":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aef9145fb264d07ab79d9f64c01e3e1","placeholder":"​","style":"IPY_MODEL_55efc9ac6267403999345da8c2e766b5","value":"special_tokens_map.json: 100%"}},"dce9ec1b68cf49aa9088341a52d258fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e53ba1af1bd46f3af935851e3330d14","IPY_MODEL_58d0c92542d54c839c74070a19ee2e09","IPY_MODEL_c6374f3b729342e0a9636cce8d636d4a"],"layout":"IPY_MODEL_b715f7d97211413c94951f0400afdb2a"}},"dec3c162337c4a1a905237fa1e01fa45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e542341aec134fc3be4ade3a6dc66dde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2f07ed0f4bb4a8895e63364c2930379","IPY_MODEL_40fc697256a347cf94a462fc06b4b83d","IPY_MODEL_694df95566db4e809a6c534e9bbfae8c"],"layout":"IPY_MODEL_64ed74ffdbd74258862477d2ff5cb53a"}},"e5b695d8721241da9638bde4e4eaa668":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0069f92422a64a5d85d7637cd35758c7","placeholder":"​","style":"IPY_MODEL_c9ec3900306c4920b5f5a25ddbcb68ba","value":" 798k/798k [00:00&lt;00:00, 4.55MB/s]"}},"e664c1df58644b24a1530d0cd1cd63f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9edcac24ac29420cb905150175e2791c","placeholder":"​","style":"IPY_MODEL_c7653971d4794207881a808997fdd548","value":" 1.08k/1.08k [00:00&lt;00:00, 76.1kB/s]"}},"e753ffe8e2e442c0b785f3c6cdd20dfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65d4801981624bd5a3bc20911c3540bc","max":2114924,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07473ba8774b4fb6b1ba541769d7b8bc","value":2114924}},"f25b06d8a9e1481f8ff50fe4b1eeddaf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6cfcec8913a46dba7fa42bc5b84269b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
