{"cells":[{"cell_type":"markdown","metadata":{"id":"q57Zf8p9ivZa"},"source":["#**Step 1: Install All the Required Packages**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-07-04T07:01:54.761491Z","iopub.status.busy":"2024-07-04T07:01:54.760738Z"},"id":"GLXwJqbjtPho","outputId":"ad745e6c-796e-40e4-b3d8-5a7ca1d7098e","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","caldera 0.0.1 requires accelerate==0.28.0, but you have accelerate 0.21.0 which is incompatible.\n","caldera 0.0.1 requires peft==0.10.0, but you have peft 0.4.0 which is incompatible.\n","caldera 0.0.1 requires tokenizers==0.15.2, but you have tokenizers 0.13.3 which is incompatible.\n","caldera 0.0.1 requires transformers==4.39.1, but you have transformers 4.31.0 which is incompatible.\n","quip-sharp 0.0.1 requires accelerate==0.28.0, but you have accelerate 0.21.0 which is incompatible.\n","quip-sharp 0.0.1 requires peft==0.10.0, but you have peft 0.4.0 which is incompatible.\n","quip-sharp 0.0.1 requires tokenizers==0.15.2, but you have tokenizers 0.13.3 which is incompatible.\n","quip-sharp 0.0.1 requires transformers==4.39.1, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"]},{"cell_type":"markdown","metadata":{"id":"8vwn6xGIi03f"},"source":["#**Step 2: Import All the Required Libraries**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"nAMzy_0FtaUZ","trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer\n","\n","from huggingface_hub import login\n","\n","# Log in to Hugging Face\n","login(token=\"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\")\n","\n","# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","# dataset_name = \"pavan01729/gpt_ai_dataset_llamm2_scaled\"\n","# dataset_name = \"nvidia/HelpSteer\"\n","# dataset_name = 'aboonaji/wiki_medical_terms_llam2_format'\n","# dataset_name = 'OneFly7/llama2-sst2-fine-tuning'\n","# dataset_name = 'pavan01729/qpiai_trial_qna_dataset'\n","# dataset_name = \"pavan01729/gpt_ai_llama2_dataset_plus\"\n","# dataset_name = \"pavan01729/LLama2_GPT_ai_dataset_2.0\"\n","dataset_name = \"pavan01729/LLama2_GPT_ai_5.0\"\n","\n","\n","\n","# Fine-tuned model name\n","new_model = \"Llama-2-7b-chat-finetune\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 200\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}\n","\n","# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()\n","\n","# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()\n","\n","# Reload model in FP16 and merge it with LoRA weights\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    low_cpu_mem_usage=True,\n","    return_dict=True,\n","    torch_dtype=torch.float16,\n","    device_map=device_map,\n",")\n","model = PeftModel.from_pretrained(base_model, new_model)\n","model = model.merge_and_unload()\n","\n","# Reload tokenizer to save it\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\"\n","\n","from huggingface_hub import login\n","\n","# Log in to Hugging Face\n","login(token=\"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\")\n","\n","model.push_to_hub(\"pavan01729/LLama2_gpt_ai_5.0_200e\", check_pr=True)\n","\n","tokenizer.push_to_hub(\"pavan01729/LLama2_gpt_ai_5.0_200e\",check_pr=True)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/azureuser/.cache/huggingface/token\n","Login successful\n"]}],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ib_We3NLtj2E","trusted":true},"outputs":[],"source":["# The model that you want to train from the Hugging Face hub\n","model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n","\n","# The instruction dataset to use\n","# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","# dataset_name = \"pavan01729/gpt_ai_dataset_llamm2_scaled\"\n","# dataset_name = \"nvidia/HelpSteer\"\n","# dataset_name = 'aboonaji/wiki_medical_terms_llam2_format'\n","# dataset_name = 'OneFly7/llama2-sst2-fine-tuning'\n","# dataset_name = 'pavan01729/qpiai_trial_qna_dataset'\n","# dataset_name = \"pavan01729/gpt_ai_llama2_dataset_plus\"\n","# dataset_name = \"pavan01729/LLama2_GPT_ai_dataset_2.0\"\n","dataset_name = \"pavan01729/LLama2_GPT_ai_5.0\"\n","\n","\n","\n","# Fine-tuned model name\n","new_model = \"Llama-2-7b-chat-finetune\"\n","\n","################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# LoRA attention dimension\n","lora_r = 64\n","\n","# Alpha parameter for LoRA scaling\n","lora_alpha = 16\n","\n","# Dropout probability for LoRA layers\n","lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# Activate 4-bit precision base model loading\n","use_4bit = True\n","\n","# Compute dtype for 4-bit base models\n","bnb_4bit_compute_dtype = \"float16\"\n","\n","# Quantization type (fp4 or nf4)\n","bnb_4bit_quant_type = \"nf4\"\n","\n","# Activate nested quantization for 4-bit base models (double quantization)\n","use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# Output directory where the model predictions and checkpoints will be stored\n","output_dir = \"./results\"\n","\n","# Number of training epochs\n","num_train_epochs = 200\n","\n","# Enable fp16/bf16 training (set bf16 to True with an A100)\n","fp16 = False\n","bf16 = False\n","\n","# Batch size per GPU for training\n","per_device_train_batch_size = 4\n","\n","# Batch size per GPU for evaluation\n","per_device_eval_batch_size = 4\n","\n","# Number of update steps to accumulate the gradients for\n","gradient_accumulation_steps = 1\n","\n","# Enable gradient checkpointing\n","gradient_checkpointing = True\n","\n","# Maximum gradient normal (gradient clipping)\n","max_grad_norm = 0.3\n","\n","# Initial learning rate (AdamW optimizer)\n","learning_rate = 2e-4\n","\n","# Weight decay to apply to all layers except bias/LayerNorm weights\n","weight_decay = 0.001\n","\n","# Optimizer to use\n","optim = \"paged_adamw_32bit\"\n","\n","# Learning rate schedule\n","lr_scheduler_type = \"cosine\"\n","\n","# Number of training steps (overrides num_train_epochs)\n","max_steps = -1\n","\n","# Ratio of steps for a linear warmup (from 0 to learning rate)\n","warmup_ratio = 0.03\n","\n","# Group sequences into batches with same length\n","# Saves memory and speeds up training considerably\n","group_by_length = True\n","\n","# Save checkpoint every X updates steps\n","save_steps = 0\n","\n","# Log every X updates steps\n","logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# Maximum sequence length to use\n","max_seq_length = None\n","\n","# Pack multiple short examples in the same input sequence to increase efficiency\n","packing = False\n","\n","# Load the entire model on the GPU 0\n","device_map = {\"\": 0}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470,"referenced_widgets":["45aeee5227e24dcf9f888745686bfc33","6fbb53d3a4a74158a55ec6960cf1afc9","95db0c46f0674f3d8e055b276e8a323b","be79c2698ac9471caabc093f5e091b2a","6efed8d43ef64dfdb2be8c9d1dc6e9ec","c2e62f6c700342f1951ca5c51f491ccc","193a5c198c174974bd461d892743ff8c","9018a049d5c94d8f9ce48517d570b684","42b4b1d4a1444694b9b8c1054b569755","fb28c6dfe1d1464c854275b6708cda09","ef971cae0e6546d38c56e952c34b2578","2d5e6def1a344afb8c92d0ea285f8b0b","7aa06abe3806471fae9d08894a97d22f","dd2e2713bd374816aa9af69daf334a5d","999cf0312a4e48acbfad8f470af9d7aa","56e8471f48c04be094290c7a895f2c6e","c5b6af55d59443169e6b897deee707d6","849e33a610d34df6bb69c28e00b96487","80178ae5af3e4f2b98d866ecdeac6d88","36655f6de13b40f4a0a04fd1a1dcb551","afe48ec698b541f19546bea1d3f67784","259e75d6c49242d59d6b098023a878f5","3dad5100ee5d416783e9f7cdc792f87a","fcca3aefca1849acb6d8f4c2b4b16e6b","8f7428a65c544ed8a3eb22c7b29d8016","e04bfa7b7b7e42ca8c4d43de8fae050f","ce77611777104f218a90b09189090fc5","6e1111b48a2b41f3a1ffd86a2139651b","5cfb6a4b1d7c4d349ecaceb675d0c7e2","5a7393c680454362b275b1629e43b954","706506aaf2e1443eb6d4bf15cda7a013","1e05c0dabdf441168025566f9a3956b7","a135f43c43b846bb82319e9acf314dfe","1b98ba97d7e7435f8f131afeb92ea44d","719b10c263e744e88e13c27ac6458364","5f7956f3f8c44bcbabfbc077d0c45805","0fcb9f8e32e445ea9ea27fbc3b5767eb","25ff3d95e46b4c75a3a3eee598711a42","8084ef93cc7a4926a8e81a119c218515","162b4f45a99e42809ac28caba50c7acb","b7f36b279fb4466497b263a61aa72478","ac38479bda4c4b09af390efa975f5441","0ab5224cb044484a90e980a562a9af5e","6ed60133717541928e8bab69570037f5","fa94767a05bf4e1c90c51be818e8791e","a6fe930e162c42ec982da5aa37afda4a","b72d768f75a44068b7266e2acb09cdd7","3129f6c5f6d248798e83e7a1ba252d19","5493117b8f124419aacf2161149b5807","279dde362b00464e8bf558ec4344b83d","aa00d4d53ec1415081a5a7de8f85b5e4","14a350f0aee54dabb207568ebd111365","a5a04973d2cb424c93d2771a5228c1ad","743f4ed534994730a47e71538a744e66","1472175afb5842d5ab3fca16762136bf","0d4cd9fdc4c647349df5da66caf30ea3","5374ae037fde41608d0da46eb9638648","15dc3e74b61e4e99bb87bbd12520cd47","e571b877af8844d6a489876582288ebf","764667e999e848f9ace39f6fb1b2adde","503e7f31a61d429ab7c038b75fb3dbeb","e76a401c013a4a1095cdd57160931a23","6441dfb03a32447893a69240d78c7159","5f3e8696d0794a7aa91cbebaed5f5154","4f6fc75822bf4cf1a267c6853ede09a2","b9007593c2344f01a66b8e9b6d43793f","f5ff5223800c45aa86dffc8285da6e2b","dd7cc5e6b5684f3fa89cf9fd34607225","365d2aa8bbcc430fbd7a51e992965022","41b9b79ff91e4964a32ee0bd39218869","9d72438292bb469eba308973ad87b84b","f61a778dd2c94c5fb881d79cfb4db10d","c36977ac39694cd4909081e1eaa9c387","09cb6fd17cd04962a3d1114848a70b96","444ddf5ac00145d39eee11cb35b85264","cc08928d26424fc6994b3a23cf1bf4ab","31dac5badd7d4a969ab36d3cf8e27db7","50428a9357b0448cb46dda8011c1a7e2","c7188263c1f5405993aec4b36e98f84a","dd3e1cdcc2a248c895f12f14de100a5e","df041f1a079e4136b977e908fa767648","1c82789a775b445781204e55a04a96a1","fb862ac01d4e4c19928dbba1ae8ac140","4b32923275274fb392b2bedae6c63ded","26dc83b1f7f747d5b2e21fe3a463d920","ba59757f79a745c4827cc4f04f1355cc","b85c7a58cfa54170a7d9a5a1328b970b","7a1f447a3729449faed21f89b554011d","b2b83f6527524f4c8cf4b728dde7734f","18cc04f69b3947b793c18d3991b64a78","c3f832d73bd34d8389678a253318fcdd","1b33d5db723d495592d71ba640066287","a103a857618e4cc5ba4f8431a4979d0e","06b65d5926ba4516be21f30fc388745d","2f0591ed4f654b6496e56c014e28e0e3","2bc364a251e047069361449f2eb178c5","86bb0e7d9f9c4587af8536a932fcb047","d4c9391fd73a4d5688148183649593ca","7aa2f3c3413d42f1908c63b723a37be2"]},"id":"OJXpOgBFuSrc","outputId":"7896cfcc-a161-4cd0-dc9d-0a052d2287d3","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e909f5d1ebb47f9a3dcdaf788cd0c54","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/426 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Downloading data: 100%|██████████| 141k/141k [00:00<00:00, 600kB/s]\n","Downloading data: 100%|██████████| 37.4k/37.4k [00:00<00:00, 173kB/s]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a45d2984fc14056a4acc263a25c8cf8","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/517 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a168f0b198ec43fca75b67e496f3ac87","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/130 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1b9eaa4a605f41e5a07f11d971a5fc13","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1f005dc747f4e3c9b608053d75fc879","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"748808e3d9ac4201b7a70b34cc75a942","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1dc6a23e3ee4be09e37e04b4db4c391","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"307e666f14b04467a6f1d8689abb70ae","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ce1868fcf274f6e8f6634b53e77cac8","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abac6cadebfc489283817ef1accf1b0b","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8f3b55977ba4cb08ebd18b0b9bb1509","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1d1931f963c4524959411284f83c4c9","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d3b675de01047f2b490def7084f85e4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5bf7de115504709bac3d7df27f71eb5","version_major":2,"version_minor":0},"text/plain":["added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c566a0cacd9043d0adf646fa920a13a3","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/build/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n","  warnings.warn(\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 209.50 MiB is free. Process 221524 has 4.29 GiB memory in use. Process 364443 has 6.66 GiB memory in use. Including non-PyTorch memory, this process has 4.47 GiB memory in use. Of the allocated memory 3.97 GiB is allocated by PyTorch, and 145.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     46\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     47\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[1;32m     48\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mnum_train_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Set supervised fine-tuning parameters\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     79\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:141\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token)\u001b[0m\n\u001b[1;32m    136\u001b[0m         model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    137\u001b[0m             model,\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 141\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_model_for_int8_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/peft/utils/other.py:106\u001b[0m, in \u001b[0;36mprepare_model_for_int8_training\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_model_for_int8_training\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    102\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    105\u001b[0m     )\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/pavan/build/lib/python3.10/site-packages/peft/utils/other.py:81\u001b[0m, in \u001b[0;36mprepare_model_for_kbit_training\u001b[0;34m(model, use_gradient_checkpointing)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16) \u001b[38;5;129;01mor\u001b[39;00m (param\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 81\u001b[0m         param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loaded_in_kbit \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_input_require_grads\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 209.50 MiB is free. Process 221524 has 4.29 GiB memory in use. Process 364443 has 6.66 GiB memory in use. Including non-PyTorch memory, this process has 4.47 GiB memory in use. Of the allocated memory 3.97 GiB is allocated by PyTorch, and 145.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["# Load dataset (you can process it here)\n","dataset = load_dataset(dataset_name, split=\"train\")\n","\n","# Load tokenizer and model with QLoRA configuration\n","compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,\n","    bnb_4bit_quant_type=bnb_4bit_quant_type,\n","    bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_use_double_quant=use_nested_quant,\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)\n","\n","# Load base model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map=device_map\n",")\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n","\n","# Load LLaMA tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","\n","# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=lora_alpha,\n","    lora_dropout=lora_dropout,\n","    r=lora_r,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=output_dir,\n","    num_train_epochs=num_train_epochs,\n","    per_device_train_batch_size=per_device_train_batch_size,\n","    gradient_accumulation_steps=gradient_accumulation_steps,\n","    optim=optim,\n","    save_steps=save_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=learning_rate,\n","    weight_decay=weight_decay,\n","    fp16=fp16,\n","    bf16=bf16,\n","    max_grad_norm=max_grad_norm,\n","    max_steps=max_steps,\n","    warmup_ratio=warmup_ratio,\n","    group_by_length=group_by_length,\n","    lr_scheduler_type=lr_scheduler_type,\n","    report_to=\"tensorboard\"\n",")\n","\n","# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=max_seq_length,\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","# Train model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQNaFYp40M6k","trusted":true},"outputs":[],"source":["# Save trained model\n","# trainer.model.save_pretrained(new_model)"]},{"cell_type":"markdown","metadata":{"id":"NBtXo_Tgw43o"},"source":["##**Step 5: Check the plots on tensorboard, as follows**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crj9svNe4hU5","trusted":true},"outputs":[],"source":["# %load_ext tensorboard\n","# %tensorboard --logdir results/runs"]},{"cell_type":"markdown","metadata":{"id":"TDqxnjvExGG6"},"source":["###**Step 6:Use the text generation pipeline to ask questions like “What is a large language model?” Note that I’m formatting the input to match Llama 2 prompt template.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"frlSLPin4IJ4","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/azureuser/pavan/build/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n","  warnings.warn(\n","/home/azureuser/pavan/build/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["<s>[INST] What is QPiAI? Who established it? [/INST] QPiAI was established in 2019 and is headquartered in Bengaluru, India. Unterscheidung between quantum computing and classical AI. It focuses on quantum hardware innovation and software development, providing end-to-end solutions for industries. Topics include optimizing supply chains, logistics, and financial models. Established by Quanthouse and NSIT Trends, QPiAI aims to make quantum computing and AI accessible and practical for businesses.\n"]}],"source":["# Ignore warnings\n","logging.set_verbosity(logging.CRITICAL)\n","\n","# Run text generation pipeline with our next model\n","# prompt = \"What is a large language model?\"\n","# prompt = \"What is meta reinforcment learning?\"\n","# prompt = \"What is gnn and autodistillation?\"\n","# prompt = \"What is ISLP book about, who wrote it?\"\n","prompt = \"What is QPiAI? Who established it?\"\n","# prompt = \"What is SVD?\"\n","\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n","print(result[0]['generated_text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkQCviG0Zta-","trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Empty VRAM\n","del model\n","del pipe\n","del trainer\n","import gc\n","gc.collect()\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"Ly6QcnNRxfNR"},"source":["You can train a Llama 2 model on the entire dataset using [mlabonne/guanaco-llama2](https://huggingface.co/datasets/mlabonne/guanaco-llama2)"]},{"cell_type":"markdown","metadata":{"id":"CwbthYYhxs8p"},"source":["#**Step 7: Store New Llama2 Model (Llama-2-7b-chat-finetune)**"]},{"cell_type":"markdown","metadata":{"id":"ugs6EbD9xtAs"},"source":["How can we store our new Llama-2-7b-chat-finetune model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQn30cRtAZ-P","trusted":true},"outputs":[],"source":["# # Reload model in FP16 and merge it with LoRA weights\n","# base_model = AutoModelForCausalLM.from_pretrained(\n","#     model_name,\n","#     low_cpu_mem_usage=True,\n","#     return_dict=True,\n","#     torch_dtype=torch.float16,\n","#     device_map=device_map,\n","# )\n","# model = PeftModel.from_pretrained(base_model, new_model)\n","# model = model.merge_and_unload()\n","\n","# # Reload tokenizer to save it\n","# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","# tokenizer.pad_token = tokenizer.eos_token\n","# tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17cf546d95d44d0698a69053d17748ec","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b7aceb1968d450686a2a4e0d9b2fa7a","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","from accelerate import init_empty_weights, infer_auto_device_map\n","\n","def clear_gpu_cache():\n","    torch.cuda.empty_cache()\n","    torch.cuda.ipc_collect()\n","\n","# Clear the GPU cache before loading the model\n","clear_gpu_cache()\n","\n","try:\n","    # Reload model in FP16 and merge it with LoRA weights\n","    with init_empty_weights():\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            low_cpu_mem_usage=True,\n","            return_dict=True,\n","            torch_dtype=torch.float16,\n","        )\n","    \n","    device_map = infer_auto_device_map(base_model, max_memory={0: \"15GB\", 1: \"15GB\"})\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        low_cpu_mem_usage=True,\n","        return_dict=True,\n","        torch_dtype=torch.float16,\n","        device_map=device_map,  # Use the generated device map\n","    )\n","    \n","    model = PeftModel.from_pretrained(base_model, new_model)\n","    model = model.merge_and_unload()\n","\n","    # Reload tokenizer to save it\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n","\n","except torch.cuda.OutOfMemoryError:\n","    print(\"CUDA out of memory. Trying to free cache and reload.\")\n","    clear_gpu_cache()\n","    # Attempt to reload the model with CPU offloading\n","    with init_empty_weights():\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            low_cpu_mem_usage=True,\n","            return_dict=True,\n","            torch_dtype=torch.float16,\n","        )\n","    \n","    device_map = infer_auto_device_map(base_model, max_memory={0: \"15GB\", 1: \"15GB\"}, no_split_module_classes=[\"GPT2Block\"])\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        model_name,\n","        low_cpu_mem_usage=True,\n","        return_dict=True,\n","        torch_dtype=torch.float16,\n","        device_map=device_map,  # Use the generated device map\n","        offload_folder=\"offload\",  # Offload to CPU\n","    )\n","    \n","    model = PeftModel.from_pretrained(base_model, new_model)\n","    model = model.merge_and_unload()\n","\n","    # Reload tokenizer to save it\n","    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"right\"\n"]},{"cell_type":"markdown","metadata":{"id":"2bn7tjdByJ_i"},"source":["#**Step 8: Push Model to Hugging Face Hub**"]},{"cell_type":"markdown","metadata":{"id":"CpyMDvc5yQrp"},"source":["Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEPnFIsv5tuo","trusted":true},"outputs":[],"source":["# !pip install datasets huggingface_hub\n","# import locale\n","# locale.getpreferredencoding = lambda: \"UTF-8\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPFj-Wfz5aB0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /home/azureuser/.cache/huggingface/token\n","Login successful\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/pavan01729/LLama2_gpt_ai_5.0_150e/commit/862e3efc6a8c7e770cd44c24e8ef09cf18287bc2', commit_message='Upload tokenizer', commit_description='', oid='862e3efc6a8c7e770cd44c24e8ef09cf18287bc2', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from huggingface_hub import login\n","\n","# Log in to Hugging Face\n","login(token=\"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\")\n","\n","model.push_to_hub(\"pavan01729/LLama2_gpt_ai_5.0_200e\", check_pr=True)\n","\n","tokenizer.push_to_hub(\"pavan01729/LLama2_gpt_ai_5.0_200e\",check_pr=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-xPb-_qB0dz","trusted":true},"outputs":[],"source":["# !huggingface-cli login\n","\n","# model.push_to_hub(\"pavan01729/llama2_Ai_gpt_data\", check_pr=True)\n","\n","# tokenizer.push_to_hub(\"pavan01729/llama2_Ai_gpt_data\",check_pr=True)\n"]},{"cell_type":"markdown","metadata":{"id":"6nxb3tkLyXeI"},"source":["You can now use this model for inference by loading it like any other Llama 2 model from the Hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UwfgUuy_ppsZ","trusted":true},"outputs":[],"source":["# import PyPDF2\n","# import requests\n","# import json\n","# import pandas as pd\n","\n","# # Extract text from PDF function\n","# def extract_text_from_pdf(pdf_path):\n","#     text = \"\"\n","#     with open(pdf_path, 'rb') as file:\n","#         reader = PyPDF2.PdfFileReader(file)\n","#         for page_num in range(reader.numPages):\n","#             page = reader.getPage(page_num)\n","#             text += page.extractText()\n","#     return text\n","\n","# # Generate Q&A pairs function\n","# def generate_qa_pairs(text):\n","#     ollama_api_endpoint = \"https://api.ollama.ai/generate_qa\"\n","#     api_key = \"your_api_key_here\"\n","#     headers = {\n","#         \"Authorization\": f\"Bearer {api_key}\",\n","#         \"Content-Type\": \"application/json\"\n","#     }\n","#     data = {\n","#         \"text\": text,\n","#         \"num_pairs\": 5  # Number of Q&A pairs to generate, adjust as needed\n","#     }\n","#     response = requests.post(ollama_api_endpoint, headers=headers, data=json.dumps(data))\n","#     return response.json()\n","\n","# # Main execution\n","# pdf_path = 'path_to_your_pdf.pdf'\n","# pdf_text = extract_text_from_pdf(pdf_path)\n","# qa_pairs = generate_qa_pairs(pdf_text)\n","\n","# # Process the generated Q&A pairs\n","# questions = [pair['question'] for pair in qa_pairs]\n","# answers = [pair['answer'] for pair in qa_pairs]\n","\n","# # Create a DataFrame and save it as CSV\n","# data = {\n","#     'question': questions,\n","#     'answer': answers\n","# }\n","# df = pd.DataFrame(data)\n","# df.to_csv('qa_dataset.csv', index=False)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"widgets":{"application/vnd.jupyter.widget-state+json":{"06b65d5926ba4516be21f30fc388745d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09cb6fd17cd04962a3d1114848a70b96":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ab5224cb044484a90e980a562a9af5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d4cd9fdc4c647349df5da66caf30ea3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5374ae037fde41608d0da46eb9638648","IPY_MODEL_15dc3e74b61e4e99bb87bbd12520cd47","IPY_MODEL_e571b877af8844d6a489876582288ebf"],"layout":"IPY_MODEL_764667e999e848f9ace39f6fb1b2adde"}},"0fcb9f8e32e445ea9ea27fbc3b5767eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ab5224cb044484a90e980a562a9af5e","placeholder":"​","style":"IPY_MODEL_6ed60133717541928e8bab69570037f5","value":" 81/81 [00:00&lt;00:00, 1489.10 examples/s]"}},"1472175afb5842d5ab3fca16762136bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14a350f0aee54dabb207568ebd111365":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15dc3e74b61e4e99bb87bbd12520cd47":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6441dfb03a32447893a69240d78c7159","max":583,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f3e8696d0794a7aa91cbebaed5f5154","value":583}},"162b4f45a99e42809ac28caba50c7acb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18cc04f69b3947b793c18d3991b64a78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06b65d5926ba4516be21f30fc388745d","placeholder":"​","style":"IPY_MODEL_2f0591ed4f654b6496e56c014e28e0e3","value":"model-00001-of-00002.safetensors:  30%"}},"193a5c198c174974bd461d892743ff8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b33d5db723d495592d71ba640066287":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4c9391fd73a4d5688148183649593ca","placeholder":"​","style":"IPY_MODEL_7aa2f3c3413d42f1908c63b723a37be2","value":" 3.02G/9.98G [00:28&lt;00:49, 142MB/s]"}},"1b98ba97d7e7435f8f131afeb92ea44d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_719b10c263e744e88e13c27ac6458364","IPY_MODEL_5f7956f3f8c44bcbabfbc077d0c45805","IPY_MODEL_0fcb9f8e32e445ea9ea27fbc3b5767eb"],"layout":"IPY_MODEL_25ff3d95e46b4c75a3a3eee598711a42"}},"1c82789a775b445781204e55a04a96a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e05c0dabdf441168025566f9a3956b7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"259e75d6c49242d59d6b098023a878f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25ff3d95e46b4c75a3a3eee598711a42":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26dc83b1f7f747d5b2e21fe3a463d920":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"279dde362b00464e8bf558ec4344b83d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bc364a251e047069361449f2eb178c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d5e6def1a344afb8c92d0ea285f8b0b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7aa06abe3806471fae9d08894a97d22f","IPY_MODEL_dd2e2713bd374816aa9af69daf334a5d","IPY_MODEL_999cf0312a4e48acbfad8f470af9d7aa"],"layout":"IPY_MODEL_56e8471f48c04be094290c7a895f2c6e"}},"2f0591ed4f654b6496e56c014e28e0e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3129f6c5f6d248798e83e7a1ba252d19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_743f4ed534994730a47e71538a744e66","placeholder":"​","style":"IPY_MODEL_1472175afb5842d5ab3fca16762136bf","value":" 21/21 [00:00&lt;00:00, 695.22 examples/s]"}},"31dac5badd7d4a969ab36d3cf8e27db7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"365d2aa8bbcc430fbd7a51e992965022":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09cb6fd17cd04962a3d1114848a70b96","max":26788,"min":0,"orientation":"horizontal","style":"IPY_MODEL_444ddf5ac00145d39eee11cb35b85264","value":26788}},"36655f6de13b40f4a0a04fd1a1dcb551":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3dad5100ee5d416783e9f7cdc792f87a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fcca3aefca1849acb6d8f4c2b4b16e6b","IPY_MODEL_8f7428a65c544ed8a3eb22c7b29d8016","IPY_MODEL_e04bfa7b7b7e42ca8c4d43de8fae050f"],"layout":"IPY_MODEL_ce77611777104f218a90b09189090fc5"}},"41b9b79ff91e4964a32ee0bd39218869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc08928d26424fc6994b3a23cf1bf4ab","placeholder":"​","style":"IPY_MODEL_31dac5badd7d4a969ab36d3cf8e27db7","value":" 26.8k/26.8k [00:00&lt;00:00, 1.86MB/s]"}},"42b4b1d4a1444694b9b8c1054b569755":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"444ddf5ac00145d39eee11cb35b85264":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45aeee5227e24dcf9f888745686bfc33":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fbb53d3a4a74158a55ec6960cf1afc9","IPY_MODEL_95db0c46f0674f3d8e055b276e8a323b","IPY_MODEL_be79c2698ac9471caabc093f5e091b2a"],"layout":"IPY_MODEL_6efed8d43ef64dfdb2be8c9d1dc6e9ec"}},"4b32923275274fb392b2bedae6c63ded":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f6fc75822bf4cf1a267c6853ede09a2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"503e7f31a61d429ab7c038b75fb3dbeb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50428a9357b0448cb46dda8011c1a7e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7188263c1f5405993aec4b36e98f84a","IPY_MODEL_dd3e1cdcc2a248c895f12f14de100a5e","IPY_MODEL_df041f1a079e4136b977e908fa767648"],"layout":"IPY_MODEL_1c82789a775b445781204e55a04a96a1"}},"5374ae037fde41608d0da46eb9638648":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_503e7f31a61d429ab7c038b75fb3dbeb","placeholder":"​","style":"IPY_MODEL_e76a401c013a4a1095cdd57160931a23","value":"config.json: 100%"}},"5493117b8f124419aacf2161149b5807":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56e8471f48c04be094290c7a895f2c6e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a7393c680454362b275b1629e43b954":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cfb6a4b1d7c4d349ecaceb675d0c7e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f3e8696d0794a7aa91cbebaed5f5154":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f7956f3f8c44bcbabfbc077d0c45805":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7f36b279fb4466497b263a61aa72478","max":81,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac38479bda4c4b09af390efa975f5441","value":81}},"6441dfb03a32447893a69240d78c7159":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e1111b48a2b41f3a1ffd86a2139651b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ed60133717541928e8bab69570037f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6efed8d43ef64dfdb2be8c9d1dc6e9ec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fbb53d3a4a74158a55ec6960cf1afc9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2e62f6c700342f1951ca5c51f491ccc","placeholder":"​","style":"IPY_MODEL_193a5c198c174974bd461d892743ff8c","value":"Downloading readme: 100%"}},"706506aaf2e1443eb6d4bf15cda7a013":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"719b10c263e744e88e13c27ac6458364":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8084ef93cc7a4926a8e81a119c218515","placeholder":"​","style":"IPY_MODEL_162b4f45a99e42809ac28caba50c7acb","value":"Generating train split: 100%"}},"743f4ed534994730a47e71538a744e66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"764667e999e848f9ace39f6fb1b2adde":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a1f447a3729449faed21f89b554011d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7aa06abe3806471fae9d08894a97d22f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5b6af55d59443169e6b897deee707d6","placeholder":"​","style":"IPY_MODEL_849e33a610d34df6bb69c28e00b96487","value":"Downloading data: 100%"}},"7aa2f3c3413d42f1908c63b723a37be2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80178ae5af3e4f2b98d866ecdeac6d88":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8084ef93cc7a4926a8e81a119c218515":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"849e33a610d34df6bb69c28e00b96487":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86bb0e7d9f9c4587af8536a932fcb047":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f7428a65c544ed8a3eb22c7b29d8016":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a7393c680454362b275b1629e43b954","max":8829,"min":0,"orientation":"horizontal","style":"IPY_MODEL_706506aaf2e1443eb6d4bf15cda7a013","value":8829}},"9018a049d5c94d8f9ce48517d570b684":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95db0c46f0674f3d8e055b276e8a323b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9018a049d5c94d8f9ce48517d570b684","max":424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_42b4b1d4a1444694b9b8c1054b569755","value":424}},"999cf0312a4e48acbfad8f470af9d7aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afe48ec698b541f19546bea1d3f67784","placeholder":"​","style":"IPY_MODEL_259e75d6c49242d59d6b098023a878f5","value":" 21.0k/21.0k [00:00&lt;00:00, 46.7kB/s]"}},"9d72438292bb469eba308973ad87b84b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a103a857618e4cc5ba4f8431a4979d0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a135f43c43b846bb82319e9acf314dfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5a04973d2cb424c93d2771a5228c1ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6fe930e162c42ec982da5aa37afda4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_279dde362b00464e8bf558ec4344b83d","placeholder":"​","style":"IPY_MODEL_aa00d4d53ec1415081a5a7de8f85b5e4","value":"Generating test split: 100%"}},"aa00d4d53ec1415081a5a7de8f85b5e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac38479bda4c4b09af390efa975f5441":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afe48ec698b541f19546bea1d3f67784":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2b83f6527524f4c8cf4b728dde7734f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18cc04f69b3947b793c18d3991b64a78","IPY_MODEL_c3f832d73bd34d8389678a253318fcdd","IPY_MODEL_1b33d5db723d495592d71ba640066287"],"layout":"IPY_MODEL_a103a857618e4cc5ba4f8431a4979d0e"}},"b72d768f75a44068b7266e2acb09cdd7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14a350f0aee54dabb207568ebd111365","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5a04973d2cb424c93d2771a5228c1ad","value":21}},"b7f36b279fb4466497b263a61aa72478":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b85c7a58cfa54170a7d9a5a1328b970b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9007593c2344f01a66b8e9b6d43793f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba59757f79a745c4827cc4f04f1355cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"be79c2698ac9471caabc093f5e091b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb28c6dfe1d1464c854275b6708cda09","placeholder":"​","style":"IPY_MODEL_ef971cae0e6546d38c56e952c34b2578","value":" 424/424 [00:00&lt;00:00, 22.7kB/s]"}},"c2e62f6c700342f1951ca5c51f491ccc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c36977ac39694cd4909081e1eaa9c387":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3f832d73bd34d8389678a253318fcdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bc364a251e047069361449f2eb178c5","max":9976576152,"min":0,"orientation":"horizontal","style":"IPY_MODEL_86bb0e7d9f9c4587af8536a932fcb047","value":3019898880}},"c5b6af55d59443169e6b897deee707d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7188263c1f5405993aec4b36e98f84a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb862ac01d4e4c19928dbba1ae8ac140","placeholder":"​","style":"IPY_MODEL_4b32923275274fb392b2bedae6c63ded","value":"Downloading shards:   0%"}},"cc08928d26424fc6994b3a23cf1bf4ab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce77611777104f218a90b09189090fc5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4c9391fd73a4d5688148183649593ca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd2e2713bd374816aa9af69daf334a5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_80178ae5af3e4f2b98d866ecdeac6d88","max":21026,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36655f6de13b40f4a0a04fd1a1dcb551","value":21026}},"dd3e1cdcc2a248c895f12f14de100a5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_26dc83b1f7f747d5b2e21fe3a463d920","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ba59757f79a745c4827cc4f04f1355cc","value":0}},"dd7cc5e6b5684f3fa89cf9fd34607225":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f61a778dd2c94c5fb881d79cfb4db10d","placeholder":"​","style":"IPY_MODEL_c36977ac39694cd4909081e1eaa9c387","value":"model.safetensors.index.json: 100%"}},"df041f1a079e4136b977e908fa767648":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b85c7a58cfa54170a7d9a5a1328b970b","placeholder":"​","style":"IPY_MODEL_7a1f447a3729449faed21f89b554011d","value":" 0/2 [00:00&lt;?, ?it/s]"}},"e04bfa7b7b7e42ca8c4d43de8fae050f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e05c0dabdf441168025566f9a3956b7","placeholder":"​","style":"IPY_MODEL_a135f43c43b846bb82319e9acf314dfe","value":" 8.83k/8.83k [00:00&lt;00:00, 23.4kB/s]"}},"e571b877af8844d6a489876582288ebf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f6fc75822bf4cf1a267c6853ede09a2","placeholder":"​","style":"IPY_MODEL_b9007593c2344f01a66b8e9b6d43793f","value":" 583/583 [00:00&lt;00:00, 44.1kB/s]"}},"e76a401c013a4a1095cdd57160931a23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ef971cae0e6546d38c56e952c34b2578":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5ff5223800c45aa86dffc8285da6e2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dd7cc5e6b5684f3fa89cf9fd34607225","IPY_MODEL_365d2aa8bbcc430fbd7a51e992965022","IPY_MODEL_41b9b79ff91e4964a32ee0bd39218869"],"layout":"IPY_MODEL_9d72438292bb469eba308973ad87b84b"}},"f61a778dd2c94c5fb881d79cfb4db10d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa94767a05bf4e1c90c51be818e8791e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a6fe930e162c42ec982da5aa37afda4a","IPY_MODEL_b72d768f75a44068b7266e2acb09cdd7","IPY_MODEL_3129f6c5f6d248798e83e7a1ba252d19"],"layout":"IPY_MODEL_5493117b8f124419aacf2161149b5807"}},"fb28c6dfe1d1464c854275b6708cda09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb862ac01d4e4c19928dbba1ae8ac140":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcca3aefca1849acb6d8f4c2b4b16e6b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e1111b48a2b41f3a1ffd86a2139651b","placeholder":"​","style":"IPY_MODEL_5cfb6a4b1d7c4d349ecaceb675d0c7e2","value":"Downloading data: 100%"}}}}},"nbformat":4,"nbformat_minor":4}
