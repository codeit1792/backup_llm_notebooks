{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [01:54<00:00, 57.06s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! I’m back!\n",
      "I’ve been away for a while,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Use the same model ID for both the tokenizer and the model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test the model and tokenizer to ensure they work\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was an old man who lived in the forest. He had no children and he loved to tell stories about his life as well-known heroes of ancient times such like Hercules or Odysseus (Ulyssius).\n",
      "One day while walking through woods near home village where people were celebrating some festival with lots foods prepared by local women cooking them over open fire using wood from nearby trees; these delicious smells made him hungry so much that when reached back house after long walk under sunshine heat which gave off warmth against cold night air coming down hillside towards river below town square area filled up every corner possible space available inside building itself! It wasn’t until later did anyone realize just how important role played here: “The Old Man Who Lived In The Forest” became known throughout entire region thanks largely due efforts put forth during those days spent together sharing tales told around campfire each evening before bedtime arrived\n",
      "Time taken: 11.86 seconds\n",
      "Tokens per second: 16.86\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.5,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged over 10 different prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Once upon a time, there was an old man who lived in the forest. He had no children and he loved to tell stories about his life as well-known heroes of ancient times such like Hercules or Odysseus (Ulyssius).\n",
      "One day while walking through woods near home village where people were celebrating some festival with lots foods prepared by local women cooking them over open fire using wood from nearby trees; these delicious smells made him hungry so much that when reached back house after long walk under sunshine heat which gave off warmth against cold night air coming down hillside towards river below town square area filled up every corner possible space available inside building itself! It wasn’t until later did anyone realize just how important role played here: “The Old Man Who Lived In The Forest” became known throughout entire region thanks largely due efforts put forth during those days spent together sharing tales told around campfire each evening before bedtime arrived\n",
      "Time taken: 11.82 seconds\n",
      "Tokens per second: 16.92\n",
      "\n",
      "Iteration 2:\n",
      "In a galaxy far, far away...\n",
      "The Star Wars saga continues with the latest installment in this epic space opera. The Force Awakens is set 30 years after Return of Jedi and introduces new characters to join Luke Skywalker (Mark Hamill), Princess Leia Organa Solo(Carrie Fishers) Han Solio M Harrison Ford). Newcombers include Daisyy Ridley as Rey; John Boyegaas Finn: Oscar Isaac's Poe Damon Lindelof’S Kylo Ren/Ben solo Adam Driverand Lupita Nyong ’O Bwabba Gadot 'Asha Tano / AhoskaTiya Sircar . This film was directed byJj Abrams who also wrote it alongside Lawrence Kasdan & Michael Arndt based on George Lucas original story idea which he later sold off due too budget constraints during production time period between Episodes III&IV\n",
      "Time taken: 11.61 seconds\n",
      "Tokens per second: 17.23\n",
      "\n",
      "Iteration 3:\n",
      "Long ago in a distant land, there lived an old man and his wife. They had no children of their own but they loved each other very much indeed!\n",
      "One day the woman said to her husband: \"I am so lonely without any little ones around me that I think we should adopt one.\" The good-hearted couple agreed with this idea at once; for it was not long before she became pregnant by some unknown father who must have been kind enough nevertheless since he left such beautiful offspring behind him when all others were dead or dying from disease which ravaged those days like wildfire through dry grasses on windy nights during summer months...\n",
      "Time taken: 7.96 seconds\n",
      "Tokens per second: 17.46\n",
      "\n",
      "Iteration 4:\n",
      "In the beginning, there was a man. He had no name and he lived in darkness with his wife who also did not have one but she called herself Eve because that is what her husband said to call himself when they were alone together at night after their children went off into other rooms of this house where it all began for them both as well-being born from nothingness itself which means everything else around us now exists only through our own existence here on earth today!\n",
      "The first thing you need before starting your business plan template excel spreadsheet download free online coursework assignment help service provider company website blog post article review essay paper thesis dissertation research proposal outline introduction conclusion bibliography appendix table chart graph diagram figure caption list reference page works cited endnotes footnote citations sources references quotable quotes statistics data analysis results discussion conclusions recommend actions steps plans goals objectives milestones timelines budgets forecasts projections estimates costs expenses revenue income\n",
      "Time taken: 11.70 seconds\n",
      "Tokens per second: 17.10\n",
      "\n",
      "Iteration 5:\n",
      "The sun was setting over the Pacific Ocean as we approached our destination.\n",
      "The airport in San Jose, Costa Rica is small and easy to navigate with a few taxi drivers waiting outside of baggage claim ready for your arrival at any time day or night!\n",
      "Time taken: 3.05 seconds\n",
      "Tokens per second: 18.38\n",
      "\n",
      "Iteration 6:\n",
      "It was a dark and stormy night.\n",
      "The rain pounded the roof of my house, making it hard to hear anything else going on outside or inside for that matter! I had just finished watching an episode from season 10 (the last one) when all hell broke loose in our living room…literally!! My husband came running into me yelling “What’s wrong?!” as he saw his wife lying face down with her head underneath their coffee table!!! He thought something terrible happened but thankfully nothing did except maybe some bruised pride because apparently she fell off this chair while trying not get wet by standing up too fast after getting outta bed during another thunderstorm earlier today.. Whoopsie Daisies are always funny until they happen right?? Well anyway here we go again folks so let us know what happens next time around shall WE???\n",
      "Time taken: 10.54 seconds\n",
      "Tokens per second: 17.45\n",
      "\n",
      "Iteration 7:\n",
      "The year is 3021, and the world has been ravaged by a nuclear war.\n",
      "The only survivors are those who have taken refuge in underground bunkers known as \"shelters.\" But even these shadows of humanity must fight for their lives against mutated creatures that roam outside...and each other!\n",
      "Time taken: 3.91 seconds\n",
      "Tokens per second: 19.17\n",
      "\n",
      "Iteration 8:\n",
      "In a small village, there lived an old man and his wife. They had no children of their own but they were very fond parents to the orphaned boy who was brought up in that house as if he belonged with them from birth itself!\n",
      "The little lad grew into adulthood without any problems whatsoever; however one day when everyone else went out for work except him – something strange happened which changed everything forever…\n",
      "Time taken: 4.95 seconds\n",
      "Tokens per second: 18.18\n",
      "\n",
      "Iteration 9:\n",
      "In the heart of the jungle, a young boy is born. His name: Tarzan (Alexander Skarsgård). He will be raised by gorillas and learn to survive in this untamed world with his best friend Terk & Crane voiced respectively...\n",
      "In 1987 director James Cameron changed cinema forever when he released The Terminator starring Arnold Schwarzenegger as an indestructible cyborg sent from our future back through time on one mission - kill Sarah Connor before she can give birth t…\n",
      "Time taken: 6.83 seconds\n",
      "Tokens per second: 17.86\n",
      "\n",
      "Iteration 10:\n",
      "Deep beneath the ocean waves, a mysterious creature is on its way to Earth. The only thing standing in it's path are two unlikely heroes: an unemployed fisherman and his young son who live alone together by their wits - until they meet Moby Dick!\n",
      "Actor : Jake Gyllenhaal , Heath Ledger\n",
      "Time taken: 4.34 seconds\n",
      "Tokens per second: 17.73\n",
      "\n",
      "Average time taken: 7.67 seconds\n",
      "Average tokens per second: 17.51\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Define 10 different prompts for text generation\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In a galaxy far, far away\",\n",
    "    \"Long ago in a distant land\",\n",
    "    \"In the beginning, there was\",\n",
    "    \"The sun was setting over\",\n",
    "    \"It was a dark and stormy night\",\n",
    "    \"The year is 3021, and\",\n",
    "    \"In a small village, there lived\",\n",
    "    \"In the heart of the jungle\",\n",
    "    \"Deep beneath the ocean waves\"\n",
    "]\n",
    "\n",
    "total_time = 0\n",
    "total_tokens = 0\n",
    "num_iterations = len(prompts)\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Generate text with additional parameters\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=200,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,  # Adjusts the randomness of predictions\n",
    "        top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "        top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "        repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Calculate the time taken and tokens generated\n",
    "    time_taken = end_time - start_time\n",
    "    tokens_generated = len(output[0])\n",
    "\n",
    "    total_time += time_taken\n",
    "    total_tokens += tokens_generated\n",
    "\n",
    "    # Print each generated text and performance metrics for debugging purposes\n",
    "    print(f\"Iteration {i + 1}:\")\n",
    "    print(generated_text)\n",
    "    print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {tokens_generated / time_taken:.2f}\\n\")\n",
    "\n",
    "# Calculate the average time and tokens per second\n",
    "average_time = total_time / num_iterations\n",
    "average_tokens_per_second = total_tokens / total_time\n",
    "\n",
    "# Print the average performance metrics\n",
    "print(f\"Average time taken: {average_time:.2f} seconds\")\n",
    "print(f\"Average tokens per second: {average_tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [00:51<00:00, 25.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.94it/s]\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer\n",
    "import time\n",
    "\n",
    "# Set the number of threads to 12\n",
    "torch.set_num_threads(12)\n",
    "torch.set_num_interop_threads(12)\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Use the same model ID for both the tokenizer and the model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"cpu\", torch_dtype=torch.float16, trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test the model and tokenizer to ensure they work\n",
    "inputs = tokenizer(\"Hello, world!\", return_tensors=\"pt\").to(\"cpu\")\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# Define a prompt for text generation\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text with additional parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.5,  # Adjusts the randomness of predictions\n",
    "    top_k=50,  # Limits the sampling pool to top-k predictions\n",
    "    top_p=0.95,  # Uses nucleus sampling to keep cumulative probability of top-p tokens\n",
    "    repetition_penalty=2.0  # Penalizes repetition of tokens\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Calculate the time taken and tokens per second\n",
    "time_taken = end_time - start_time\n",
    "tokens_generated = len(output[0])\n",
    "tokens_per_second = tokens_generated / time_taken\n",
    "\n",
    "# Print the generated text and performance metrics\n",
    "print(generated_text)\n",
    "print(f\"Time taken: {time_taken:.2f} seconds\")\n",
    "print(f\"Tokens per second: {tokens_per_second:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
