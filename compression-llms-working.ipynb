{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e542341aec134fc3be4ade3a6dc66dde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b2f07ed0f4bb4a8895e63364c2930379","IPY_MODEL_40fc697256a347cf94a462fc06b4b83d","IPY_MODEL_694df95566db4e809a6c534e9bbfae8c"],"layout":"IPY_MODEL_64ed74ffdbd74258862477d2ff5cb53a"}},"b2f07ed0f4bb4a8895e63364c2930379":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_731c58a04c4445fc99688de7497f82a6","placeholder":"​","style":"IPY_MODEL_0413433de3cf453dae054bd9f710f586","value":"tokenizer_config.json: 100%"}},"40fc697256a347cf94a462fc06b4b83d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3da7e881c8fa45f8b79c2546fc0c6162","max":237,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78ba2947a2874a36be79c95d7f3894f2","value":237}},"694df95566db4e809a6c534e9bbfae8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c99ef41becc5410ab1ad5bdc18ff888f","placeholder":"​","style":"IPY_MODEL_0426fe4d9fb649b29bb5a4052aebf30c","value":" 237/237 [00:00&lt;00:00, 8.07kB/s]"}},"64ed74ffdbd74258862477d2ff5cb53a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"731c58a04c4445fc99688de7497f82a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0413433de3cf453dae054bd9f710f586":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3da7e881c8fa45f8b79c2546fc0c6162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78ba2947a2874a36be79c95d7f3894f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c99ef41becc5410ab1ad5bdc18ff888f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0426fe4d9fb649b29bb5a4052aebf30c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69d80ef24d5148bcb067b318b2e96381":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd5adf52c0aa4927b489ef9a64bece68","IPY_MODEL_902f4c7ef4764ecc8a54b99c905d08f5","IPY_MODEL_e5b695d8721241da9638bde4e4eaa668"],"layout":"IPY_MODEL_c27738fccd734d4f94efd58dcc6c46cb"}},"bd5adf52c0aa4927b489ef9a64bece68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d10b2289f6c496b8b594c94e4ce8564","placeholder":"​","style":"IPY_MODEL_f6cfcec8913a46dba7fa42bc5b84269b","value":"vocab.json: 100%"}},"902f4c7ef4764ecc8a54b99c905d08f5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46af57e9bf484b368e269f3fa7b597df","max":798156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd001dc3bd554e5ea2c8ebbfe22ea279","value":798156}},"e5b695d8721241da9638bde4e4eaa668":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0069f92422a64a5d85d7637cd35758c7","placeholder":"​","style":"IPY_MODEL_c9ec3900306c4920b5f5a25ddbcb68ba","value":" 798k/798k [00:00&lt;00:00, 4.55MB/s]"}},"c27738fccd734d4f94efd58dcc6c46cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d10b2289f6c496b8b594c94e4ce8564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6cfcec8913a46dba7fa42bc5b84269b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46af57e9bf484b368e269f3fa7b597df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd001dc3bd554e5ea2c8ebbfe22ea279":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0069f92422a64a5d85d7637cd35758c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9ec3900306c4920b5f5a25ddbcb68ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dce9ec1b68cf49aa9088341a52d258fa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e53ba1af1bd46f3af935851e3330d14","IPY_MODEL_58d0c92542d54c839c74070a19ee2e09","IPY_MODEL_c6374f3b729342e0a9636cce8d636d4a"],"layout":"IPY_MODEL_b715f7d97211413c94951f0400afdb2a"}},"2e53ba1af1bd46f3af935851e3330d14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fb329b7ba344edab6939ffc0502516f","placeholder":"​","style":"IPY_MODEL_68902abaeb174ec2ad798589aa3b3cd2","value":"merges.txt: 100%"}},"58d0c92542d54c839c74070a19ee2e09":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e8be4a6feaf47038b12959f4a2c23e7","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f25b06d8a9e1481f8ff50fe4b1eeddaf","value":456318}},"c6374f3b729342e0a9636cce8d636d4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40e0d6b64a0144b9bc24eeeb180325e1","placeholder":"​","style":"IPY_MODEL_96a0a280d76b4924849980c3ddb0b4a8","value":" 456k/456k [00:00&lt;00:00, 891kB/s]"}},"b715f7d97211413c94951f0400afdb2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fb329b7ba344edab6939ffc0502516f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68902abaeb174ec2ad798589aa3b3cd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e8be4a6feaf47038b12959f4a2c23e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25b06d8a9e1481f8ff50fe4b1eeddaf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40e0d6b64a0144b9bc24eeeb180325e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a0a280d76b4924849980c3ddb0b4a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24394d99d942436d99d7275b9f3308b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f60bb96e4f14cf98267447026fbe636","IPY_MODEL_e753ffe8e2e442c0b785f3c6cdd20dfe","IPY_MODEL_40e4c10079844ad1ad192c558867468c"],"layout":"IPY_MODEL_55ba1263d06f4b5fb8d92588f84d6d9f"}},"1f60bb96e4f14cf98267447026fbe636":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1432527c5674691b82cceeb36fe683f","placeholder":"​","style":"IPY_MODEL_ad7b0edc469440019eefe55621bb88b5","value":"tokenizer.json: 100%"}},"e753ffe8e2e442c0b785f3c6cdd20dfe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_65d4801981624bd5a3bc20911c3540bc","max":2114924,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07473ba8774b4fb6b1ba541769d7b8bc","value":2114924}},"40e4c10079844ad1ad192c558867468c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dd4e10b536941a2abfa4f632104863b","placeholder":"​","style":"IPY_MODEL_9c1d40ef87b6408aa44e7628a8c655da","value":" 2.11M/2.11M [00:00&lt;00:00, 2.51MB/s]"}},"55ba1263d06f4b5fb8d92588f84d6d9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1432527c5674691b82cceeb36fe683f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad7b0edc469440019eefe55621bb88b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65d4801981624bd5a3bc20911c3540bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07473ba8774b4fb6b1ba541769d7b8bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4dd4e10b536941a2abfa4f632104863b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c1d40ef87b6408aa44e7628a8c655da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba5b19e74e6948b2b2a57336feb3ebc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_488f707db81548cda349ddd990334998","IPY_MODEL_76d50c0508f145ecabc124e3f016b27c","IPY_MODEL_e664c1df58644b24a1530d0cd1cd63f1"],"layout":"IPY_MODEL_2822d6f2dbfa427993e462e799ae2800"}},"488f707db81548cda349ddd990334998":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb0c2b5aa4e4b599fb74404fbe57a3f","placeholder":"​","style":"IPY_MODEL_dec3c162337c4a1a905237fa1e01fa45","value":"added_tokens.json: 100%"}},"76d50c0508f145ecabc124e3f016b27c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3c9969838bb4078b5aad7ecacb8c19f","max":1080,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2517d13755b74a1d8edf145627ba20d0","value":1080}},"e664c1df58644b24a1530d0cd1cd63f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9edcac24ac29420cb905150175e2791c","placeholder":"​","style":"IPY_MODEL_c7653971d4794207881a808997fdd548","value":" 1.08k/1.08k [00:00&lt;00:00, 76.1kB/s]"}},"2822d6f2dbfa427993e462e799ae2800":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cb0c2b5aa4e4b599fb74404fbe57a3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec3c162337c4a1a905237fa1e01fa45":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3c9969838bb4078b5aad7ecacb8c19f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2517d13755b74a1d8edf145627ba20d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9edcac24ac29420cb905150175e2791c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7653971d4794207881a808997fdd548":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"203bd42827d144e7bfdd52b23615141a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1fb72fcde3e4dc886c94b7fa9a10107","IPY_MODEL_4eca8d10bf1f478aacda60d96589961f","IPY_MODEL_c46affb99ccd475c9950a5b17cb50d7e"],"layout":"IPY_MODEL_1cab7a0b22094a098519ad7632b6f097"}},"d1fb72fcde3e4dc886c94b7fa9a10107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aef9145fb264d07ab79d9f64c01e3e1","placeholder":"​","style":"IPY_MODEL_55efc9ac6267403999345da8c2e766b5","value":"special_tokens_map.json: 100%"}},"4eca8d10bf1f478aacda60d96589961f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e123209fe8c45de8113457c6acc5f36","max":99,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6783f42c4ae4a149a26f143d47a16f9","value":99}},"c46affb99ccd475c9950a5b17cb50d7e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93c9b8892beb4a258c4697cd040750e7","placeholder":"​","style":"IPY_MODEL_3d6866f0013e4790a8215d01789362c6","value":" 99.0/99.0 [00:00&lt;00:00, 6.99kB/s]"}},"1cab7a0b22094a098519ad7632b6f097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aef9145fb264d07ab79d9f64c01e3e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55efc9ac6267403999345da8c2e766b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e123209fe8c45de8113457c6acc5f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6783f42c4ae4a149a26f143d47a16f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93c9b8892beb4a258c4697cd040750e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d6866f0013e4790a8215d01789362c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Compression happening\n\n## distilbert = 60%\n\n## T5 = 60%\n\n## T5 base 83.00%\n\n## funnel_transformer smallbase 68%\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom torch.nn import functional as F\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank])\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank])\n        self.Vh = nn.Parameter(Vh[:self.rank, :])\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nrank = 32  # Adjust this for more or less aggressive compression\nmodel = replace_with_low_rank(model, rank)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Test the compressed model with a random input\ninput_text = \"Hello, how are you?\"\ninputs = tokenizer(input_text, return_tensors='pt')\noutput = model(**inputs)\n\n# Print the output shape and the actual output\nprint(\"Output shape:\", output.last_hidden_state.shape)\nprint(\"Output:\", output.last_hidden_state)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T07:36:19.145556Z","iopub.execute_input":"2024-07-01T07:36:19.145915Z","iopub.status.idle":"2024-07-01T07:36:23.916369Z","shell.execute_reply.started":"2024-07-01T07:36:19.145890Z","shell.execute_reply":"2024-07-01T07:36:23.915406Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34bbc6b976934536a9a5e484d71293f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"161ad070a3eb420082e04515f09d84d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87a893d193414286b74fd696652ee45e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a7cb577698242dd9515521f37539d00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4c8dce360f4687bb236e6cde62849d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b0eaa7d29aa49a2adaf4e6bb921c24a"}},"metadata":{}},{"name":"stdout","text":"Original model size (parameters): 124439808\nCompressed model size (parameters): 124439808\nCompression rate: 0.00%\nOutput shape: torch.Size([1, 6, 768])\nOutput: tensor([[[-8.5444e-06, -1.4021e-01, -2.0845e-01,  ..., -1.5329e-01,\n          -6.7826e-02, -1.9630e-01],\n         [ 4.1949e-01,  2.3525e-01,  3.4816e-01,  ...,  4.5321e-02,\n           1.5447e-01,  1.9547e-02],\n         [ 2.5089e-01, -3.9139e-01, -2.6851e-01,  ..., -3.5611e-01,\n          -1.5503e-01, -1.0117e-01],\n         [-3.7585e-02,  4.5083e-01, -6.3438e-02,  ..., -5.4199e-01,\n           2.9846e-01,  6.0528e-02],\n         [ 1.1579e-01, -3.7055e-01, -7.1209e-01,  ..., -8.2506e-02,\n           5.2692e-02,  1.6689e-01],\n         [ 2.8985e-01, -2.3842e-01,  5.7513e-02,  ..., -8.6427e-02,\n          -4.7248e-02,  3.3461e-01]]], device='cuda:0',\n       grad_fn=<ViewBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom torch.nn import functional as F\nfrom huggingface_hub import HfApi, login\n\n# Log in to Hugging Face\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank])\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank])\n        self.Vh = nn.Parameter(Vh[:self.rank, :])\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained('distilbert-base-uncased')\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nrank = 32  # Adjust this for more or less aggressive compression\nmodel = replace_with_low_rank(model, rank)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Test the compressed model with a random input\ninput_text = \"Hello, how are you?\"\ninputs = tokenizer(input_text, return_tensors='pt')\noutput = model(**inputs)\nprint(\"Output shape:\", output.last_hidden_state.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T07:48:09.763941Z","iopub.execute_input":"2024-07-01T07:48:09.764762Z","iopub.status.idle":"2024-07-01T07:48:13.516961Z","shell.execute_reply.started":"2024-07-01T07:48:09.764733Z","shell.execute_reply":"2024-07-01T07:48:13.516081Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\nReplaced layer: transformer.layer.0.attention.q_lin\nReplaced layer: transformer.layer.0.attention.k_lin\nReplaced layer: transformer.layer.0.attention.v_lin\nReplaced layer: transformer.layer.0.attention.out_lin\nReplaced layer: transformer.layer.0.ffn.lin1\nReplaced layer: transformer.layer.0.ffn.lin2\nReplaced layer: transformer.layer.1.attention.q_lin\nReplaced layer: transformer.layer.1.attention.k_lin\nReplaced layer: transformer.layer.1.attention.v_lin\nReplaced layer: transformer.layer.1.attention.out_lin\nReplaced layer: transformer.layer.1.ffn.lin1\nReplaced layer: transformer.layer.1.ffn.lin2\nReplaced layer: transformer.layer.2.attention.q_lin\nReplaced layer: transformer.layer.2.attention.k_lin\nReplaced layer: transformer.layer.2.attention.v_lin\nReplaced layer: transformer.layer.2.attention.out_lin\nReplaced layer: transformer.layer.2.ffn.lin1\nReplaced layer: transformer.layer.2.ffn.lin2\nReplaced layer: transformer.layer.3.attention.q_lin\nReplaced layer: transformer.layer.3.attention.k_lin\nReplaced layer: transformer.layer.3.attention.v_lin\nReplaced layer: transformer.layer.3.attention.out_lin\nReplaced layer: transformer.layer.3.ffn.lin1\nReplaced layer: transformer.layer.3.ffn.lin2\nReplaced layer: transformer.layer.4.attention.q_lin\nReplaced layer: transformer.layer.4.attention.k_lin\nReplaced layer: transformer.layer.4.attention.v_lin\nReplaced layer: transformer.layer.4.attention.out_lin\nReplaced layer: transformer.layer.4.ffn.lin1\nReplaced layer: transformer.layer.4.ffn.lin2\nReplaced layer: transformer.layer.5.attention.q_lin\nReplaced layer: transformer.layer.5.attention.k_lin\nReplaced layer: transformer.layer.5.attention.v_lin\nReplaced layer: transformer.layer.5.attention.out_lin\nReplaced layer: transformer.layer.5.ffn.lin1\nReplaced layer: transformer.layer.5.ffn.lin2\nOriginal model size (parameters): 66362880\nCompressed model size (parameters): 26586624\nCompression rate: 59.94%\nOutput shape: torch.Size([1, 8, 768])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom torch.nn import functional as F\nfrom huggingface_hub import HfApi, create_repo, upload_folder\nfrom huggingface_hub import HfApi, login\n\n# Log in to Hugging Face\ntoken = \"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\"  # Replace with your Hugging Face token\nlogin(token)\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained('distilbert-base-uncased')\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nrank = 32  # Adjust this for more or less aggressive compression\nmodel = replace_with_low_rank(model, rank)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_distilbert\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\n# Create a new repository on Hugging Face\nrepo_name = \"pavan01729/compressed_distilbert\"\ncreate_repo(repo_name, exist_ok=True)\n\n# Upload the model directory to the repository\nupload_folder(repo_id=repo_name, folder_path=model_dir)\n\nprint(f\"Model pushed to Hugging Face Hub at: https://huggingface.co/{repo_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:05:44.745487Z","iopub.execute_input":"2024-07-01T08:05:44.746216Z","iopub.status.idle":"2024-07-01T08:06:09.702006Z","shell.execute_reply.started":"2024-07-01T08:05:44.746189Z","shell.execute_reply":"2024-07-01T08:06:09.701015Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1074bc90cd453db49ac1ead69a0c08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f639636bc30e493f8d9bb3bac7404079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c804f00f5c01441984be1fb610bd8311"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec944ee052f14c17bd6b89ba2fbb4bac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce555d0986342b693763b8e3cefa0ed"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: transformer.layer.0.attention.q_lin\nReplaced layer: transformer.layer.0.attention.k_lin\nReplaced layer: transformer.layer.0.attention.v_lin\nReplaced layer: transformer.layer.0.attention.out_lin\nReplaced layer: transformer.layer.0.ffn.lin1\nReplaced layer: transformer.layer.0.ffn.lin2\nReplaced layer: transformer.layer.1.attention.q_lin\nReplaced layer: transformer.layer.1.attention.k_lin\nReplaced layer: transformer.layer.1.attention.v_lin\nReplaced layer: transformer.layer.1.attention.out_lin\nReplaced layer: transformer.layer.1.ffn.lin1\nReplaced layer: transformer.layer.1.ffn.lin2\nReplaced layer: transformer.layer.2.attention.q_lin\nReplaced layer: transformer.layer.2.attention.k_lin\nReplaced layer: transformer.layer.2.attention.v_lin\nReplaced layer: transformer.layer.2.attention.out_lin\nReplaced layer: transformer.layer.2.ffn.lin1\nReplaced layer: transformer.layer.2.ffn.lin2\nReplaced layer: transformer.layer.3.attention.q_lin\nReplaced layer: transformer.layer.3.attention.k_lin\nReplaced layer: transformer.layer.3.attention.v_lin\nReplaced layer: transformer.layer.3.attention.out_lin\nReplaced layer: transformer.layer.3.ffn.lin1\nReplaced layer: transformer.layer.3.ffn.lin2\nReplaced layer: transformer.layer.4.attention.q_lin\nReplaced layer: transformer.layer.4.attention.k_lin\nReplaced layer: transformer.layer.4.attention.v_lin\nReplaced layer: transformer.layer.4.attention.out_lin\nReplaced layer: transformer.layer.4.ffn.lin1\nReplaced layer: transformer.layer.4.ffn.lin2\nReplaced layer: transformer.layer.5.attention.q_lin\nReplaced layer: transformer.layer.5.attention.k_lin\nReplaced layer: transformer.layer.5.attention.v_lin\nReplaced layer: transformer.layer.5.attention.out_lin\nReplaced layer: transformer.layer.5.ffn.lin1\nReplaced layer: transformer.layer.5.ffn.lin2\nOriginal model size (parameters): 66362880\nCompressed model size (parameters): 26586624\nCompression rate: 59.94%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/106M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e8020aab14d45fd96be0dc31a07d711"}},"metadata":{}},{"name":"stdout","text":"Model pushed to Hugging Face Hub at: https://huggingface.co/pavan01729/compressed_distilbert\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## distilbert_base 60%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'distilbert-base-uncased'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:47:58.870241Z","iopub.execute_input":"2024-07-01T08:47:58.871175Z","iopub.status.idle":"2024-07-01T08:48:08.725471Z","shell.execute_reply.started":"2024-07-01T08:47:58.871142Z","shell.execute_reply":"2024-07-01T08:48:08.724657Z"},"scrolled":true,"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Replaced layer: transformer.layer.0.attention.q_lin\nReplaced layer: transformer.layer.0.attention.k_lin\nReplaced layer: transformer.layer.0.attention.v_lin\nReplaced layer: transformer.layer.0.attention.out_lin\nReplaced layer: transformer.layer.0.ffn.lin1\nReplaced layer: transformer.layer.0.ffn.lin2\nReplaced layer: transformer.layer.1.attention.q_lin\nReplaced layer: transformer.layer.1.attention.k_lin\nReplaced layer: transformer.layer.1.attention.v_lin\nReplaced layer: transformer.layer.1.attention.out_lin\nReplaced layer: transformer.layer.1.ffn.lin1\nReplaced layer: transformer.layer.1.ffn.lin2\nReplaced layer: transformer.layer.2.attention.q_lin\nReplaced layer: transformer.layer.2.attention.k_lin\nReplaced layer: transformer.layer.2.attention.v_lin\nReplaced layer: transformer.layer.2.attention.out_lin\nReplaced layer: transformer.layer.2.ffn.lin1\nReplaced layer: transformer.layer.2.ffn.lin2\nReplaced layer: transformer.layer.3.attention.q_lin\nReplaced layer: transformer.layer.3.attention.k_lin\nReplaced layer: transformer.layer.3.attention.v_lin\nReplaced layer: transformer.layer.3.attention.out_lin\nReplaced layer: transformer.layer.3.ffn.lin1\nReplaced layer: transformer.layer.3.ffn.lin2\nReplaced layer: transformer.layer.4.attention.q_lin\nReplaced layer: transformer.layer.4.attention.k_lin\nReplaced layer: transformer.layer.4.attention.v_lin\nReplaced layer: transformer.layer.4.attention.out_lin\nReplaced layer: transformer.layer.4.ffn.lin1\nReplaced layer: transformer.layer.4.ffn.lin2\nReplaced layer: transformer.layer.5.attention.q_lin\nReplaced layer: transformer.layer.5.attention.k_lin\nReplaced layer: transformer.layer.5.attention.v_lin\nReplaced layer: transformer.layer.5.attention.out_lin\nReplaced layer: transformer.layer.5.ffn.lin1\nReplaced layer: transformer.layer.5.ffn.lin2\nOriginal model size (parameters): 66362880\nCompressed model size (parameters): 26586624\nCompression rate: 59.94%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Bert base uncased 70%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'bert-base-uncased'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:32:18.296260Z","iopub.execute_input":"2024-07-01T12:32:18.296933Z","iopub.status.idle":"2024-07-01T12:32:44.262779Z","shell.execute_reply.started":"2024-07-01T12:32:18.296894Z","shell.execute_reply":"2024-07-01T12:32:44.261681Z"},"scrolled":true,"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14bbbdb9aaf243aeb00d38320b2a89e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1297b2fbeaa54b8388cde94e13ec978d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4261ea07ee124032b7ae8cabc6ef8133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef1c2cdb97ff4480926b08a69ee2b5b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a634a4af6ed406280807f78626ef813"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.layer.0.attention.self.query\nReplaced layer: encoder.layer.0.attention.self.key\nReplaced layer: encoder.layer.0.attention.self.value\nReplaced layer: encoder.layer.0.attention.output.dense\nReplaced layer: encoder.layer.0.intermediate.dense\nReplaced layer: encoder.layer.0.output.dense\nReplaced layer: encoder.layer.1.attention.self.query\nReplaced layer: encoder.layer.1.attention.self.key\nReplaced layer: encoder.layer.1.attention.self.value\nReplaced layer: encoder.layer.1.attention.output.dense\nReplaced layer: encoder.layer.1.intermediate.dense\nReplaced layer: encoder.layer.1.output.dense\nReplaced layer: encoder.layer.2.attention.self.query\nReplaced layer: encoder.layer.2.attention.self.key\nReplaced layer: encoder.layer.2.attention.self.value\nReplaced layer: encoder.layer.2.attention.output.dense\nReplaced layer: encoder.layer.2.intermediate.dense\nReplaced layer: encoder.layer.2.output.dense\nReplaced layer: encoder.layer.3.attention.self.query\nReplaced layer: encoder.layer.3.attention.self.key\nReplaced layer: encoder.layer.3.attention.self.value\nReplaced layer: encoder.layer.3.attention.output.dense\nReplaced layer: encoder.layer.3.intermediate.dense\nReplaced layer: encoder.layer.3.output.dense\nReplaced layer: encoder.layer.4.attention.self.query\nReplaced layer: encoder.layer.4.attention.self.key\nReplaced layer: encoder.layer.4.attention.self.value\nReplaced layer: encoder.layer.4.attention.output.dense\nReplaced layer: encoder.layer.4.intermediate.dense\nReplaced layer: encoder.layer.4.output.dense\nReplaced layer: encoder.layer.5.attention.self.query\nReplaced layer: encoder.layer.5.attention.self.key\nReplaced layer: encoder.layer.5.attention.self.value\nReplaced layer: encoder.layer.5.attention.output.dense\nReplaced layer: encoder.layer.5.intermediate.dense\nReplaced layer: encoder.layer.5.output.dense\nReplaced layer: encoder.layer.6.attention.self.query\nReplaced layer: encoder.layer.6.attention.self.key\nReplaced layer: encoder.layer.6.attention.self.value\nReplaced layer: encoder.layer.6.attention.output.dense\nReplaced layer: encoder.layer.6.intermediate.dense\nReplaced layer: encoder.layer.6.output.dense\nReplaced layer: encoder.layer.7.attention.self.query\nReplaced layer: encoder.layer.7.attention.self.key\nReplaced layer: encoder.layer.7.attention.self.value\nReplaced layer: encoder.layer.7.attention.output.dense\nReplaced layer: encoder.layer.7.intermediate.dense\nReplaced layer: encoder.layer.7.output.dense\nReplaced layer: encoder.layer.8.attention.self.query\nReplaced layer: encoder.layer.8.attention.self.key\nReplaced layer: encoder.layer.8.attention.self.value\nReplaced layer: encoder.layer.8.attention.output.dense\nReplaced layer: encoder.layer.8.intermediate.dense\nReplaced layer: encoder.layer.8.output.dense\nReplaced layer: encoder.layer.9.attention.self.query\nReplaced layer: encoder.layer.9.attention.self.key\nReplaced layer: encoder.layer.9.attention.self.value\nReplaced layer: encoder.layer.9.attention.output.dense\nReplaced layer: encoder.layer.9.intermediate.dense\nReplaced layer: encoder.layer.9.output.dense\nReplaced layer: encoder.layer.10.attention.self.query\nReplaced layer: encoder.layer.10.attention.self.key\nReplaced layer: encoder.layer.10.attention.self.value\nReplaced layer: encoder.layer.10.attention.output.dense\nReplaced layer: encoder.layer.10.intermediate.dense\nReplaced layer: encoder.layer.10.output.dense\nReplaced layer: encoder.layer.11.attention.self.query\nReplaced layer: encoder.layer.11.attention.self.key\nReplaced layer: encoder.layer.11.attention.self.value\nReplaced layer: encoder.layer.11.attention.output.dense\nReplaced layer: encoder.layer.11.intermediate.dense\nReplaced layer: encoder.layer.11.output.dense\nReplaced layer: pooler.dense\nOriginal model size (parameters): 109482240\nCompressed model size (parameters): 29390080\nCompression rate: 73.16%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## facebook bart 66%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'facebook/bart-base'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:34:16.464170Z","iopub.execute_input":"2024-07-01T12:34:16.464909Z","iopub.status.idle":"2024-07-01T12:34:46.864855Z","shell.execute_reply.started":"2024-07-01T12:34:16.464870Z","shell.execute_reply":"2024-07-01T12:34:46.863685Z"},"scrolled":true,"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90fb2127acd417ebfe38a1f3799de56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61717271a1ad4e63b41dcdfd8ee4cb09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b970aaecc9ff4b518a272f6e5b41530f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e0e2749172643198897ef14ff3d5395"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4361c1cb2ed24e9b8e09fb326f1701fc"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.layers.0.self_attn.k_proj\nReplaced layer: encoder.layers.0.self_attn.v_proj\nReplaced layer: encoder.layers.0.self_attn.q_proj\nReplaced layer: encoder.layers.0.self_attn.out_proj\nReplaced layer: encoder.layers.0.fc1\nReplaced layer: encoder.layers.0.fc2\nReplaced layer: encoder.layers.1.self_attn.k_proj\nReplaced layer: encoder.layers.1.self_attn.v_proj\nReplaced layer: encoder.layers.1.self_attn.q_proj\nReplaced layer: encoder.layers.1.self_attn.out_proj\nReplaced layer: encoder.layers.1.fc1\nReplaced layer: encoder.layers.1.fc2\nReplaced layer: encoder.layers.2.self_attn.k_proj\nReplaced layer: encoder.layers.2.self_attn.v_proj\nReplaced layer: encoder.layers.2.self_attn.q_proj\nReplaced layer: encoder.layers.2.self_attn.out_proj\nReplaced layer: encoder.layers.2.fc1\nReplaced layer: encoder.layers.2.fc2\nReplaced layer: encoder.layers.3.self_attn.k_proj\nReplaced layer: encoder.layers.3.self_attn.v_proj\nReplaced layer: encoder.layers.3.self_attn.q_proj\nReplaced layer: encoder.layers.3.self_attn.out_proj\nReplaced layer: encoder.layers.3.fc1\nReplaced layer: encoder.layers.3.fc2\nReplaced layer: encoder.layers.4.self_attn.k_proj\nReplaced layer: encoder.layers.4.self_attn.v_proj\nReplaced layer: encoder.layers.4.self_attn.q_proj\nReplaced layer: encoder.layers.4.self_attn.out_proj\nReplaced layer: encoder.layers.4.fc1\nReplaced layer: encoder.layers.4.fc2\nReplaced layer: encoder.layers.5.self_attn.k_proj\nReplaced layer: encoder.layers.5.self_attn.v_proj\nReplaced layer: encoder.layers.5.self_attn.q_proj\nReplaced layer: encoder.layers.5.self_attn.out_proj\nReplaced layer: encoder.layers.5.fc1\nReplaced layer: encoder.layers.5.fc2\nReplaced layer: decoder.layers.0.self_attn.k_proj\nReplaced layer: decoder.layers.0.self_attn.v_proj\nReplaced layer: decoder.layers.0.self_attn.q_proj\nReplaced layer: decoder.layers.0.self_attn.out_proj\nReplaced layer: decoder.layers.0.encoder_attn.k_proj\nReplaced layer: decoder.layers.0.encoder_attn.v_proj\nReplaced layer: decoder.layers.0.encoder_attn.q_proj\nReplaced layer: decoder.layers.0.encoder_attn.out_proj\nReplaced layer: decoder.layers.0.fc1\nReplaced layer: decoder.layers.0.fc2\nReplaced layer: decoder.layers.1.self_attn.k_proj\nReplaced layer: decoder.layers.1.self_attn.v_proj\nReplaced layer: decoder.layers.1.self_attn.q_proj\nReplaced layer: decoder.layers.1.self_attn.out_proj\nReplaced layer: decoder.layers.1.encoder_attn.k_proj\nReplaced layer: decoder.layers.1.encoder_attn.v_proj\nReplaced layer: decoder.layers.1.encoder_attn.q_proj\nReplaced layer: decoder.layers.1.encoder_attn.out_proj\nReplaced layer: decoder.layers.1.fc1\nReplaced layer: decoder.layers.1.fc2\nReplaced layer: decoder.layers.2.self_attn.k_proj\nReplaced layer: decoder.layers.2.self_attn.v_proj\nReplaced layer: decoder.layers.2.self_attn.q_proj\nReplaced layer: decoder.layers.2.self_attn.out_proj\nReplaced layer: decoder.layers.2.encoder_attn.k_proj\nReplaced layer: decoder.layers.2.encoder_attn.v_proj\nReplaced layer: decoder.layers.2.encoder_attn.q_proj\nReplaced layer: decoder.layers.2.encoder_attn.out_proj\nReplaced layer: decoder.layers.2.fc1\nReplaced layer: decoder.layers.2.fc2\nReplaced layer: decoder.layers.3.self_attn.k_proj\nReplaced layer: decoder.layers.3.self_attn.v_proj\nReplaced layer: decoder.layers.3.self_attn.q_proj\nReplaced layer: decoder.layers.3.self_attn.out_proj\nReplaced layer: decoder.layers.3.encoder_attn.k_proj\nReplaced layer: decoder.layers.3.encoder_attn.v_proj\nReplaced layer: decoder.layers.3.encoder_attn.q_proj\nReplaced layer: decoder.layers.3.encoder_attn.out_proj\nReplaced layer: decoder.layers.3.fc1\nReplaced layer: decoder.layers.3.fc2\nReplaced layer: decoder.layers.4.self_attn.k_proj\nReplaced layer: decoder.layers.4.self_attn.v_proj\nReplaced layer: decoder.layers.4.self_attn.q_proj\nReplaced layer: decoder.layers.4.self_attn.out_proj\nReplaced layer: decoder.layers.4.encoder_attn.k_proj\nReplaced layer: decoder.layers.4.encoder_attn.v_proj\nReplaced layer: decoder.layers.4.encoder_attn.q_proj\nReplaced layer: decoder.layers.4.encoder_attn.out_proj\nReplaced layer: decoder.layers.4.fc1\nReplaced layer: decoder.layers.4.fc2\nReplaced layer: decoder.layers.5.self_attn.k_proj\nReplaced layer: decoder.layers.5.self_attn.v_proj\nReplaced layer: decoder.layers.5.self_attn.q_proj\nReplaced layer: decoder.layers.5.self_attn.out_proj\nReplaced layer: decoder.layers.5.encoder_attn.k_proj\nReplaced layer: decoder.layers.5.encoder_attn.v_proj\nReplaced layer: decoder.layers.5.encoder_attn.q_proj\nReplaced layer: decoder.layers.5.encoder_attn.out_proj\nReplaced layer: decoder.layers.5.fc1\n","output_type":"stream"},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","output_type":"stream"},{"name":"stdout","text":"Replaced layer: decoder.layers.5.fc2\nOriginal model size (parameters): 139420416\nCompressed model size (parameters): 46916352\nCompression rate: 66.35%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## T5 small 60% compress","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'google-t5/t5-small'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T11:41:15.150858Z","iopub.execute_input":"2024-07-01T11:41:15.151475Z","iopub.status.idle":"2024-07-01T11:41:33.218111Z","shell.execute_reply.started":"2024-07-01T11:41:15.151431Z","shell.execute_reply":"2024-07-01T11:41:33.217032Z"},"scrolled":true,"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"782f9e13e63b4faf902fd3c4a6c20ec1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a817e360ee4f708993d96352a37bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26158c9d51b4592a50f9596ac15f1d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf9b6129ec443e0b8c00b48c12fbd9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6717febe2fbc4a9094187c8d2714fd2e"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.block.0.layer.0.SelfAttention.q\nReplaced layer: encoder.block.0.layer.0.SelfAttention.k\nReplaced layer: encoder.block.0.layer.0.SelfAttention.v\nReplaced layer: encoder.block.0.layer.0.SelfAttention.o\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.1.layer.0.SelfAttention.q\nReplaced layer: encoder.block.1.layer.0.SelfAttention.k\nReplaced layer: encoder.block.1.layer.0.SelfAttention.v\nReplaced layer: encoder.block.1.layer.0.SelfAttention.o\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.2.layer.0.SelfAttention.q\nReplaced layer: encoder.block.2.layer.0.SelfAttention.k\nReplaced layer: encoder.block.2.layer.0.SelfAttention.v\nReplaced layer: encoder.block.2.layer.0.SelfAttention.o\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.3.layer.0.SelfAttention.q\nReplaced layer: encoder.block.3.layer.0.SelfAttention.k\nReplaced layer: encoder.block.3.layer.0.SelfAttention.v\nReplaced layer: encoder.block.3.layer.0.SelfAttention.o\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.4.layer.0.SelfAttention.q\nReplaced layer: encoder.block.4.layer.0.SelfAttention.k\nReplaced layer: encoder.block.4.layer.0.SelfAttention.v\nReplaced layer: encoder.block.4.layer.0.SelfAttention.o\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.5.layer.0.SelfAttention.q\nReplaced layer: encoder.block.5.layer.0.SelfAttention.k\nReplaced layer: encoder.block.5.layer.0.SelfAttention.v\nReplaced layer: encoder.block.5.layer.0.SelfAttention.o\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wo\nReplaced layer: decoder.block.0.layer.0.SelfAttention.q\nReplaced layer: decoder.block.0.layer.0.SelfAttention.k\nReplaced layer: decoder.block.0.layer.0.SelfAttention.v\nReplaced layer: decoder.block.0.layer.0.SelfAttention.o\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.1.layer.0.SelfAttention.q\nReplaced layer: decoder.block.1.layer.0.SelfAttention.k\nReplaced layer: decoder.block.1.layer.0.SelfAttention.v\nReplaced layer: decoder.block.1.layer.0.SelfAttention.o\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.2.layer.0.SelfAttention.q\nReplaced layer: decoder.block.2.layer.0.SelfAttention.k\nReplaced layer: decoder.block.2.layer.0.SelfAttention.v\nReplaced layer: decoder.block.2.layer.0.SelfAttention.o\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.3.layer.0.SelfAttention.q\nReplaced layer: decoder.block.3.layer.0.SelfAttention.k\nReplaced layer: decoder.block.3.layer.0.SelfAttention.v\nReplaced layer: decoder.block.3.layer.0.SelfAttention.o\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.4.layer.0.SelfAttention.q\nReplaced layer: decoder.block.4.layer.0.SelfAttention.k\nReplaced layer: decoder.block.4.layer.0.SelfAttention.v\nReplaced layer: decoder.block.4.layer.0.SelfAttention.o\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.5.layer.0.SelfAttention.q\nReplaced layer: decoder.block.5.layer.0.SelfAttention.k\nReplaced layer: decoder.block.5.layer.0.SelfAttention.v\nReplaced layer: decoder.block.5.layer.0.SelfAttention.o\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wo\nOriginal model size (parameters): 60506624\nCompressed model size (parameters): 20890112\nCompression rate: 65.47%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## T5 base 83.00%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'google-t5/t5-base'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T11:44:28.970228Z","iopub.execute_input":"2024-07-01T11:44:28.971032Z","iopub.status.idle":"2024-07-01T11:45:24.979675Z","shell.execute_reply.started":"2024-07-01T11:44:28.970983Z","shell.execute_reply":"2024-07-01T11:45:24.978496Z"},"scrolled":true,"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b415e84c6f1475dbf2fb59e3582c69c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258f9cd5dc2345b99ea478e115634210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9234ccd8d4a4129a2bbcf5d5a8580a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795ffe19a0e64b8a805e0f5919097268"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.block.0.layer.0.SelfAttention.q\nReplaced layer: encoder.block.0.layer.0.SelfAttention.k\nReplaced layer: encoder.block.0.layer.0.SelfAttention.v\nReplaced layer: encoder.block.0.layer.0.SelfAttention.o\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.1.layer.0.SelfAttention.q\nReplaced layer: encoder.block.1.layer.0.SelfAttention.k\nReplaced layer: encoder.block.1.layer.0.SelfAttention.v\nReplaced layer: encoder.block.1.layer.0.SelfAttention.o\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.2.layer.0.SelfAttention.q\nReplaced layer: encoder.block.2.layer.0.SelfAttention.k\nReplaced layer: encoder.block.2.layer.0.SelfAttention.v\nReplaced layer: encoder.block.2.layer.0.SelfAttention.o\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.3.layer.0.SelfAttention.q\nReplaced layer: encoder.block.3.layer.0.SelfAttention.k\nReplaced layer: encoder.block.3.layer.0.SelfAttention.v\nReplaced layer: encoder.block.3.layer.0.SelfAttention.o\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.4.layer.0.SelfAttention.q\nReplaced layer: encoder.block.4.layer.0.SelfAttention.k\nReplaced layer: encoder.block.4.layer.0.SelfAttention.v\nReplaced layer: encoder.block.4.layer.0.SelfAttention.o\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.5.layer.0.SelfAttention.q\nReplaced layer: encoder.block.5.layer.0.SelfAttention.k\nReplaced layer: encoder.block.5.layer.0.SelfAttention.v\nReplaced layer: encoder.block.5.layer.0.SelfAttention.o\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.6.layer.0.SelfAttention.q\nReplaced layer: encoder.block.6.layer.0.SelfAttention.k\nReplaced layer: encoder.block.6.layer.0.SelfAttention.v\nReplaced layer: encoder.block.6.layer.0.SelfAttention.o\nReplaced layer: encoder.block.6.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.6.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.7.layer.0.SelfAttention.q\nReplaced layer: encoder.block.7.layer.0.SelfAttention.k\nReplaced layer: encoder.block.7.layer.0.SelfAttention.v\nReplaced layer: encoder.block.7.layer.0.SelfAttention.o\nReplaced layer: encoder.block.7.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.7.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.8.layer.0.SelfAttention.q\nReplaced layer: encoder.block.8.layer.0.SelfAttention.k\nReplaced layer: encoder.block.8.layer.0.SelfAttention.v\nReplaced layer: encoder.block.8.layer.0.SelfAttention.o\nReplaced layer: encoder.block.8.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.8.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.9.layer.0.SelfAttention.q\nReplaced layer: encoder.block.9.layer.0.SelfAttention.k\nReplaced layer: encoder.block.9.layer.0.SelfAttention.v\nReplaced layer: encoder.block.9.layer.0.SelfAttention.o\nReplaced layer: encoder.block.9.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.9.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.10.layer.0.SelfAttention.q\nReplaced layer: encoder.block.10.layer.0.SelfAttention.k\nReplaced layer: encoder.block.10.layer.0.SelfAttention.v\nReplaced layer: encoder.block.10.layer.0.SelfAttention.o\nReplaced layer: encoder.block.10.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.10.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.11.layer.0.SelfAttention.q\nReplaced layer: encoder.block.11.layer.0.SelfAttention.k\nReplaced layer: encoder.block.11.layer.0.SelfAttention.v\nReplaced layer: encoder.block.11.layer.0.SelfAttention.o\nReplaced layer: encoder.block.11.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.11.layer.1.DenseReluDense.wo\nReplaced layer: decoder.block.0.layer.0.SelfAttention.q\nReplaced layer: decoder.block.0.layer.0.SelfAttention.k\nReplaced layer: decoder.block.0.layer.0.SelfAttention.v\nReplaced layer: decoder.block.0.layer.0.SelfAttention.o\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.1.layer.0.SelfAttention.q\nReplaced layer: decoder.block.1.layer.0.SelfAttention.k\nReplaced layer: decoder.block.1.layer.0.SelfAttention.v\nReplaced layer: decoder.block.1.layer.0.SelfAttention.o\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.2.layer.0.SelfAttention.q\nReplaced layer: decoder.block.2.layer.0.SelfAttention.k\nReplaced layer: decoder.block.2.layer.0.SelfAttention.v\nReplaced layer: decoder.block.2.layer.0.SelfAttention.o\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.3.layer.0.SelfAttention.q\nReplaced layer: decoder.block.3.layer.0.SelfAttention.k\nReplaced layer: decoder.block.3.layer.0.SelfAttention.v\nReplaced layer: decoder.block.3.layer.0.SelfAttention.o\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.4.layer.0.SelfAttention.q\nReplaced layer: decoder.block.4.layer.0.SelfAttention.k\nReplaced layer: decoder.block.4.layer.0.SelfAttention.v\nReplaced layer: decoder.block.4.layer.0.SelfAttention.o\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.5.layer.0.SelfAttention.q\nReplaced layer: decoder.block.5.layer.0.SelfAttention.k\nReplaced layer: decoder.block.5.layer.0.SelfAttention.v\nReplaced layer: decoder.block.5.layer.0.SelfAttention.o\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.6.layer.0.SelfAttention.q\nReplaced layer: decoder.block.6.layer.0.SelfAttention.k\nReplaced layer: decoder.block.6.layer.0.SelfAttention.v\nReplaced layer: decoder.block.6.layer.0.SelfAttention.o\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.6.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.6.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.7.layer.0.SelfAttention.q\nReplaced layer: decoder.block.7.layer.0.SelfAttention.k\nReplaced layer: decoder.block.7.layer.0.SelfAttention.v\nReplaced layer: decoder.block.7.layer.0.SelfAttention.o\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.7.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.7.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.8.layer.0.SelfAttention.q\nReplaced layer: decoder.block.8.layer.0.SelfAttention.k\nReplaced layer: decoder.block.8.layer.0.SelfAttention.v\nReplaced layer: decoder.block.8.layer.0.SelfAttention.o\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.8.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.8.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.9.layer.0.SelfAttention.q\nReplaced layer: decoder.block.9.layer.0.SelfAttention.k\nReplaced layer: decoder.block.9.layer.0.SelfAttention.v\nReplaced layer: decoder.block.9.layer.0.SelfAttention.o\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.9.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.9.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.10.layer.0.SelfAttention.q\nReplaced layer: decoder.block.10.layer.0.SelfAttention.k\nReplaced layer: decoder.block.10.layer.0.SelfAttention.v\nReplaced layer: decoder.block.10.layer.0.SelfAttention.o\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.10.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.10.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.11.layer.0.SelfAttention.q\nReplaced layer: decoder.block.11.layer.0.SelfAttention.k\nReplaced layer: decoder.block.11.layer.0.SelfAttention.v\nReplaced layer: decoder.block.11.layer.0.SelfAttention.o\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.11.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.11.layer.2.DenseReluDense.wo\nOriginal model size (parameters): 222903552\nCompressed model size (parameters): 37895424\nCompression rate: 83.00%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## T5 3b 95%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'google-t5/t5-3b'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T11:47:14.868864Z","iopub.execute_input":"2024-07-01T11:47:14.869458Z","iopub.status.idle":"2024-07-01T12:16:15.975468Z","shell.execute_reply.started":"2024-07-01T11:47:14.869409Z","shell.execute_reply":"2024-07-01T12:16:15.973229Z"},"scrolled":true,"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f32b078991a64f86958a5e549f77a5e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c622e8464da14e9d855200c0f01be6f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c12886c21c4a8da4acc377a0af5699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/11.4G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"076d5f632e0d44d6aa471bc4d1872fa1"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.block.0.layer.0.SelfAttention.q\nReplaced layer: encoder.block.0.layer.0.SelfAttention.k\nReplaced layer: encoder.block.0.layer.0.SelfAttention.v\nReplaced layer: encoder.block.0.layer.0.SelfAttention.o\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.1.layer.0.SelfAttention.q\nReplaced layer: encoder.block.1.layer.0.SelfAttention.k\nReplaced layer: encoder.block.1.layer.0.SelfAttention.v\nReplaced layer: encoder.block.1.layer.0.SelfAttention.o\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.2.layer.0.SelfAttention.q\nReplaced layer: encoder.block.2.layer.0.SelfAttention.k\nReplaced layer: encoder.block.2.layer.0.SelfAttention.v\nReplaced layer: encoder.block.2.layer.0.SelfAttention.o\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.3.layer.0.SelfAttention.q\nReplaced layer: encoder.block.3.layer.0.SelfAttention.k\nReplaced layer: encoder.block.3.layer.0.SelfAttention.v\nReplaced layer: encoder.block.3.layer.0.SelfAttention.o\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.4.layer.0.SelfAttention.q\nReplaced layer: encoder.block.4.layer.0.SelfAttention.k\nReplaced layer: encoder.block.4.layer.0.SelfAttention.v\nReplaced layer: encoder.block.4.layer.0.SelfAttention.o\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.5.layer.0.SelfAttention.q\nReplaced layer: encoder.block.5.layer.0.SelfAttention.k\nReplaced layer: encoder.block.5.layer.0.SelfAttention.v\nReplaced layer: encoder.block.5.layer.0.SelfAttention.o\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.6.layer.0.SelfAttention.q\nReplaced layer: encoder.block.6.layer.0.SelfAttention.k\nReplaced layer: encoder.block.6.layer.0.SelfAttention.v\nReplaced layer: encoder.block.6.layer.0.SelfAttention.o\nReplaced layer: encoder.block.6.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.6.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.7.layer.0.SelfAttention.q\nReplaced layer: encoder.block.7.layer.0.SelfAttention.k\nReplaced layer: encoder.block.7.layer.0.SelfAttention.v\nReplaced layer: encoder.block.7.layer.0.SelfAttention.o\nReplaced layer: encoder.block.7.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.7.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.8.layer.0.SelfAttention.q\nReplaced layer: encoder.block.8.layer.0.SelfAttention.k\nReplaced layer: encoder.block.8.layer.0.SelfAttention.v\nReplaced layer: encoder.block.8.layer.0.SelfAttention.o\nReplaced layer: encoder.block.8.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.8.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.9.layer.0.SelfAttention.q\nReplaced layer: encoder.block.9.layer.0.SelfAttention.k\nReplaced layer: encoder.block.9.layer.0.SelfAttention.v\nReplaced layer: encoder.block.9.layer.0.SelfAttention.o\nReplaced layer: encoder.block.9.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.9.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.10.layer.0.SelfAttention.q\nReplaced layer: encoder.block.10.layer.0.SelfAttention.k\nReplaced layer: encoder.block.10.layer.0.SelfAttention.v\nReplaced layer: encoder.block.10.layer.0.SelfAttention.o\nReplaced layer: encoder.block.10.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.10.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.11.layer.0.SelfAttention.q\nReplaced layer: encoder.block.11.layer.0.SelfAttention.k\nReplaced layer: encoder.block.11.layer.0.SelfAttention.v\nReplaced layer: encoder.block.11.layer.0.SelfAttention.o\nReplaced layer: encoder.block.11.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.11.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.12.layer.0.SelfAttention.q\nReplaced layer: encoder.block.12.layer.0.SelfAttention.k\nReplaced layer: encoder.block.12.layer.0.SelfAttention.v\nReplaced layer: encoder.block.12.layer.0.SelfAttention.o\nReplaced layer: encoder.block.12.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.12.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.13.layer.0.SelfAttention.q\nReplaced layer: encoder.block.13.layer.0.SelfAttention.k\nReplaced layer: encoder.block.13.layer.0.SelfAttention.v\nReplaced layer: encoder.block.13.layer.0.SelfAttention.o\nReplaced layer: encoder.block.13.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.13.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.14.layer.0.SelfAttention.q\nReplaced layer: encoder.block.14.layer.0.SelfAttention.k\nReplaced layer: encoder.block.14.layer.0.SelfAttention.v\nReplaced layer: encoder.block.14.layer.0.SelfAttention.o\nReplaced layer: encoder.block.14.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.14.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.15.layer.0.SelfAttention.q\nReplaced layer: encoder.block.15.layer.0.SelfAttention.k\nReplaced layer: encoder.block.15.layer.0.SelfAttention.v\nReplaced layer: encoder.block.15.layer.0.SelfAttention.o\nReplaced layer: encoder.block.15.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.15.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.16.layer.0.SelfAttention.q\nReplaced layer: encoder.block.16.layer.0.SelfAttention.k\nReplaced layer: encoder.block.16.layer.0.SelfAttention.v\nReplaced layer: encoder.block.16.layer.0.SelfAttention.o\nReplaced layer: encoder.block.16.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.16.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.17.layer.0.SelfAttention.q\nReplaced layer: encoder.block.17.layer.0.SelfAttention.k\nReplaced layer: encoder.block.17.layer.0.SelfAttention.v\nReplaced layer: encoder.block.17.layer.0.SelfAttention.o\nReplaced layer: encoder.block.17.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.17.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.18.layer.0.SelfAttention.q\nReplaced layer: encoder.block.18.layer.0.SelfAttention.k\nReplaced layer: encoder.block.18.layer.0.SelfAttention.v\nReplaced layer: encoder.block.18.layer.0.SelfAttention.o\nReplaced layer: encoder.block.18.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.18.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.19.layer.0.SelfAttention.q\nReplaced layer: encoder.block.19.layer.0.SelfAttention.k\nReplaced layer: encoder.block.19.layer.0.SelfAttention.v\nReplaced layer: encoder.block.19.layer.0.SelfAttention.o\nReplaced layer: encoder.block.19.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.19.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.20.layer.0.SelfAttention.q\nReplaced layer: encoder.block.20.layer.0.SelfAttention.k\nReplaced layer: encoder.block.20.layer.0.SelfAttention.v\nReplaced layer: encoder.block.20.layer.0.SelfAttention.o\nReplaced layer: encoder.block.20.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.20.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.21.layer.0.SelfAttention.q\nReplaced layer: encoder.block.21.layer.0.SelfAttention.k\nReplaced layer: encoder.block.21.layer.0.SelfAttention.v\nReplaced layer: encoder.block.21.layer.0.SelfAttention.o\nReplaced layer: encoder.block.21.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.21.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.22.layer.0.SelfAttention.q\nReplaced layer: encoder.block.22.layer.0.SelfAttention.k\nReplaced layer: encoder.block.22.layer.0.SelfAttention.v\nReplaced layer: encoder.block.22.layer.0.SelfAttention.o\nReplaced layer: encoder.block.22.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.22.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.23.layer.0.SelfAttention.q\nReplaced layer: encoder.block.23.layer.0.SelfAttention.k\nReplaced layer: encoder.block.23.layer.0.SelfAttention.v\nReplaced layer: encoder.block.23.layer.0.SelfAttention.o\nReplaced layer: encoder.block.23.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.23.layer.1.DenseReluDense.wo\nReplaced layer: decoder.block.0.layer.0.SelfAttention.q\nReplaced layer: decoder.block.0.layer.0.SelfAttention.k\nReplaced layer: decoder.block.0.layer.0.SelfAttention.v\nReplaced layer: decoder.block.0.layer.0.SelfAttention.o\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.1.layer.0.SelfAttention.q\nReplaced layer: decoder.block.1.layer.0.SelfAttention.k\nReplaced layer: decoder.block.1.layer.0.SelfAttention.v\nReplaced layer: decoder.block.1.layer.0.SelfAttention.o\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.2.layer.0.SelfAttention.q\nReplaced layer: decoder.block.2.layer.0.SelfAttention.k\nReplaced layer: decoder.block.2.layer.0.SelfAttention.v\nReplaced layer: decoder.block.2.layer.0.SelfAttention.o\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.3.layer.0.SelfAttention.q\nReplaced layer: decoder.block.3.layer.0.SelfAttention.k\nReplaced layer: decoder.block.3.layer.0.SelfAttention.v\nReplaced layer: decoder.block.3.layer.0.SelfAttention.o\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.4.layer.0.SelfAttention.q\nReplaced layer: decoder.block.4.layer.0.SelfAttention.k\nReplaced layer: decoder.block.4.layer.0.SelfAttention.v\nReplaced layer: decoder.block.4.layer.0.SelfAttention.o\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.5.layer.0.SelfAttention.q\nReplaced layer: decoder.block.5.layer.0.SelfAttention.k\nReplaced layer: decoder.block.5.layer.0.SelfAttention.v\nReplaced layer: decoder.block.5.layer.0.SelfAttention.o\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.6.layer.0.SelfAttention.q\nReplaced layer: decoder.block.6.layer.0.SelfAttention.k\nReplaced layer: decoder.block.6.layer.0.SelfAttention.v\nReplaced layer: decoder.block.6.layer.0.SelfAttention.o\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.6.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.6.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.7.layer.0.SelfAttention.q\nReplaced layer: decoder.block.7.layer.0.SelfAttention.k\nReplaced layer: decoder.block.7.layer.0.SelfAttention.v\nReplaced layer: decoder.block.7.layer.0.SelfAttention.o\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.7.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.7.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.8.layer.0.SelfAttention.q\nReplaced layer: decoder.block.8.layer.0.SelfAttention.k\nReplaced layer: decoder.block.8.layer.0.SelfAttention.v\nReplaced layer: decoder.block.8.layer.0.SelfAttention.o\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.8.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.8.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.9.layer.0.SelfAttention.q\nReplaced layer: decoder.block.9.layer.0.SelfAttention.k\nReplaced layer: decoder.block.9.layer.0.SelfAttention.v\nReplaced layer: decoder.block.9.layer.0.SelfAttention.o\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.9.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.9.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.10.layer.0.SelfAttention.q\nReplaced layer: decoder.block.10.layer.0.SelfAttention.k\nReplaced layer: decoder.block.10.layer.0.SelfAttention.v\nReplaced layer: decoder.block.10.layer.0.SelfAttention.o\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.10.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.10.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.11.layer.0.SelfAttention.q\nReplaced layer: decoder.block.11.layer.0.SelfAttention.k\nReplaced layer: decoder.block.11.layer.0.SelfAttention.v\nReplaced layer: decoder.block.11.layer.0.SelfAttention.o\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.11.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.11.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.12.layer.0.SelfAttention.q\nReplaced layer: decoder.block.12.layer.0.SelfAttention.k\nReplaced layer: decoder.block.12.layer.0.SelfAttention.v\nReplaced layer: decoder.block.12.layer.0.SelfAttention.o\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.12.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.12.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.13.layer.0.SelfAttention.q\nReplaced layer: decoder.block.13.layer.0.SelfAttention.k\nReplaced layer: decoder.block.13.layer.0.SelfAttention.v\nReplaced layer: decoder.block.13.layer.0.SelfAttention.o\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.13.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.13.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.14.layer.0.SelfAttention.q\nReplaced layer: decoder.block.14.layer.0.SelfAttention.k\nReplaced layer: decoder.block.14.layer.0.SelfAttention.v\nReplaced layer: decoder.block.14.layer.0.SelfAttention.o\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.14.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.14.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.15.layer.0.SelfAttention.q\nReplaced layer: decoder.block.15.layer.0.SelfAttention.k\nReplaced layer: decoder.block.15.layer.0.SelfAttention.v\nReplaced layer: decoder.block.15.layer.0.SelfAttention.o\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.15.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.15.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.16.layer.0.SelfAttention.q\nReplaced layer: decoder.block.16.layer.0.SelfAttention.k\nReplaced layer: decoder.block.16.layer.0.SelfAttention.v\nReplaced layer: decoder.block.16.layer.0.SelfAttention.o\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.16.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.16.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.17.layer.0.SelfAttention.q\nReplaced layer: decoder.block.17.layer.0.SelfAttention.k\nReplaced layer: decoder.block.17.layer.0.SelfAttention.v\nReplaced layer: decoder.block.17.layer.0.SelfAttention.o\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.17.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.17.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.18.layer.0.SelfAttention.q\nReplaced layer: decoder.block.18.layer.0.SelfAttention.k\nReplaced layer: decoder.block.18.layer.0.SelfAttention.v\nReplaced layer: decoder.block.18.layer.0.SelfAttention.o\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.18.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.18.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.19.layer.0.SelfAttention.q\nReplaced layer: decoder.block.19.layer.0.SelfAttention.k\nReplaced layer: decoder.block.19.layer.0.SelfAttention.v\nReplaced layer: decoder.block.19.layer.0.SelfAttention.o\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.19.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.19.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.20.layer.0.SelfAttention.q\nReplaced layer: decoder.block.20.layer.0.SelfAttention.k\nReplaced layer: decoder.block.20.layer.0.SelfAttention.v\nReplaced layer: decoder.block.20.layer.0.SelfAttention.o\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.20.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.20.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.21.layer.0.SelfAttention.q\nReplaced layer: decoder.block.21.layer.0.SelfAttention.k\nReplaced layer: decoder.block.21.layer.0.SelfAttention.v\nReplaced layer: decoder.block.21.layer.0.SelfAttention.o\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.21.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.21.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.22.layer.0.SelfAttention.q\nReplaced layer: decoder.block.22.layer.0.SelfAttention.k\nReplaced layer: decoder.block.22.layer.0.SelfAttention.v\nReplaced layer: decoder.block.22.layer.0.SelfAttention.o\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.22.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.22.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.23.layer.0.SelfAttention.q\nReplaced layer: decoder.block.23.layer.0.SelfAttention.k\nReplaced layer: decoder.block.23.layer.0.SelfAttention.v\nReplaced layer: decoder.block.23.layer.0.SelfAttention.o\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.23.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.23.layer.2.DenseReluDense.wo\nOriginal model size (parameters): 2851598336\nCompressed model size (parameters): 134082560\nCompression rate: 95.30%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## T5 large 90%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'google-t5/t5-large'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:19:55.862143Z","iopub.execute_input":"2024-07-01T12:19:55.862628Z","iopub.status.idle":"2024-07-01T12:24:06.879053Z","shell.execute_reply.started":"2024-07-01T12:19:55.862588Z","shell.execute_reply":"2024-07-01T12:24:06.877955Z"},"scrolled":true,"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de00f79b6f634ac0a3b7a536700ee70d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c71c11b42c46049169d7aa961e2269"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b4ce667e234d178aa1e195a0d1ef02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7094e6538094751b20ec1d616a813ca"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.block.0.layer.0.SelfAttention.q\nReplaced layer: encoder.block.0.layer.0.SelfAttention.k\nReplaced layer: encoder.block.0.layer.0.SelfAttention.v\nReplaced layer: encoder.block.0.layer.0.SelfAttention.o\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.1.layer.0.SelfAttention.q\nReplaced layer: encoder.block.1.layer.0.SelfAttention.k\nReplaced layer: encoder.block.1.layer.0.SelfAttention.v\nReplaced layer: encoder.block.1.layer.0.SelfAttention.o\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.2.layer.0.SelfAttention.q\nReplaced layer: encoder.block.2.layer.0.SelfAttention.k\nReplaced layer: encoder.block.2.layer.0.SelfAttention.v\nReplaced layer: encoder.block.2.layer.0.SelfAttention.o\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.3.layer.0.SelfAttention.q\nReplaced layer: encoder.block.3.layer.0.SelfAttention.k\nReplaced layer: encoder.block.3.layer.0.SelfAttention.v\nReplaced layer: encoder.block.3.layer.0.SelfAttention.o\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.4.layer.0.SelfAttention.q\nReplaced layer: encoder.block.4.layer.0.SelfAttention.k\nReplaced layer: encoder.block.4.layer.0.SelfAttention.v\nReplaced layer: encoder.block.4.layer.0.SelfAttention.o\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.5.layer.0.SelfAttention.q\nReplaced layer: encoder.block.5.layer.0.SelfAttention.k\nReplaced layer: encoder.block.5.layer.0.SelfAttention.v\nReplaced layer: encoder.block.5.layer.0.SelfAttention.o\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.6.layer.0.SelfAttention.q\nReplaced layer: encoder.block.6.layer.0.SelfAttention.k\nReplaced layer: encoder.block.6.layer.0.SelfAttention.v\nReplaced layer: encoder.block.6.layer.0.SelfAttention.o\nReplaced layer: encoder.block.6.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.6.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.7.layer.0.SelfAttention.q\nReplaced layer: encoder.block.7.layer.0.SelfAttention.k\nReplaced layer: encoder.block.7.layer.0.SelfAttention.v\nReplaced layer: encoder.block.7.layer.0.SelfAttention.o\nReplaced layer: encoder.block.7.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.7.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.8.layer.0.SelfAttention.q\nReplaced layer: encoder.block.8.layer.0.SelfAttention.k\nReplaced layer: encoder.block.8.layer.0.SelfAttention.v\nReplaced layer: encoder.block.8.layer.0.SelfAttention.o\nReplaced layer: encoder.block.8.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.8.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.9.layer.0.SelfAttention.q\nReplaced layer: encoder.block.9.layer.0.SelfAttention.k\nReplaced layer: encoder.block.9.layer.0.SelfAttention.v\nReplaced layer: encoder.block.9.layer.0.SelfAttention.o\nReplaced layer: encoder.block.9.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.9.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.10.layer.0.SelfAttention.q\nReplaced layer: encoder.block.10.layer.0.SelfAttention.k\nReplaced layer: encoder.block.10.layer.0.SelfAttention.v\nReplaced layer: encoder.block.10.layer.0.SelfAttention.o\nReplaced layer: encoder.block.10.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.10.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.11.layer.0.SelfAttention.q\nReplaced layer: encoder.block.11.layer.0.SelfAttention.k\nReplaced layer: encoder.block.11.layer.0.SelfAttention.v\nReplaced layer: encoder.block.11.layer.0.SelfAttention.o\nReplaced layer: encoder.block.11.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.11.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.12.layer.0.SelfAttention.q\nReplaced layer: encoder.block.12.layer.0.SelfAttention.k\nReplaced layer: encoder.block.12.layer.0.SelfAttention.v\nReplaced layer: encoder.block.12.layer.0.SelfAttention.o\nReplaced layer: encoder.block.12.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.12.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.13.layer.0.SelfAttention.q\nReplaced layer: encoder.block.13.layer.0.SelfAttention.k\nReplaced layer: encoder.block.13.layer.0.SelfAttention.v\nReplaced layer: encoder.block.13.layer.0.SelfAttention.o\nReplaced layer: encoder.block.13.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.13.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.14.layer.0.SelfAttention.q\nReplaced layer: encoder.block.14.layer.0.SelfAttention.k\nReplaced layer: encoder.block.14.layer.0.SelfAttention.v\nReplaced layer: encoder.block.14.layer.0.SelfAttention.o\nReplaced layer: encoder.block.14.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.14.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.15.layer.0.SelfAttention.q\nReplaced layer: encoder.block.15.layer.0.SelfAttention.k\nReplaced layer: encoder.block.15.layer.0.SelfAttention.v\nReplaced layer: encoder.block.15.layer.0.SelfAttention.o\nReplaced layer: encoder.block.15.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.15.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.16.layer.0.SelfAttention.q\nReplaced layer: encoder.block.16.layer.0.SelfAttention.k\nReplaced layer: encoder.block.16.layer.0.SelfAttention.v\nReplaced layer: encoder.block.16.layer.0.SelfAttention.o\nReplaced layer: encoder.block.16.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.16.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.17.layer.0.SelfAttention.q\nReplaced layer: encoder.block.17.layer.0.SelfAttention.k\nReplaced layer: encoder.block.17.layer.0.SelfAttention.v\nReplaced layer: encoder.block.17.layer.0.SelfAttention.o\nReplaced layer: encoder.block.17.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.17.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.18.layer.0.SelfAttention.q\nReplaced layer: encoder.block.18.layer.0.SelfAttention.k\nReplaced layer: encoder.block.18.layer.0.SelfAttention.v\nReplaced layer: encoder.block.18.layer.0.SelfAttention.o\nReplaced layer: encoder.block.18.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.18.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.19.layer.0.SelfAttention.q\nReplaced layer: encoder.block.19.layer.0.SelfAttention.k\nReplaced layer: encoder.block.19.layer.0.SelfAttention.v\nReplaced layer: encoder.block.19.layer.0.SelfAttention.o\nReplaced layer: encoder.block.19.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.19.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.20.layer.0.SelfAttention.q\nReplaced layer: encoder.block.20.layer.0.SelfAttention.k\nReplaced layer: encoder.block.20.layer.0.SelfAttention.v\nReplaced layer: encoder.block.20.layer.0.SelfAttention.o\nReplaced layer: encoder.block.20.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.20.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.21.layer.0.SelfAttention.q\nReplaced layer: encoder.block.21.layer.0.SelfAttention.k\nReplaced layer: encoder.block.21.layer.0.SelfAttention.v\nReplaced layer: encoder.block.21.layer.0.SelfAttention.o\nReplaced layer: encoder.block.21.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.21.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.22.layer.0.SelfAttention.q\nReplaced layer: encoder.block.22.layer.0.SelfAttention.k\nReplaced layer: encoder.block.22.layer.0.SelfAttention.v\nReplaced layer: encoder.block.22.layer.0.SelfAttention.o\nReplaced layer: encoder.block.22.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.22.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.23.layer.0.SelfAttention.q\nReplaced layer: encoder.block.23.layer.0.SelfAttention.k\nReplaced layer: encoder.block.23.layer.0.SelfAttention.v\nReplaced layer: encoder.block.23.layer.0.SelfAttention.o\nReplaced layer: encoder.block.23.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.23.layer.1.DenseReluDense.wo\nReplaced layer: decoder.block.0.layer.0.SelfAttention.q\nReplaced layer: decoder.block.0.layer.0.SelfAttention.k\nReplaced layer: decoder.block.0.layer.0.SelfAttention.v\nReplaced layer: decoder.block.0.layer.0.SelfAttention.o\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.1.layer.0.SelfAttention.q\nReplaced layer: decoder.block.1.layer.0.SelfAttention.k\nReplaced layer: decoder.block.1.layer.0.SelfAttention.v\nReplaced layer: decoder.block.1.layer.0.SelfAttention.o\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.2.layer.0.SelfAttention.q\nReplaced layer: decoder.block.2.layer.0.SelfAttention.k\nReplaced layer: decoder.block.2.layer.0.SelfAttention.v\nReplaced layer: decoder.block.2.layer.0.SelfAttention.o\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.3.layer.0.SelfAttention.q\nReplaced layer: decoder.block.3.layer.0.SelfAttention.k\nReplaced layer: decoder.block.3.layer.0.SelfAttention.v\nReplaced layer: decoder.block.3.layer.0.SelfAttention.o\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.4.layer.0.SelfAttention.q\nReplaced layer: decoder.block.4.layer.0.SelfAttention.k\nReplaced layer: decoder.block.4.layer.0.SelfAttention.v\nReplaced layer: decoder.block.4.layer.0.SelfAttention.o\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.5.layer.0.SelfAttention.q\nReplaced layer: decoder.block.5.layer.0.SelfAttention.k\nReplaced layer: decoder.block.5.layer.0.SelfAttention.v\nReplaced layer: decoder.block.5.layer.0.SelfAttention.o\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.6.layer.0.SelfAttention.q\nReplaced layer: decoder.block.6.layer.0.SelfAttention.k\nReplaced layer: decoder.block.6.layer.0.SelfAttention.v\nReplaced layer: decoder.block.6.layer.0.SelfAttention.o\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.6.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.6.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.6.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.7.layer.0.SelfAttention.q\nReplaced layer: decoder.block.7.layer.0.SelfAttention.k\nReplaced layer: decoder.block.7.layer.0.SelfAttention.v\nReplaced layer: decoder.block.7.layer.0.SelfAttention.o\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.7.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.7.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.7.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.8.layer.0.SelfAttention.q\nReplaced layer: decoder.block.8.layer.0.SelfAttention.k\nReplaced layer: decoder.block.8.layer.0.SelfAttention.v\nReplaced layer: decoder.block.8.layer.0.SelfAttention.o\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.8.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.8.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.8.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.9.layer.0.SelfAttention.q\nReplaced layer: decoder.block.9.layer.0.SelfAttention.k\nReplaced layer: decoder.block.9.layer.0.SelfAttention.v\nReplaced layer: decoder.block.9.layer.0.SelfAttention.o\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.9.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.9.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.9.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.10.layer.0.SelfAttention.q\nReplaced layer: decoder.block.10.layer.0.SelfAttention.k\nReplaced layer: decoder.block.10.layer.0.SelfAttention.v\nReplaced layer: decoder.block.10.layer.0.SelfAttention.o\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.10.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.10.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.10.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.11.layer.0.SelfAttention.q\nReplaced layer: decoder.block.11.layer.0.SelfAttention.k\nReplaced layer: decoder.block.11.layer.0.SelfAttention.v\nReplaced layer: decoder.block.11.layer.0.SelfAttention.o\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.11.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.11.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.11.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.12.layer.0.SelfAttention.q\nReplaced layer: decoder.block.12.layer.0.SelfAttention.k\nReplaced layer: decoder.block.12.layer.0.SelfAttention.v\nReplaced layer: decoder.block.12.layer.0.SelfAttention.o\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.12.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.12.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.12.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.13.layer.0.SelfAttention.q\nReplaced layer: decoder.block.13.layer.0.SelfAttention.k\nReplaced layer: decoder.block.13.layer.0.SelfAttention.v\nReplaced layer: decoder.block.13.layer.0.SelfAttention.o\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.13.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.13.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.13.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.14.layer.0.SelfAttention.q\nReplaced layer: decoder.block.14.layer.0.SelfAttention.k\nReplaced layer: decoder.block.14.layer.0.SelfAttention.v\nReplaced layer: decoder.block.14.layer.0.SelfAttention.o\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.14.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.14.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.14.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.15.layer.0.SelfAttention.q\nReplaced layer: decoder.block.15.layer.0.SelfAttention.k\nReplaced layer: decoder.block.15.layer.0.SelfAttention.v\nReplaced layer: decoder.block.15.layer.0.SelfAttention.o\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.15.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.15.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.15.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.16.layer.0.SelfAttention.q\nReplaced layer: decoder.block.16.layer.0.SelfAttention.k\nReplaced layer: decoder.block.16.layer.0.SelfAttention.v\nReplaced layer: decoder.block.16.layer.0.SelfAttention.o\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.16.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.16.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.16.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.17.layer.0.SelfAttention.q\nReplaced layer: decoder.block.17.layer.0.SelfAttention.k\nReplaced layer: decoder.block.17.layer.0.SelfAttention.v\nReplaced layer: decoder.block.17.layer.0.SelfAttention.o\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.17.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.17.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.17.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.18.layer.0.SelfAttention.q\nReplaced layer: decoder.block.18.layer.0.SelfAttention.k\nReplaced layer: decoder.block.18.layer.0.SelfAttention.v\nReplaced layer: decoder.block.18.layer.0.SelfAttention.o\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.18.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.18.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.18.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.19.layer.0.SelfAttention.q\nReplaced layer: decoder.block.19.layer.0.SelfAttention.k\nReplaced layer: decoder.block.19.layer.0.SelfAttention.v\nReplaced layer: decoder.block.19.layer.0.SelfAttention.o\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.19.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.19.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.19.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.20.layer.0.SelfAttention.q\nReplaced layer: decoder.block.20.layer.0.SelfAttention.k\nReplaced layer: decoder.block.20.layer.0.SelfAttention.v\nReplaced layer: decoder.block.20.layer.0.SelfAttention.o\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.20.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.20.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.20.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.21.layer.0.SelfAttention.q\nReplaced layer: decoder.block.21.layer.0.SelfAttention.k\nReplaced layer: decoder.block.21.layer.0.SelfAttention.v\nReplaced layer: decoder.block.21.layer.0.SelfAttention.o\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.21.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.21.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.21.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.22.layer.0.SelfAttention.q\nReplaced layer: decoder.block.22.layer.0.SelfAttention.k\nReplaced layer: decoder.block.22.layer.0.SelfAttention.v\nReplaced layer: decoder.block.22.layer.0.SelfAttention.o\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.22.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.22.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.22.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.23.layer.0.SelfAttention.q\nReplaced layer: decoder.block.23.layer.0.SelfAttention.k\nReplaced layer: decoder.block.23.layer.0.SelfAttention.v\nReplaced layer: decoder.block.23.layer.0.SelfAttention.o\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.23.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.23.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.23.layer.2.DenseReluDense.wo\nOriginal model size (parameters): 737668096\nCompressed model size (parameters): 68021248\nCompression rate: 90.78%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## funnel_transformer smallbase 68%","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'funnel-transformer/small-base'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:51:41.012658Z","iopub.execute_input":"2024-07-01T08:51:41.013235Z","iopub.status.idle":"2024-07-01T08:52:12.594977Z","shell.execute_reply.started":"2024-07-01T08:51:41.013204Z","shell.execute_reply":"2024-07-01T08:52:12.593982Z"},"scrolled":true,"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce070c4aac04ef3acb819938ee9a9ce"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc16158c608c40fa96cd1a0e14ccaedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/231k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"934adfecff9f4ec8af7c5aebc501efcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"186b2429a1a148c5816a0b564c644329"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/153 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701b62ddbe384b0b860c5a577efcf4f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/462M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334ddbbbadd24b71b4708c58e5d19afa"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.blocks.0.0.attention.q_head\nReplaced layer: encoder.blocks.0.0.attention.k_head\nReplaced layer: encoder.blocks.0.0.attention.v_head\nReplaced layer: encoder.blocks.0.0.attention.post_proj\nReplaced layer: encoder.blocks.0.0.ffn.linear_1\nReplaced layer: encoder.blocks.0.0.ffn.linear_2\nReplaced layer: encoder.blocks.0.1.attention.q_head\nReplaced layer: encoder.blocks.0.1.attention.k_head\nReplaced layer: encoder.blocks.0.1.attention.v_head\nReplaced layer: encoder.blocks.0.1.attention.post_proj\nReplaced layer: encoder.blocks.0.1.ffn.linear_1\nReplaced layer: encoder.blocks.0.1.ffn.linear_2\nReplaced layer: encoder.blocks.0.2.attention.q_head\nReplaced layer: encoder.blocks.0.2.attention.k_head\nReplaced layer: encoder.blocks.0.2.attention.v_head\nReplaced layer: encoder.blocks.0.2.attention.post_proj\nReplaced layer: encoder.blocks.0.2.ffn.linear_1\nReplaced layer: encoder.blocks.0.2.ffn.linear_2\nReplaced layer: encoder.blocks.0.3.attention.q_head\nReplaced layer: encoder.blocks.0.3.attention.k_head\nReplaced layer: encoder.blocks.0.3.attention.v_head\nReplaced layer: encoder.blocks.0.3.attention.post_proj\nReplaced layer: encoder.blocks.0.3.ffn.linear_1\nReplaced layer: encoder.blocks.0.3.ffn.linear_2\nReplaced layer: encoder.blocks.1.0.attention.q_head\nReplaced layer: encoder.blocks.1.0.attention.k_head\nReplaced layer: encoder.blocks.1.0.attention.v_head\nReplaced layer: encoder.blocks.1.0.attention.post_proj\nReplaced layer: encoder.blocks.1.0.ffn.linear_1\nReplaced layer: encoder.blocks.1.0.ffn.linear_2\nReplaced layer: encoder.blocks.1.1.attention.q_head\nReplaced layer: encoder.blocks.1.1.attention.k_head\nReplaced layer: encoder.blocks.1.1.attention.v_head\nReplaced layer: encoder.blocks.1.1.attention.post_proj\nReplaced layer: encoder.blocks.1.1.ffn.linear_1\nReplaced layer: encoder.blocks.1.1.ffn.linear_2\nReplaced layer: encoder.blocks.1.2.attention.q_head\nReplaced layer: encoder.blocks.1.2.attention.k_head\nReplaced layer: encoder.blocks.1.2.attention.v_head\nReplaced layer: encoder.blocks.1.2.attention.post_proj\nReplaced layer: encoder.blocks.1.2.ffn.linear_1\nReplaced layer: encoder.blocks.1.2.ffn.linear_2\nReplaced layer: encoder.blocks.1.3.attention.q_head\nReplaced layer: encoder.blocks.1.3.attention.k_head\nReplaced layer: encoder.blocks.1.3.attention.v_head\nReplaced layer: encoder.blocks.1.3.attention.post_proj\nReplaced layer: encoder.blocks.1.3.ffn.linear_1\nReplaced layer: encoder.blocks.1.3.ffn.linear_2\nReplaced layer: encoder.blocks.2.0.attention.q_head\nReplaced layer: encoder.blocks.2.0.attention.k_head\nReplaced layer: encoder.blocks.2.0.attention.v_head\nReplaced layer: encoder.blocks.2.0.attention.post_proj\nReplaced layer: encoder.blocks.2.0.ffn.linear_1\nReplaced layer: encoder.blocks.2.0.ffn.linear_2\nReplaced layer: encoder.blocks.2.1.attention.q_head\nReplaced layer: encoder.blocks.2.1.attention.k_head\nReplaced layer: encoder.blocks.2.1.attention.v_head\nReplaced layer: encoder.blocks.2.1.attention.post_proj\nReplaced layer: encoder.blocks.2.1.ffn.linear_1\nReplaced layer: encoder.blocks.2.1.ffn.linear_2\nReplaced layer: encoder.blocks.2.2.attention.q_head\nReplaced layer: encoder.blocks.2.2.attention.k_head\nReplaced layer: encoder.blocks.2.2.attention.v_head\nReplaced layer: encoder.blocks.2.2.attention.post_proj\nReplaced layer: encoder.blocks.2.2.ffn.linear_1\nReplaced layer: encoder.blocks.2.2.ffn.linear_2\nReplaced layer: encoder.blocks.2.3.attention.q_head\nReplaced layer: encoder.blocks.2.3.attention.k_head\nReplaced layer: encoder.blocks.2.3.attention.v_head\nReplaced layer: encoder.blocks.2.3.attention.post_proj\nReplaced layer: encoder.blocks.2.3.ffn.linear_1\nReplaced layer: encoder.blocks.2.3.ffn.linear_2\nOriginal model size (parameters): 115611648\nCompressed model size (parameters): 36059136\nCompression rate: 68.81%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:53:21.743850Z","iopub.execute_input":"2024-07-01T08:53:21.744645Z","iopub.status.idle":"2024-07-01T08:53:25.595661Z","shell.execute_reply.started":"2024-07-01T08:53:21.744589Z","shell.execute_reply":"2024-07-01T08:53:25.594290Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c699645d15a4a4abb3bb398df24e876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97257b1950b24fd5a96054cda7c5848f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3d25ba31c814cfea3ad125a3e74c71e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b69c7f4271d471f933a257bbd6dce19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39f9cb7e781417c8ae7be1d1a3d94ac"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.embedding_hidden_mapping_in\nReplaced layer: encoder.albert_layer_groups.0.albert_layers.0.attention.query\nReplaced layer: encoder.albert_layer_groups.0.albert_layers.0.attention.key\nReplaced layer: encoder.albert_layer_groups.0.albert_layers.0.attention.value\nReplaced layer: encoder.albert_layer_groups.0.albert_layers.0.attention.dense\nReplaced layer: encoder.albert_layer_groups.0.albert_layers.0.ffn\nReplaced layer: encoder.albert_layer_groups.0.albert_layers.0.ffn_output\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m original_size \u001b[38;5;241m=\u001b[39m count_parameters(model)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Replace linear layers with low-rank approximations\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mreplace_with_low_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOMPRESSION_RANK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Get final size\u001b[39;00m\n\u001b[1;32m     62\u001b[0m compressed_size \u001b[38;5;241m=\u001b[39m count_parameters(model)\n","Cell \u001b[0;32mIn[10], line 41\u001b[0m, in \u001b[0;36mreplace_with_low_rank\u001b[0;34m(model, rank)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Create a LowRankLayer to replace the full-rank linear layer\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     low_rank_layer \u001b[38;5;241m=\u001b[39m LowRankLayer(rank, module)\n\u001b[0;32m---> 41\u001b[0m     parent_name, child_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m     parent_module \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_submodule(parent_name)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(parent_module, child_name, low_rank_layer)\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"],"ename":"ValueError","evalue":"not enough values to unpack (expected 2, got 1)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.nn import functional as F\n\n# Specify the model name and compression rank\nMODEL_NAME = 'google-t5/t5-small'  # Replace with your desired model name\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:59:15.863373Z","iopub.execute_input":"2024-07-01T08:59:15.863675Z","iopub.status.idle":"2024-07-01T08:59:24.636442Z","shell.execute_reply.started":"2024-07-01T08:59:15.863646Z","shell.execute_reply":"2024-07-01T08:59:24.635530Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Replaced layer: encoder.block.0.layer.0.SelfAttention.q\nReplaced layer: encoder.block.0.layer.0.SelfAttention.k\nReplaced layer: encoder.block.0.layer.0.SelfAttention.v\nReplaced layer: encoder.block.0.layer.0.SelfAttention.o\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.1.layer.0.SelfAttention.q\nReplaced layer: encoder.block.1.layer.0.SelfAttention.k\nReplaced layer: encoder.block.1.layer.0.SelfAttention.v\nReplaced layer: encoder.block.1.layer.0.SelfAttention.o\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.2.layer.0.SelfAttention.q\nReplaced layer: encoder.block.2.layer.0.SelfAttention.k\nReplaced layer: encoder.block.2.layer.0.SelfAttention.v\nReplaced layer: encoder.block.2.layer.0.SelfAttention.o\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.3.layer.0.SelfAttention.q\nReplaced layer: encoder.block.3.layer.0.SelfAttention.k\nReplaced layer: encoder.block.3.layer.0.SelfAttention.v\nReplaced layer: encoder.block.3.layer.0.SelfAttention.o\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.4.layer.0.SelfAttention.q\nReplaced layer: encoder.block.4.layer.0.SelfAttention.k\nReplaced layer: encoder.block.4.layer.0.SelfAttention.v\nReplaced layer: encoder.block.4.layer.0.SelfAttention.o\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.5.layer.0.SelfAttention.q\nReplaced layer: encoder.block.5.layer.0.SelfAttention.k\nReplaced layer: encoder.block.5.layer.0.SelfAttention.v\nReplaced layer: encoder.block.5.layer.0.SelfAttention.o\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wo\nReplaced layer: decoder.block.0.layer.0.SelfAttention.q\nReplaced layer: decoder.block.0.layer.0.SelfAttention.k\nReplaced layer: decoder.block.0.layer.0.SelfAttention.v\nReplaced layer: decoder.block.0.layer.0.SelfAttention.o\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.1.layer.0.SelfAttention.q\nReplaced layer: decoder.block.1.layer.0.SelfAttention.k\nReplaced layer: decoder.block.1.layer.0.SelfAttention.v\nReplaced layer: decoder.block.1.layer.0.SelfAttention.o\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.2.layer.0.SelfAttention.q\nReplaced layer: decoder.block.2.layer.0.SelfAttention.k\nReplaced layer: decoder.block.2.layer.0.SelfAttention.v\nReplaced layer: decoder.block.2.layer.0.SelfAttention.o\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.3.layer.0.SelfAttention.q\nReplaced layer: decoder.block.3.layer.0.SelfAttention.k\nReplaced layer: decoder.block.3.layer.0.SelfAttention.v\nReplaced layer: decoder.block.3.layer.0.SelfAttention.o\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.4.layer.0.SelfAttention.q\nReplaced layer: decoder.block.4.layer.0.SelfAttention.k\nReplaced layer: decoder.block.4.layer.0.SelfAttention.v\nReplaced layer: decoder.block.4.layer.0.SelfAttention.o\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.5.layer.0.SelfAttention.q\nReplaced layer: decoder.block.5.layer.0.SelfAttention.k\nReplaced layer: decoder.block.5.layer.0.SelfAttention.v\nReplaced layer: decoder.block.5.layer.0.SelfAttention.o\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wo\nOriginal model size (parameters): 60506624\nCompressed model size (parameters): 20890112\nCompression rate: 65.47%\nModel saved to directory: compressed_model\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\nfrom datasets import load_dataset, load_metric\n\n# Load the dataset\ndataset = load_dataset(\"glue\", \"mrpc\")\nmetric = load_metric(\"glue\", \"mrpc\")\n\n# Preprocess the dataset\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n\ndef preprocess_function(examples):\n    inputs = [\"mrpc sentence1: \" + ex for ex in examples[\"sentence1\"]]\n    targets = [ex for ex in examples[\"sentence2\"]]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\n\n# Load the original model for comparison\noriginal_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:58:52.465007Z","iopub.execute_input":"2024-07-01T08:58:52.465935Z","iopub.status.idle":"2024-07-01T08:59:15.861271Z","shell.execute_reply.started":"2024-07-01T08:58:52.465902Z","shell.execute_reply":"2024-07-01T08:59:15.860376Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-07-01 08:58:54.893556: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 08:58:54.893658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 08:58:55.050221: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e013a7cb3554c7ebe2509d0c0e12aef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/649k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b381da091e4d00b29b65a0055e07ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/75.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64f0b8f953ce4b138efd2defd3aa42f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/308k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0772b41924a34a519a36135121c991ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb52fde85c24c519b416dbce5a0e8d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"272c11c09a2f4e989635e39d717333ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef7eba1aeb464a4f94540ee98130a038"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_34/859671200.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"glue\", \"mrpc\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/glue/glue.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c542f6f6535240eda9f7b8319ecedb26"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce9a1767e8994abeb8e9c830e6da72c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9d38b11eae5414a9b880e4ec475a8bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0473d51bdbb24380b72d09af0a9364ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93058b58a7434ac6b1dc7242673d5ecb"}},"metadata":{}}]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    return metric.compute(predictions=preds, references=labels)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:59:24.638181Z","iopub.execute_input":"2024-07-01T08:59:24.638551Z","iopub.status.idle":"2024-07-01T08:59:24.750528Z","shell.execute_reply.started":"2024-07-01T08:59:24.638518Z","shell.execute_reply":"2024-07-01T08:59:24.749562Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Trainer for the original model\noriginal_trainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the original model\noriginal_trainer.train()\n\n# Evaluate the original model\noriginal_eval_results = original_trainer.evaluate()\n\n# Trainer for the compressed model\ncompressed_model = T5ForConditionalGeneration.from_pretrained(\"compressed_model\")\n\ncompressed_trainer = Trainer(\n    model=compressed_model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the compressed model\ncompressed_trainer.train()\n\n# Evaluate the compressed model\ncompressed_eval_results = compressed_trainer.evaluate()\n\n# Print the evaluation results\nprint(\"Original Model Evaluation Results:\", original_eval_results)\nprint(\"Compressed Model Evaluation Results:\", compressed_eval_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T09:02:15.380701Z","iopub.execute_input":"2024-07-01T09:02:15.381045Z","iopub.status.idle":"2024-07-01T09:04:42.617208Z","shell.execute_reply.started":"2024-07-01T09:02:15.381019Z","shell.execute_reply":"2024-07-01T09:04:42.615187Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240701_090333-0cc9ebv3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ee200002059/huggingface/runs/0cc9ebv3' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ee200002059/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ee200002059/huggingface' target=\"_blank\">https://wandb.ai/ee200002059/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ee200002059/huggingface/runs/0cc9ebv3' target=\"_blank\">https://wandb.ai/ee200002059/huggingface/runs/0cc9ebv3</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='116' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [116/345 00:44 < 01:29, 2.55 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='11' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/13 00:02 < 00:00, 4.10 it/s]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m original_trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39moriginal_model,\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fine-tune the original model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43moriginal_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate the original model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m original_eval_results \u001b[38;5;241m=\u001b[39m original_trainer\u001b[38;5;241m.\u001b[39mevaluate()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2311\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2311\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2315\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2721\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2719\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3569\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3571\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3572\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3582\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3779\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3777\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   3778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 3779\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3781\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:326\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:138\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    135\u001b[0m     new_tensors\n\u001b[1;32m    136\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:138\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    135\u001b[0m     new_tensors\n\u001b[1;32m    136\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:140\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    143\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    144\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:99\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     96\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m    102\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.88 GiB. GPU 0 has a total capacty of 14.75 GiB of which 5.23 GiB is free. Process 2525 has 9.51 GiB memory in use. Of the allocated memory 6.67 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 5.88 GiB. GPU 0 has a total capacty of 14.75 GiB of which 5.23 GiB is free. Process 2525 has 9.51 GiB memory in use. Of the allocated memory 6.67 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, T5ForConditionalGeneration\nfrom torch.nn import functional as F\nfrom datasets import load_dataset, load_metric\n\n# Specify the model name and compression rank\nMODEL_NAME = 't5-small'  # Replace with your desired model name\n# MODEL_NAME = 'distilbert-base-uncased'  # Replace with your desired model name\n\nCOMPRESSION_RANK = 32 # Adjust this for more or less aggressive compression\nHUGGINGFACE_TOKEN = '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'  # Replace with your Hugging Face token\n\n# Define LowRankLayer class for low-rank decomposition\nclass LowRankLayer(nn.Module):\n    \"\"\"Given a linear layer, find low rank decomposition.\"\"\"\n    def __init__(self, rank, full_rank_layer):\n        super().__init__()\n        self.rank = rank\n\n        # Perform SVD on the full-rank layer's weight matrix\n        U, S, Vh = torch.linalg.svd(full_rank_layer.weight.float())\n        S_diag = torch.diag(S)\n        self.U = nn.Parameter(U[:, :self.rank].contiguous())\n        self.S = nn.Parameter(S_diag[:self.rank, :self.rank].contiguous())\n        self.Vh = nn.Parameter(Vh[:self.rank, :].contiguous())\n\n        # Handle the bias term if it exists\n        if full_rank_layer.bias is not None:\n            self.bias = nn.Parameter(full_rank_layer.bias.float().contiguous())\n        else:\n            self.bias = None\n\n    def forward(self, x):\n        aprox_weight_matrix = self.U @ self.S @ self.Vh\n        output = F.linear(x, aprox_weight_matrix, self.bias)\n        return output\n\n# Function to replace linear layers with LowRankLayer\ndef replace_with_low_rank(model, rank):\n    for name, module in model.named_modules():\n        if isinstance(module, nn.Linear):\n            # Create a LowRankLayer to replace the full-rank linear layer\n            low_rank_layer = LowRankLayer(rank, module)\n            parent_name, child_name = name.rsplit('.', 1)\n            parent_module = model.get_submodule(parent_name)\n            setattr(parent_module, child_name, low_rank_layer)\n            print(f\"Replaced layer: {name}\")\n    return model\n\n# Function to calculate the total number of parameters in the model\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Load the tokenizer and model with the Hugging Face token\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\nmodel = AutoModel.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\n\n# Get initial size\noriginal_size = count_parameters(model)\n\n# Replace linear layers with low-rank approximations\nmodel = replace_with_low_rank(model, COMPRESSION_RANK)\n\n# Get final size\ncompressed_size = count_parameters(model)\n\n# Print sizes and compression rate\nprint(f\"Original model size (parameters): {original_size}\")\nprint(f\"Compressed model size (parameters): {compressed_size}\")\nprint(f\"Compression rate: {(original_size - compressed_size) / original_size:.2%}\")\n\n# Save the tokenizer and model to the directory\nmodel_dir = \"compressed_model\"\ntokenizer.save_pretrained(model_dir)\nmodel.save_pretrained(model_dir)\n\nprint(f\"Model saved to directory: {model_dir}\")\n\n# Evaluation\ndataset = load_dataset(\"glue\", \"mrpc\")\nmetric = load_metric(\"glue\", \"mrpc\")\n\ndef preprocess_function(examples):\n    inputs = [\"mrpc sentence1: \" + ex for ex in examples[\"sentence1\"]]\n    targets = [ex for ex in examples[\"sentence2\"]]\n    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\").input_ids\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\n\ndef compute_metrics(p):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.argmax(preds, axis=1)\n    return metric.compute(predictions=preds, references=p.label_ids)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=[]  # Disable WandB logging\n)\n\n# Trainer for the original model\noriginal_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN)\noriginal_trainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the original model\noriginal_trainer.train()\n\n# Evaluate the original model\noriginal_eval_results = original_trainer.evaluate()\n\n# Trainer for the compressed model\ncompressed_model = T5ForConditionalGeneration.from_pretrained(\"compressed_model\", use_auth_token=HUGGINGFACE_TOKEN)\n\ncompressed_trainer = Trainer(\n    model=compressed_model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    compute_metrics=compute_metrics,\n)\n\n# Fine-tune the compressed model\ncompressed_trainer.train()\n\n# Evaluate the compressed model\ncompressed_eval_results = compressed_trainer.evaluate()\n\n# Print the evaluation results\nprint(\"Original Model Evaluation Results:\", original_eval_results)\nprint(\"Compressed Model Evaluation Results:\", compressed_eval_results)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:48:17.355447Z","iopub.execute_input":"2024-07-01T12:48:17.356409Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-07-01 12:48:20.919350: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-01 12:48:20.919469: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-01 12:48:21.085912: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58a00228e94348c1a789d2f754cd5646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946a3f94602b489fa56c9e96a17564c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa355b9220f4fd5802b835d37cd7c58"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a588dd0b7e824feda35bded6d9b78897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"804fb38426d34dddbbe6f001443fd759"}},"metadata":{}},{"name":"stdout","text":"Replaced layer: encoder.block.0.layer.0.SelfAttention.q\nReplaced layer: encoder.block.0.layer.0.SelfAttention.k\nReplaced layer: encoder.block.0.layer.0.SelfAttention.v\nReplaced layer: encoder.block.0.layer.0.SelfAttention.o\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.0.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.1.layer.0.SelfAttention.q\nReplaced layer: encoder.block.1.layer.0.SelfAttention.k\nReplaced layer: encoder.block.1.layer.0.SelfAttention.v\nReplaced layer: encoder.block.1.layer.0.SelfAttention.o\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.1.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.2.layer.0.SelfAttention.q\nReplaced layer: encoder.block.2.layer.0.SelfAttention.k\nReplaced layer: encoder.block.2.layer.0.SelfAttention.v\nReplaced layer: encoder.block.2.layer.0.SelfAttention.o\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.2.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.3.layer.0.SelfAttention.q\nReplaced layer: encoder.block.3.layer.0.SelfAttention.k\nReplaced layer: encoder.block.3.layer.0.SelfAttention.v\nReplaced layer: encoder.block.3.layer.0.SelfAttention.o\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.3.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.4.layer.0.SelfAttention.q\nReplaced layer: encoder.block.4.layer.0.SelfAttention.k\nReplaced layer: encoder.block.4.layer.0.SelfAttention.v\nReplaced layer: encoder.block.4.layer.0.SelfAttention.o\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.4.layer.1.DenseReluDense.wo\nReplaced layer: encoder.block.5.layer.0.SelfAttention.q\nReplaced layer: encoder.block.5.layer.0.SelfAttention.k\nReplaced layer: encoder.block.5.layer.0.SelfAttention.v\nReplaced layer: encoder.block.5.layer.0.SelfAttention.o\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wi\nReplaced layer: encoder.block.5.layer.1.DenseReluDense.wo\nReplaced layer: decoder.block.0.layer.0.SelfAttention.q\nReplaced layer: decoder.block.0.layer.0.SelfAttention.k\nReplaced layer: decoder.block.0.layer.0.SelfAttention.v\nReplaced layer: decoder.block.0.layer.0.SelfAttention.o\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.0.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.0.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.1.layer.0.SelfAttention.q\nReplaced layer: decoder.block.1.layer.0.SelfAttention.k\nReplaced layer: decoder.block.1.layer.0.SelfAttention.v\nReplaced layer: decoder.block.1.layer.0.SelfAttention.o\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.1.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.1.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.2.layer.0.SelfAttention.q\nReplaced layer: decoder.block.2.layer.0.SelfAttention.k\nReplaced layer: decoder.block.2.layer.0.SelfAttention.v\nReplaced layer: decoder.block.2.layer.0.SelfAttention.o\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.2.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.2.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.3.layer.0.SelfAttention.q\nReplaced layer: decoder.block.3.layer.0.SelfAttention.k\nReplaced layer: decoder.block.3.layer.0.SelfAttention.v\nReplaced layer: decoder.block.3.layer.0.SelfAttention.o\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.3.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.3.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.4.layer.0.SelfAttention.q\nReplaced layer: decoder.block.4.layer.0.SelfAttention.k\nReplaced layer: decoder.block.4.layer.0.SelfAttention.v\nReplaced layer: decoder.block.4.layer.0.SelfAttention.o\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.4.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.4.layer.2.DenseReluDense.wo\nReplaced layer: decoder.block.5.layer.0.SelfAttention.q\nReplaced layer: decoder.block.5.layer.0.SelfAttention.k\nReplaced layer: decoder.block.5.layer.0.SelfAttention.v\nReplaced layer: decoder.block.5.layer.0.SelfAttention.o\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.q\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.k\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.v\nReplaced layer: decoder.block.5.layer.1.EncDecAttention.o\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wi\nReplaced layer: decoder.block.5.layer.2.DenseReluDense.wo\nOriginal model size (parameters): 60506624\nCompressed model size (parameters): 20890112\nCompression rate: 65.47%\nModel saved to directory: compressed_model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04add8b5f67f41acae59a5c42584ab93"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}