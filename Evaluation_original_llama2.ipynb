{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Time: 1729 seconds == 28.8 minutes\n",
    "## Tokens per Second: 0.1156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single response on CPU ORIGINAL llama2 MODEL\n",
    "\n",
    "Generated Text: Answer the below question: How does Quantum Computers work?\n",
    "How does Quantum Computers work?\n",
    "Quantum computers are based on the principles of quantum mechanics.\n",
    "Quantum mechanics is a theory that explains the behavior of\n",
    "Response Time: 420.5998430252075 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/azureuser/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s]\n",
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "# Move the model to CPU\n",
    "model = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Answer the below question: How does Quantum Computers work?\n",
      "How does Quantum Computers work?\n",
      "Quantum computers are based on the principles of quantum mechanics.\n",
      "Quantum mechanics is a theory that explains the behavior of\n",
      "Response Time: 420.5998430252075 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Function to generate text and calculate the response time\n",
    "def generate_text_with_timing(prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    response_time = end_time - start_time\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text, response_time\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Answer the below question: How does Quantum Computers work?\"\n",
    "generated_text, response_time = generate_text_with_timing(prompt)\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Response Time: {response_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Answer the below question: How does Quantum Computers work?\n",
      "Answer: Quantum computers are computers that use quantum-mechanical phenomena to perform operations on data.\n",
      "They are based on quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data.\n",
      "Quantum computers are expected to have advantages over classical computers in certain tasks, such as factoring large numbers and simulating quantum systems.\n",
      "Quantum computers work by using quantum-mechanical phenomena to perform operations on data.\n",
      "These operations are based on quantum-mechanical phenomena, such as superposition and entanglement.\n",
      "Quantum computers are expected to have advantages over classical computers in certain tasks, such as factoring large numbers and simulating quantum systems.\n",
      "Quantum computers work by using quantum-mechanical phenomena to perform operations on data. These operations are based on quantum-mechanical phenomena, such as\n",
      "Response Time: 1729.0396835803986 seconds\n",
      "Tokens per Second: 0.11567114502881229\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Answer the below question: How does Quantum Computers work?\"\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "# Generate text and calculate response time\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=200)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate tokens per second\n",
    "response_time = end_time - start_time\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "num_tokens = len(outputs[0])\n",
    "tokens_per_second = num_tokens / response_time\n",
    "\n",
    "# Print results\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "print(f\"Response Time: {response_time} seconds\")\n",
    "print(f\"Tokens per Second: {tokens_per_second}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.8 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/pavan/train/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 2/2 [00:49<00:00, 24.85s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6738415616\n",
      "Total file size: 25707.39 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Calculate the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Create a temporary directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Save the model and tokenizer to the temporary directory\n",
    "    model.save_pretrained(temp_dir)\n",
    "    tokenizer.save_pretrained(temp_dir)\n",
    "    \n",
    "    # Calculate the total size of the saved model files\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(temp_dir):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    \n",
    "    print(f\"Total file size: {total_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# The temporary directory and its contents are automatically deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [00:39<00:00, 13.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at hahnyuan/Llama-2-7b-hf-asvd85 and are newly initialized: ['model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.0.mlp.down_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 6738415616\n",
      "Total file size: 25707.40 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tempfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_name = \"hahnyuan/Llama-2-7b-hf-asvd85\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Calculate the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "\n",
    "# Create a temporary directory\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    # Save the model and tokenizer to the temporary directory\n",
    "    model.save_pretrained(temp_dir)\n",
    "    tokenizer.save_pretrained(temp_dir)\n",
    "    \n",
    "    # Calculate the total size of the saved model files\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(temp_dir):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    \n",
    "    print(f\"Total file size: {total_size / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# The temporary directory and its contents are automatically deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
